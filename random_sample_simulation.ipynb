{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d4bb4a",
   "metadata": {},
   "source": [
    "# Random Sample Analysis: Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f3444",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Tasks\n",
    "\n",
    "- Define query\n",
    "- Define functions\n",
    "- Describe current random sample for selected segments\n",
    "- Define necessary sample size for selected segment\n",
    "- Run simulations and then compare metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d06e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages from requirements.txt ...\n",
      "Requirement already satisfied: pandas in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.3.1)\n",
      "Requirement already satisfied: numpy in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (2.3.1)\n",
      "Requirement already satisfied: scipy in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: plotly in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (6.2.0)\n",
      "Requirement already satisfied: joblib in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: scikit-learn in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (1.7.0)\n",
      "Requirement already satisfied: matplotlib in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 7)) (3.10.3)\n",
      "Requirement already satisfied: seaborn in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (0.13.2)\n",
      "Requirement already satisfied: statsmodels in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 9)) (0.14.5)\n",
      "Requirement already satisfied: psycopg2-binary in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 10)) (2.9.10)\n",
      "Requirement already satisfied: redshift_connector in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 11)) (2.1.8)\n",
      "Requirement already satisfied: ipywidgets in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 12)) (8.1.7)\n",
      "Requirement already satisfied: tqdm in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 13)) (4.67.1)\n",
      "Requirement already satisfied: IPython in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 14)) (9.4.0)\n",
      "Collecting notebook (from -r requirements.txt (line 15))\n",
      "  Using cached notebook-7.4.4-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jupyter (from -r requirements.txt (line 16))\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions in ./.coding-venv/lib/python3.13/site-packages (from -r requirements.txt (line 17)) (4.14.1)\n",
      "Collecting jupyterlab (from -r requirements.txt (line 19))\n",
      "  Using cached jupyterlab-4.4.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.coding-venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.coding-venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.coding-venv/lib/python3.13/site-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in ./.coding-venv/lib/python3.13/site-packages (from plotly->-r requirements.txt (line 4)) (1.46.0)\n",
      "Requirement already satisfied: packaging in ./.coding-venv/lib/python3.13/site-packages (from plotly->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.coding-venv/lib/python3.13/site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.coding-venv/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.coding-venv/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 7)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.coding-venv/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 7)) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.coding-venv/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in ./.coding-venv/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 7)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.coding-venv/lib/python3.13/site-packages (from matplotlib->-r requirements.txt (line 7)) (3.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in ./.coding-venv/lib/python3.13/site-packages (from statsmodels->-r requirements.txt (line 9)) (1.0.1)\n",
      "Requirement already satisfied: scramp<1.5.0,>=1.2.0 in ./.coding-venv/lib/python3.13/site-packages (from redshift_connector->-r requirements.txt (line 11)) (1.4.6)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in ./.coding-venv/lib/python3.13/site-packages (from redshift_connector->-r requirements.txt (line 11)) (4.13.4)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.9.201 in ./.coding-venv/lib/python3.13/site-packages (from redshift_connector->-r requirements.txt (line 11)) (1.39.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in ./.coding-venv/lib/python3.13/site-packages (from redshift_connector->-r requirements.txt (line 11)) (2.32.4)\n",
      "Requirement already satisfied: lxml<6.0.0,>=4.6.5 in ./.coding-venv/lib/python3.13/site-packages (from redshift_connector->-r requirements.txt (line 11)) (5.4.0)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.12.201 in ./.coding-venv/lib/python3.13/site-packages (from redshift_connector->-r requirements.txt (line 11)) (1.39.4)\n",
      "Requirement already satisfied: setuptools in ./.coding-venv/lib/python3.13/site-packages (from redshift_connector->-r requirements.txt (line 11)) (80.9.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.coding-venv/lib/python3.13/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift_connector->-r requirements.txt (line 11)) (2.7)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./.coding-venv/lib/python3.13/site-packages (from boto3<2.0.0,>=1.9.201->redshift_connector->-r requirements.txt (line 11)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in ./.coding-venv/lib/python3.13/site-packages (from boto3<2.0.0,>=1.9.201->redshift_connector->-r requirements.txt (line 11)) (0.13.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in ./.coding-venv/lib/python3.13/site-packages (from botocore<2.0.0,>=1.12.201->redshift_connector->-r requirements.txt (line 11)) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.coding-venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.coding-venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.23.0->redshift_connector->-r requirements.txt (line 11)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.coding-venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.23.0->redshift_connector->-r requirements.txt (line 11)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.coding-venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.23.0->redshift_connector->-r requirements.txt (line 11)) (2025.7.14)\n",
      "Requirement already satisfied: asn1crypto>=1.5.1 in ./.coding-venv/lib/python3.13/site-packages (from scramp<1.5.0,>=1.2.0->redshift_connector->-r requirements.txt (line 11)) (1.5.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.coding-venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 12)) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.coding-venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 12)) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./.coding-venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 12)) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.coding-venv/lib/python3.13/site-packages (from ipywidgets->-r requirements.txt (line 12)) (3.0.15)\n",
      "Requirement already satisfied: decorator in ./.coding-venv/lib/python3.13/site-packages (from IPython->-r requirements.txt (line 14)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.coding-venv/lib/python3.13/site-packages (from IPython->-r requirements.txt (line 14)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.coding-venv/lib/python3.13/site-packages (from IPython->-r requirements.txt (line 14)) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.coding-venv/lib/python3.13/site-packages (from IPython->-r requirements.txt (line 14)) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.coding-venv/lib/python3.13/site-packages (from IPython->-r requirements.txt (line 14)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.coding-venv/lib/python3.13/site-packages (from IPython->-r requirements.txt (line 14)) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.coding-venv/lib/python3.13/site-packages (from IPython->-r requirements.txt (line 14)) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./.coding-venv/lib/python3.13/site-packages (from IPython->-r requirements.txt (line 14)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in ./.coding-venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython->-r requirements.txt (line 14)) (0.2.13)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from notebook->-r requirements.txt (line 15))\n",
      "  Using cached jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from notebook->-r requirements.txt (line 15))\n",
      "  Using cached jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim<0.3,>=0.2 (from notebook->-r requirements.txt (line 15))\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in ./.coding-venv/lib/python3.13/site-packages (from notebook->-r requirements.txt (line 15)) (6.5.1)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->-r requirements.txt (line 19))\n",
      "  Using cached async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting httpx>=0.25.0 (from jupyterlab->-r requirements.txt (line 19))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in ./.coding-venv/lib/python3.13/site-packages (from jupyterlab->-r requirements.txt (line 19)) (6.29.5)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in ./.coding-venv/lib/python3.13/site-packages (from jupyterlab->-r requirements.txt (line 19)) (3.1.6)\n",
      "Requirement already satisfied: jupyter-core in ./.coding-venv/lib/python3.13/site-packages (from jupyterlab->-r requirements.txt (line 19)) (5.8.1)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->-r requirements.txt (line 19))\n",
      "  Using cached jupyter_lsp-2.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting anyio>=3.1.0 (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in ./.coding-venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (8.6.3)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in ./.coding-venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (7.16.6)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in ./.coding-venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (5.10.4)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached prometheus_client-0.22.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pyzmq>=24 in ./.coding-venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (27.0.0)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 15))\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 15))\n",
      "  Using cached json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in ./.coding-venv/lib/python3.13/site-packages (from jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 15)) (4.24.0)\n",
      "Collecting jupyter-console (from jupyter->-r requirements.txt (line 16))\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting sniffio>=1.1 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp38-abi3-macosx_10_9_universal2.whl.metadata (6.7 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 19))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.0->jupyterlab->-r requirements.txt (line 19))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: appnope in ./.coding-venv/lib/python3.13/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 19)) (0.1.4)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./.coding-venv/lib/python3.13/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 19)) (1.8.14)\n",
      "Requirement already satisfied: nest-asyncio in ./.coding-venv/lib/python3.13/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 19)) (1.6.0)\n",
      "Requirement already satisfied: psutil in ./.coding-venv/lib/python3.13/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 19)) (7.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.coding-venv/lib/python3.13/site-packages (from jedi>=0.16->IPython->-r requirements.txt (line 14)) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.coding-venv/lib/python3.13/site-packages (from jinja2>=3.0.3->jupyterlab->-r requirements.txt (line 19)) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.coding-venv/lib/python3.13/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 15)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.coding-venv/lib/python3.13/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 15)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.coding-venv/lib/python3.13/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 15)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.coding-venv/lib/python3.13/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 15)) (0.26.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.coding-venv/lib/python3.13/site-packages (from jupyter-core->jupyterlab->-r requirements.txt (line 19)) (4.3.8)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3 in ./.coding-venv/lib/python3.13/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (6.0.2)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: bleach!=5.0.0 in ./.coding-venv/lib/python3.13/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in ./.coding-venv/lib/python3.13/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in ./.coding-venv/lib/python3.13/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in ./.coding-venv/lib/python3.13/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in ./.coding-venv/lib/python3.13/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./.coding-venv/lib/python3.13/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (1.5.1)\n",
      "Requirement already satisfied: webencodings in ./.coding-venv/lib/python3.13/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in ./.coding-venv/lib/python3.13/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./.coding-venv/lib/python3.13/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15)) (2.21.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.coding-venv/lib/python3.13/site-packages (from pexpect>4.3->IPython->-r requirements.txt (line 14)) (0.7.0)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached cffi-1.17.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 15))\n",
      "  Using cached types_python_dateutil-2.9.0.20250708-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.coding-venv/lib/python3.13/site-packages (from stack_data->IPython->-r requirements.txt (line 14)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.coding-venv/lib/python3.13/site-packages (from stack_data->IPython->-r requirements.txt (line 14)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.coding-venv/lib/python3.13/site-packages (from stack_data->IPython->-r requirements.txt (line 14)) (0.2.3)\n",
      "Using cached notebook-7.4.4-py3-none-any.whl (14.3 MB)\n",
      "Using cached jupyterlab-4.4.5-py3-none-any.whl (12.3 MB)\n",
      "Using cached jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
      "Using cached jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Using cached async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached json5-0.12.0-py3-none-any.whl (36 kB)\n",
      "Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jupyter_lsp-2.2.6-py3-none-any.whl (69 kB)\n",
      "Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached prometheus_client-0.22.1-py3-none-any.whl (58 kB)\n",
      "Using cached python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Using cached webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached argon2_cffi_bindings-21.2.0-cp38-abi3-macosx_10_9_universal2.whl (53 kB)\n",
      "Using cached cffi-1.17.1-cp313-cp313-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached types_python_dateutil-2.9.0.20250708-py3-none-any.whl (17 kB)\n",
      "Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: websocket-client, webcolors, uri-template, types-python-dateutil, terminado, sniffio, send2trash, rfc3986-validator, rfc3339-validator, python-json-logger, pycparser, prometheus-client, overrides, jsonpointer, json5, h11, fqdn, babel, async-lru, jupyter-server-terminals, httpcore, cffi, arrow, anyio, isoduration, httpx, argon2-cffi-bindings, jupyter-console, argon2-cffi, jupyter-events, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37/37\u001b[0m [jupyter]5/37\u001b[0m [notebook]b]ver]\n",
      "\u001b[1A\u001b[2KSuccessfully installed anyio-4.9.0 argon2-cffi-25.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 async-lru-2.0.5 babel-2.17.0 cffi-1.17.1 fqdn-1.5.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 isoduration-20.11.0 json5-0.12.0 jsonpointer-3.0.0 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.12.0 jupyter-lsp-2.2.6 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.5 jupyterlab-server-2.27.3 notebook-7.4.4 notebook-shim-0.2.4 overrides-7.7.0 prometheus-client-0.22.1 pycparser-2.22 python-json-logger-3.3.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 send2trash-1.8.3 sniffio-1.3.1 terminado-0.18.1 types-python-dateutil-2.9.0.20250708 uri-template-1.3.0 webcolors-24.11.1 websocket-client-1.8.0\n"
     ]
    }
   ],
   "source": [
    "# Install requirements.txt if not already installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    if os.path.exists('requirements.txt'):\n",
    "        print(\"Installing packages from requirements.txt ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n",
    "    else:\n",
    "        print(\"requirements.txt not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error installing requirements: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400317f",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6749568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "#do you need to download an existing data?\n",
    "download = True\n",
    "\n",
    "# Define the end date for your analysis (e.g., 7 days ago to allow for QA review time)s\n",
    "START_DATE = '2025-06-01'\n",
    "END_DATE = '2025-06-30'\n",
    "\n",
    "#OR Relative date range\n",
    "# days_back_for_sql = 30  # Number of days back from the end date for SQL\n",
    "# START_DATE = (end_date - timedelta(days=days_back_for_sql)).strftime('%Y-%m-%d')\n",
    "# END_DATE = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "# Other query parameters\n",
    "REVIEW_TYPE = ['random_sample'] #['random_sample', 'customer_reporting_1', 'authentication_product']\n",
    "REVIEW_TYPE_FOR_QUERY = ', '.join([\"'{}'\".format(review_type) for review_type in REVIEW_TYPE])\n",
    "\n",
    "# for Redshift connections\n",
    "REDSHIFT_HOST = \"wg-serverless-1.875431798560.eu-west-1.redshift-serverless.amazonaws.com\"\n",
    "REDSHIFT_DATABASE = \"vaire\"\n",
    "REDSHIFT_USER = os.environ.get(\"REDSHIFT_USER\")\n",
    "REDSHIFT_PASSWORD = os.environ.get(\"REDSHIFT_PASSWORD\")\n",
    "REDSHIFT_PORT = 5439\n",
    "\n",
    "## ready file if any\n",
    "file_path = 'sample_data_monthly.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe707d",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4bd487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "WITH \n",
    "customers as (\n",
    "select\n",
    "    fvs.vendor_display_name as customer\n",
    "    ,fvs.vendor_dim_id\n",
    "    ,COUNT(*) AS sessions\n",
    "from\n",
    "    analytics_mart_kpi.mart_kpi__verification_session_metrics fvs\n",
    "where true\n",
    "    and date(fvs.verified_at) >= CAST('{START_DATE}' AS DATE)\n",
    "    and date(fvs.verified_at) <= CAST('{END_DATE}' AS DATE)\n",
    "group by\n",
    "    1,2\n",
    ")\n",
    ", customer_ranked as (\n",
    "select \n",
    "    c.customer\n",
    "    , c.vendor_dim_id\n",
    "    , ROW_NUMBER() OVER (ORDER BY c.sessions DESC) as customer_rank\n",
    "from customers c\n",
    ")\n",
    ", data AS (\n",
    "select \n",
    "--dimensions\n",
    "    vs.document_country_sf_region as region\n",
    "    , coalesce(vs.document_specimen_country_name, 'UNKNOWN') as document_country\n",
    "    , coalesce(vs.geoip_country_iso2, 'UNKNOWN') as geoip_country\n",
    "    , coalesce(vs.platform, 'UNKNOWN') as platform\n",
    "    , coalesce(vs.primary_product, 'UNKNOWN') as primary_product\n",
    "    , coalesce(vs.primary_industry, 'UNKNOWN') as industry\n",
    "    , coalesce(vs.sf_customer_priority, 'UNKNOWN') as customer_priority\n",
    "    , coalesce(vs.vendor_display_name, 'UNKNOWN') as customer\n",
    "    , CASE WHEN c.customer_rank <= 20 THEN vs.vendor_display_name ELSE 'Other' END as customer_top_20\n",
    "    , CASE\n",
    "        WHEN vs.primary_product IN ('IDV', 'Document Verification', 'AIC') THEN 'IDV'\n",
    "        WHEN vs.primary_product LIKE 'Full Auto%' THEN 'Full Auto'\n",
    "        WHEN vs.primary_product LIKE '%Biometric%' OR vs.primary_product IN ('Face Match', 'Age Estimation') THEN 'Biometric Solutions'\n",
    "        WHEN vs.primary_product = 'Unknown' THEN 'Unknown'\n",
    "        ELSE 'Data Verification'\n",
    "    END as product_portfolio\n",
    "    --session related\n",
    "    , vs.verification_session_uuid\n",
    "    , CAST(vs.verified_at as DATE) as verified_date\n",
    "    , vs.verification_session_status\n",
    "    , vs.verification_session_status = 'declined' as is_decline\n",
    "    , vs.verification_session_status = 'approved' as is_approve\n",
    "    , vs.is_full_auto_session\n",
    "    --fraud\n",
    "    , vs.is_fraudulent_sessions_other or vs.is_fraudulent_sessions_velocity as is_declined_due_to_fraud\n",
    "    --qa related\n",
    "    , qa.verification_session_uuid as qa_verification_session_uuid\n",
    "    , qa.qa_review_type\n",
    "    , qa.flag_session_has_decision_error\n",
    "    , qa.flag_session_has_critical_approve_error\n",
    "    , qa.flag_session_has_critical_decline_error\n",
    "    , qa.flag_session_has_extraction_error\n",
    "    , qa.is_session_approved_fraud\n",
    "    , qa.is_session_fraud\n",
    "    , qa.qa_decision\n",
    "    , qa.original_decision\n",
    "    , qa.qa_decision = 'declined' as qa_declinable\n",
    "    , qa.qa_decision = 'approved' as qa_approvable\n",
    "from analytics_mart_kpi.mart_kpi__verification_session_metrics vs\n",
    "left join analytics_mart_automation.mart_automation__verification_session_qa_review qa\n",
    "    on vs.verification_session_uuid = qa.verification_session_uuid\n",
    "    and vs.shard_key = qa.shard_key\n",
    "    and qa.qa_review_type IN ({REVIEW_TYPE_FOR_QUERY})\n",
    "left join customer_ranked c on vs.vendor_dim_id = c.vendor_dim_id\n",
    "where true\n",
    "    and date(verified_at) >= CAST('{START_DATE}' AS DATE)\n",
    "    and date(verified_at) <= CAST('{END_DATE}' AS DATE)\n",
    "    --and vs.service_agreement_stage NOT IN ('test', 'test_low')\n",
    ")\n",
    ", unique_dates_count AS (\n",
    "    SELECT COUNT(DISTINCT verified_date) AS days_count\n",
    "    FROM data\n",
    ")\n",
    ", aggregated_data AS (\n",
    "select \n",
    "    udc.days_count as days_count\n",
    "    , 'Overall' as overall\n",
    "    , a.product_portfolio\n",
    "    , a.primary_product\n",
    "    , a.platform\n",
    "    , a.region\n",
    "    , a.document_country\n",
    "    , a.geoip_country\n",
    "    , a.industry\n",
    "    , a.customer\n",
    "    , a.customer_top_20\n",
    "    , a.customer_priority\n",
    "    , COALESCE(COUNT(DISTINCT a.verification_session_uuid),0) as volume\n",
    "    , COALESCE(COUNT(DISTINCT a.qa_verification_session_uuid),0) as qa_reviewed\n",
    "    --from total population\n",
    "    , SUM(a.is_full_auto_session::int) as full_auto_sessions\n",
    "    , SUM(a.is_decline::int) as declined_sessions\n",
    "    , SUM(a.is_approve::int) as approved_sessions\n",
    "    , SUM(a.is_declined_due_to_fraud::int) as declined_due_to_fraud_sessions\n",
    "    --from QA \n",
    "    , SUM(a.flag_session_has_decision_error::int) as decision_error_sessions\n",
    "    , SUM(a.flag_session_has_extraction_error::int) as extraction_error_sessions\n",
    "    , SUM(a.flag_session_has_critical_approve_error::int) as false_approves\n",
    "    , SUM(a.flag_session_has_critical_decline_error::int) as false_declines\n",
    "    , SUM(a.is_session_approved_fraud::int) as missed_fraud_sessions\n",
    "    , SUM(a.qa_declinable::int) as declinable_sessions\n",
    "    , SUM(a.qa_approvable::int) as approvable_sessions\n",
    "    , SUM(a.is_session_fraud::int) as fraud_sessions\n",
    "    , 1.00 * full_auto_sessions / NULLIF(volume, 0) as automation_rate \n",
    "    , 1.00 * declined_sessions / NULLIF(volume, 0) as decline_rate_in_population\n",
    "    , 1.00 * approved_sessions / NULLIF(volume, 0) as approve_rate_in_population\n",
    "    , 1.00 * declined_due_to_fraud_sessions / NULLIF(volume, 0) as fraud_rate_in_population\n",
    "    , 1.00 * decision_error_sessions / NULLIF(qa_reviewed, 0) as decision_error_rate\n",
    "    , 1.00 * extraction_error_sessions / NULLIF(qa_reviewed, 0) as extraction_error_rate\n",
    "    , 1.00 * false_approves / NULLIF(declinable_sessions, 0) as false_approve_rate\n",
    "    , 1.00 * false_declines / NULLIF(approvable_sessions, 0) as false_decline_rate\n",
    "    , 1.00 * missed_fraud_sessions / NULLIF(fraud_sessions, 0) as missed_fraud_rate\n",
    "    , 1.00 * declinable_sessions / NULLIF(qa_reviewed, 0) as decline_rate_in_qa\n",
    "    , 1.00 * approvable_sessions / NULLIF(qa_reviewed, 0) as approve_rate_in_qa\n",
    "    , 1.00 * fraud_sessions / NULLIF(qa_reviewed, 0) as fraud_rate_in_qa\n",
    "    , sum(volume) over () as total_volume\n",
    "    , sum(qa_reviewed) over () as total_qa_reviewed\n",
    "    , 1.00 * volume / NULLIF(total_volume,0) as proportion_of_total_volume\n",
    "    , 1.00 * qa_reviewed / NULLIF(total_qa_reviewed,0) as proportion_of_qa_sample\n",
    "from data a\n",
    "cross join unique_dates_count udc\n",
    "group by 1,2,3,4,5,6,7,8,9,10,11,12\n",
    "order by volume desc\n",
    ")\n",
    "SELECT *\n",
    "from aggregated_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382fb25",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0234103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redshift Connector \n",
    "\n",
    "import redshift_connector\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging for better error visibility\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def get_redshift_dataframe(\n",
    "    sql_query: str,\n",
    "    host: str,\n",
    "    database: str,\n",
    "    user: str,\n",
    "    password: str,\n",
    "    port: int = 5439 # Default Redshift port\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Connects to Amazon Redshift, executes a SQL query, and returns the result\n",
    "    as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        sql_query (str): The SQL query string to execute.\n",
    "        host (str): The Redshift cluster endpoint (e.g., 'your-cluster.xxxx.region.redshift.amazonaws.com').\n",
    "        database (str): The name of the database to connect to.\n",
    "        user (str): The username for Redshift authentication.\n",
    "        password (str): The password for Redshift authentication.\n",
    "        port (int): The port number for the Redshift connection (default is 5439).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the query results.\n",
    "                      Returns an empty DataFrame if an error occurs or no data is found.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Re-raises any connection or query execution errors after logging.\n",
    "    \"\"\"\n",
    "    conn = None # Initialize connection to None\n",
    "    try:\n",
    "        logging.info(f\"Attempting to connect to Redshift database: {database}@{host}:{port} with user: {user}\")\n",
    "        conn = redshift_connector.connect(\n",
    "            host=host,\n",
    "            database=database,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            port=port\n",
    "        )\n",
    "        logging.info(\"Successfully connected to Redshift.\")\n",
    "\n",
    "        logging.info(\"Executing SQL query...\")\n",
    "        # Use pandas read_sql for direct DataFrame conversion\n",
    "        df = pd.read_sql(sql_query, conn)\n",
    "        logging.info(f\"Query executed successfully. Fetched {len(df)} rows.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except redshift_connector.Error as e:\n",
    "        logging.error(f\"Redshift connection or query error: {e}\")\n",
    "        return pd.DataFrame() # Return empty DataFrame on specific Redshift errors\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        # Optionally re-raise the exception if you want calling code to handle it\n",
    "        raise\n",
    "    finally:\n",
    "        if conn:\n",
    "            logging.info(\"Closing Redshift connection.\")\n",
    "            conn.close()\n",
    "        else:\n",
    "            logging.warning(\"Connection was not established, so nothing to close.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b450b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ccafe5",
   "metadata": {},
   "source": [
    "## Run Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1b5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    df = pd.read_csv(file_path)\n",
    "else:\n",
    "    df = get_redshift_dataframe(\n",
    "                sql_query=query, # Pass the multi-line query variable here\n",
    "                host=REDSHIFT_HOST,\n",
    "                database=REDSHIFT_DATABASE,\n",
    "                user=REDSHIFT_USER,\n",
    "                password=REDSHIFT_PASSWORD,\n",
    "                port=REDSHIFT_PORT\n",
    "            )\n",
    "\n",
    "    if not df.empty:\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(\"\\nSuccessfully fetched data from Redshift:\")\n",
    "        print(df.head())\n",
    "        print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "    else:\n",
    "        print(\"\\nNo data fetched or an error occurred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b103a68",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34e32c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN CODE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "import math\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from typing import List, Dict, Any, Union, Tuple, Callable\n",
    "\n",
    "np.random.seed(20)\n",
    "\n",
    "# Suppress all warnings for cleaner output during execution.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration Parameters (Default Values for UI) ---\n",
    "# These parameters serve as initial default values for the interactive UI.\n",
    "# They will be updated by the UI input and passed via the 'params' dictionary.\n",
    "DEFAULT_SELECTED_METRIC = 'Missed Fraud Rate'      \n",
    "DEFAULT_TARGET_OVERALL_ERROR_PERCENT = 2.0         \n",
    "DEFAULT_TARGET_DIMENSION_ERROR_PERCENT = 2.0       \n",
    "DEFAULT_CONFIDENCE_LEVEL = 0.95                    \n",
    "DEFAULT_REVIEW_COST_PER_SESSION = 0.6       \n",
    "DEFAULT_PREDEFINED_TOTAL_COST_BUDGET = 60000.0   \n",
    "DEFAULT_WEIGHT_FACTOR_SMALL_CUSTOMERS = 10        \n",
    "DEFAULT_SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD = 0.99\n",
    "DEFAULT_ERROR_MARGIN_LOGIC = 'Absolute Error Margin'\n",
    "DEFAULT_ABSOLUTE_ERROR_MARGIN_VALUE = 0.02         \n",
    "DEFAULT_RELATIVE_ERROR_MARGIN_VALUE = 0.5       \n",
    "DEFAULT_MIN_ABSOLUTE_ERROR_MARGIN = 0.02         \n",
    "DEFAULT_REPORTING_DIMENSIONS = ['customer', 'customer_top_20', 'primary_product', 'industry', 'region','document_country', 'platform', 'customer_priority', 'product_portfolio']        \n",
    "DEFAULT_STRATIFY_BY_DIMENSION = ['primary_product', 'industry', 'region', 'platform', 'customer_top_20', 'document_country']\n",
    "DEFAULT_AVAILABLE_PRODUCTS = ['Full Auto IDV (Document Only)', 'Full Auto IDV (Selfie & Document)', 'Document Verification', 'IDV', 'Biometric Authentication', 'Biometric Enrolment', 'Biometric Passive Liveness']\n",
    "DEFAULT_TOP_N = 10                                 \n",
    "DEFAULT_SAVE_PLOTS_TO_HTML = False          \n",
    "DEFAULT_MIN_PER_STRATUM = 1000       \n",
    "\n",
    "# --- Global Variables (will be set by UI or default values in generate_report) ---\n",
    "# These are defined globally for convenience but will be effectively\n",
    "# managed by the 'params' dictionary passed to generate_report.\n",
    "SELECTED_METRIC = DEFAULT_SELECTED_METRIC\n",
    "TARGET_OVERALL_ERROR_PERCENT = DEFAULT_TARGET_OVERALL_ERROR_PERCENT\n",
    "TARGET_DIMENSION_ERROR_PERCENT = DEFAULT_TARGET_DIMENSION_ERROR_PERCENT\n",
    "CONFIDENCE_LEVEL = DEFAULT_CONFIDENCE_LEVEL\n",
    "REVIEW_COST_PER_SESSION = DEFAULT_REVIEW_COST_PER_SESSION\n",
    "PREDEFINED_TOTAL_COST_BUDGET = DEFAULT_PREDEFINED_TOTAL_COST_BUDGET\n",
    "WEIGHT_FACTOR_SMALL_CUSTOMERS = DEFAULT_WEIGHT_FACTOR_SMALL_CUSTOMERS\n",
    "SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD = DEFAULT_SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD\n",
    "ERROR_MARGIN_LOGIC = DEFAULT_ERROR_MARGIN_LOGIC\n",
    "ABSOLUTE_ERROR_MARGIN_VALUE = DEFAULT_ABSOLUTE_ERROR_MARGIN_VALUE\n",
    "RELATIVE_ERROR_MARGIN_VALUE = DEFAULT_RELATIVE_ERROR_MARGIN_VALUE\n",
    "MIN_ABSOLUTE_ERROR_MARGIN = DEFAULT_MIN_ABSOLUTE_ERROR_MARGIN\n",
    "REPORTING_DIMENSIONS = DEFAULT_REPORTING_DIMENSIONS\n",
    "STRATIFY_BY_DIMENSION = DEFAULT_STRATIFY_BY_DIMENSION\n",
    "TOP_N = DEFAULT_TOP_N\n",
    "SAVE_PLOTS_TO_HTML = DEFAULT_SAVE_PLOTS_TO_HTML\n",
    "\n",
    "TARGET_QA_PROPORTIONS = {\n",
    "    'Tier Platinum': 0.20,\n",
    "    'Tier 1': 0.15,\n",
    "    'Tier 2': 0.15,\n",
    "    'Tier 3': 0.15,\n",
    "    'Tier 4': 0.15,\n",
    "    'Tier 5': 0.10,\n",
    "    'UNKNOWN': 0.10\n",
    "}\n",
    "\n",
    "# Pre-calculate Z-score for the given confidence level.\n",
    "# This Z-score is used in confidence interval calculations (e.g., Wilson Score).\n",
    "Z_SCORE = stats.norm.ppf(1 - (1 - CONFIDENCE_LEVEL) / 2)\n",
    "\n",
    "# --- Metric Properties ---\n",
    "# This dictionary defines the columns associated with each metric,\n",
    "# including numerator, denominator, and base for frequency calculations.\n",
    "METRIC_PROPERTIES = {\n",
    "    'Decision Error Rate': {\n",
    "        'numerator_col': 'decision_error_sessions',\n",
    "        'denominator_col': 'qa_reviewed',\n",
    "        'freq_denominator_base_col': 'qa_reviewed',\n",
    "    },\n",
    "    'Extraction Error Rate': {\n",
    "        'numerator_col': 'extraction_error_sessions',\n",
    "        'denominator_col': 'qa_reviewed',\n",
    "        'freq_denominator_base_col': 'qa_reviewed',\n",
    "    },\n",
    "    'False Approve Rate': {\n",
    "        'numerator_col': 'false_approves',\n",
    "        'denominator_col': 'declinable_sessions',\n",
    "        'freq_denominator_base_col': 'qa_reviewed',\n",
    "    },\n",
    "    'False Decline Rate': {\n",
    "        'numerator_col': 'false_declines',\n",
    "        'denominator_col': 'approvable_sessions',\n",
    "        'freq_denominator_base_col': 'qa_reviewed',\n",
    "    },\n",
    "    'Missed Fraud Rate': {\n",
    "        'numerator_col': 'missed_fraud_sessions',\n",
    "        'denominator_col': 'fraud_sessions',\n",
    "        'freq_denominator_base_col': 'qa_reviewed',\n",
    "    },\n",
    "    'automation_rate': {\n",
    "        'numerator_col': 'full_auto_sessions',\n",
    "        'denominator_col': 'volume',\n",
    "        'freq_denominator_base_col': 'volume',\n",
    "    },\n",
    "    'decline_rate_in_population': {\n",
    "        'numerator_col': 'declined_sessions',\n",
    "        'denominator_col': 'volume',\n",
    "        'freq_denominator_base_col': 'volume'\n",
    "    },\n",
    "    'approve_rate_in_population': {\n",
    "        'numerator_col': 'approved_sessions',\n",
    "        'denominator_col': 'volume',\n",
    "        'freq_denominator_base_col': 'volume'\n",
    "    },\n",
    "    'fraud_rate_in_population': {\n",
    "        'numerator_col': 'declined_due_to_fraud_sessions',\n",
    "        'denominator_col': 'volume',\n",
    "        'freq_denominator_base_col': 'volume'\n",
    "    },\n",
    "    'fraud_rate_in_qa': {\n",
    "        'numerator_col': 'fraud_sessions',\n",
    "        'denominator_col': 'qa_reviewed',\n",
    "        'freq_denominator_base_col': 'qa_reviewed'\n",
    "    },\n",
    "    'decline_rate_in_qa': {\n",
    "        'numerator_col': 'declinable_sessions',\n",
    "        'denominator_col': 'qa_reviewed',\n",
    "        'freq_denominator_base_col': 'qa_reviewed'\n",
    "    },\n",
    "    'approve_rate_in_qa': {\n",
    "        'numerator_col': 'approvable_sessions',\n",
    "        'denominator_col': 'qa_reviewed',\n",
    "        'freq_denominator_base_col': 'qa_reviewed'\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- Statistical Calculation Functions ---\n",
    "\n",
    "def assign_true_random_sample(df: pd.DataFrame, total_qa_reviewed: int, random_state: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assigns a truly random sample (proportional to volume) to the DataFrame.\n",
    "    Adds a column 'true_random_qa_reviewed' with the number of QA-reviewed sessions per row.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    df = df.copy()\n",
    "    if df['volume'].sum() == 0:\n",
    "        df['true_random_qa_reviewed'] = 0\n",
    "        return df\n",
    "    probs = df['volume'] / df['volume'].sum()\n",
    "    df['true_random_qa_reviewed'] = np.random.multinomial(total_qa_reviewed, probs)\n",
    "    return df\n",
    "\n",
    "def calculate_adjusted_proportion_agresti_coull(numerator: Union[float, np.ndarray], denominator: Union[float, np.ndarray], z_score: float) -> Union[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates the Agresti-Coull adjusted proportion.\n",
    "    \"\"\"\n",
    "    numerator = np.asarray(numerator).astype(float)\n",
    "    denominator = np.asarray(denominator).astype(float)\n",
    "    \n",
    "    z_squared = z_score**2\n",
    "    adjusted_num = numerator + z_squared / 2\n",
    "    adjusted_den = denominator + z_squared\n",
    "\n",
    "    result = np.where(adjusted_den == 0, 0.0, adjusted_num / adjusted_den)\n",
    "    return result\n",
    "\n",
    "# def calculate_margin_wilson(k: Union[float, np.ndarray], n: Union[float, np.ndarray], z_score: float, min_required_n=None) -> Dict[str, Union[float, np.ndarray]]:\n",
    "#     \"\"\"\n",
    "#     Calculates the error margin for a proportion using the Wilson Score Interval.\n",
    "#     \"\"\"\n",
    "#     k = np.asarray(k).astype(float)\n",
    "#     n = np.asarray(n).astype(float)\n",
    "\n",
    "#     p_hat = np.where(n > 0, k / n, 0.0)\n",
    "\n",
    "#     z_squared = z_score**2\n",
    "    \n",
    "#     denominator_wilson = np.where(n > 0, 1.0 + z_squared / n, np.nan) \n",
    "\n",
    "#     sqrt_content = np.where(n > 0, \n",
    "#                             p_hat * (1 - p_hat) / n + z_squared / (4 * n**2), \n",
    "#                             0.0) \n",
    "\n",
    "#     numerator_wilson_base = np.where(n > 0, p_hat + z_squared / (2 * n), np.nan) \n",
    "\n",
    "#     lower_bound = np.where(\n",
    "#         (denominator_wilson > 0) & np.isfinite(numerator_wilson_base),\n",
    "#         (numerator_wilson_base - z_score * np.sqrt(sqrt_content)) / denominator_wilson,\n",
    "#         np.nan\n",
    "#     )\n",
    "#     upper_bound = np.where(\n",
    "#         (denominator_wilson > 0) & np.isfinite(numerator_wilson_base),\n",
    "#         (numerator_wilson_base + z_score * np.sqrt(sqrt_content)) / denominator_wilson,\n",
    "#         np.nan\n",
    "#     )\n",
    "\n",
    "#     lower_bound = np.maximum(0.0, lower_bound)\n",
    "#     upper_bound = np.minimum(1.0, upper_bound)\n",
    "\n",
    "#     margin_abs = (upper_bound - lower_bound) / 2\n",
    "\n",
    "#     # # If denominator is zero, margin_abs should be np.nan\n",
    "#     # margin_abs = np.where(n == 0, np.nan, margin_abs)\n",
    "#     # # margin_rel: if margin_abs is nan or p_hat==0, set to np.nan\n",
    "#     # margin_rel = np.where((margin_abs == 0) & (p_hat > 0), 0.0, np.where((margin_abs == 0) & (p_hat == 0), np.nan, margin_abs / p_hat))\n",
    "#     # margin_rel = np.where(n == 0, np.nan, margin_rel)\n",
    "\n",
    "#     # return {'margin': margin_abs, 'margin_rel': margin_rel}\n",
    "\n",
    "#     margin_rel = np.where(p_hat > 0, margin_abs / p_hat, np.inf)\n",
    "\n",
    "#     # --- PATCH: If n < min_required_n, set margin to np.nan or flag ---\n",
    "#     if min_required_n is not None:\n",
    "#         margin_abs = np.where(n < min_required_n, np.nan, margin_abs)\n",
    "#         margin_rel = np.where(n < min_required_n, np.nan, margin_rel)\n",
    "\n",
    "#     margin_abs = np.nan_to_num(margin_abs, nan=0.9, posinf=0.9, neginf=0.9)\n",
    "#     margin_rel = np.where(margin_abs == 0, np.nan, np.nan_to_num(margin_rel, nan=0.9, posinf=0.9, neginf=0.9))\n",
    "\n",
    "#     return {'margin': margin_abs, 'margin_rel': margin_rel}\n",
    "\n",
    "def calculate_margin_wilson(k: Union[float, np.ndarray], n: Union[float, np.ndarray], z_score: float, min_required_n=None) -> Dict[str, Union[float, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Calculates the error margin for a proportion using the Wilson Score Interval.\n",
    "    If denominator is zero, assigns a large error margin (e.g., 0.9).\n",
    "    \"\"\"\n",
    "    k = np.asarray(k).astype(float)\n",
    "    n = np.asarray(n).astype(float)\n",
    "\n",
    "    p_hat = np.where(n > 0, k / n, 0.0)\n",
    "    z_squared = z_score**2\n",
    "\n",
    "    denominator_wilson = np.where(n > 0, 1.0 + z_squared / n, np.nan)\n",
    "    sqrt_content = np.where(n > 0, p_hat * (1 - p_hat) / n + z_squared / (4 * n**2), 0.0)\n",
    "    numerator_wilson_base = np.where(n > 0, p_hat + z_squared / (2 * n), np.nan)\n",
    "\n",
    "    lower_bound = np.where(\n",
    "        (denominator_wilson > 0) & np.isfinite(numerator_wilson_base),\n",
    "        (numerator_wilson_base - z_score * np.sqrt(sqrt_content)) / denominator_wilson,\n",
    "        np.nan\n",
    "    )\n",
    "    upper_bound = np.where(\n",
    "        (denominator_wilson > 0) & np.isfinite(numerator_wilson_base),\n",
    "        (numerator_wilson_base + z_score * np.sqrt(sqrt_content)) / denominator_wilson,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    lower_bound = np.maximum(0.0, lower_bound)\n",
    "    upper_bound = np.minimum(1.0, upper_bound)\n",
    "\n",
    "    margin_abs = (upper_bound - lower_bound) / 2\n",
    "\n",
    "    # If denominator is zero, assign large error margin (e.g., 0.9)\n",
    "    margin_abs = np.where(n == 0, 0.9, margin_abs)\n",
    "    margin_abs = np.nan_to_num(margin_abs, nan=0.9, posinf=0.9, neginf=0.9)\n",
    "\n",
    "    margin_rel = np.where(p_hat > 0, margin_abs / p_hat, 0.9)\n",
    "    margin_rel = np.where(n == 0, 0.9, margin_rel)\n",
    "    margin_rel = np.nan_to_num(margin_rel, nan=0.9, posinf=0.9, neginf=0.9)\n",
    "\n",
    "    # Optionally: If min_required_n is set, set margin to 0.9 if n < min_required_n\n",
    "    if min_required_n is not None:\n",
    "        margin_abs = np.where(n < min_required_n, 0.9, margin_abs)\n",
    "        margin_rel = np.where(n < min_required_n, 0.9, margin_rel)\n",
    "\n",
    "    return {'margin': margin_abs, 'margin_rel': margin_rel}\n",
    "\n",
    "def get_margins_df(df: pd.DataFrame, k_col: str, n_col: str, z_score: float,  min_required_n=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies the Wilson Score interval calculation to DataFrame columns to get error margins.\n",
    "    \"\"\"\n",
    "    k = df[k_col].values\n",
    "    n = df[n_col].values\n",
    "    results = calculate_margin_wilson(k, n, z_score, min_required_n=min_required_n)\n",
    "    \n",
    "    results_df = pd.DataFrame(results, index=df.index)\n",
    "     # Flag problematic rows\n",
    "    results_df['margin_flag'] = np.where(\n",
    "        (df[n_col] == 0) | (np.isnan(results_df['margin'])) | (np.isinf(results_df['margin'])),\n",
    "        'Problematic', 'OK'\n",
    "    )\n",
    "    return pd.concat([df, results_df], axis=1)\n",
    "\n",
    "def calculate_required_sample_size_proportion(population_proportion: float, absolute_margin_of_error: float, confidence_level: float, population_size: int, z_score: float) -> Union[int, float]:\n",
    "    \"\"\"\n",
    "    Calculates the required sample size for estimating a population proportion\n",
    "    with a specified absolute margin of error and confidence level, using the finite population correction factor.\n",
    "    \"\"\"\n",
    "    z = z_score\n",
    "    p = population_proportion\n",
    "    e = absolute_margin_of_error\n",
    "\n",
    "    if p == 0: p = 1e-9\n",
    "    if p == 1: p = 1 - 1e-9\n",
    "\n",
    "    numerator_val = (z**2 * p * (1 - p)) / e**2\n",
    "\n",
    "    if population_size <= 1:\n",
    "        denominator_fpc = 1.0\n",
    "    else:\n",
    "        denominator_fpc = 1.0 + (numerator_val / (population_size - 1))\n",
    "\n",
    "    if denominator_fpc == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    n = numerator_val / denominator_fpc\n",
    "\n",
    "    # --- FIX: handle NaN or inf ---\n",
    "    if not np.isfinite(n) or np.isnan(n):\n",
    "        return float('inf')\n",
    "    return max(1, math.ceil(n))\n",
    "\n",
    "# --- Data Preparation Functions ---\n",
    "\n",
    "def get_tier_weights_by_target_proportion(df, target_proportions):\n",
    "    # Defensive: If 'customer_priority' or 'volume' missing, return all weights as 1\n",
    "    if 'customer_priority' not in df.columns or 'volume' not in df.columns:\n",
    "        # Return a dict with all tiers set to 1.0\n",
    "        return {tier: 1.0 for tier in target_proportions}\n",
    "    # Calculate actual volume proportions\n",
    "    actual = df.groupby('customer_priority')['volume'].sum()\n",
    "    total = actual.sum()\n",
    "    actual_proportions = (actual / total).to_dict()\n",
    "    # Calculate weights as ratio of target to actual\n",
    "    weights = {}\n",
    "    for tier, target in target_proportions.items():\n",
    "        actual_prop = actual_proportions.get(tier, 0.01)  # avoid zero\n",
    "        weights[tier] = target / actual_prop\n",
    "    return weights\n",
    "\n",
    "def calculate_all_rates(df: pd.DataFrame, metric_properties: Dict[str, Dict[str, str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates various rates (e.g., automation rate, error rates) based on the provided DataFrame.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    rates_to_calculate = {\n",
    "        'automation_rate': ('full_auto_sessions', 'volume'),\n",
    "        'decline_rate_in_population': ('declined_sessions', 'volume'),\n",
    "        'approve_rate_in_population': ('approved_sessions', 'volume'),\n",
    "        'fraud_rate_in_population': ('declined_due_to_fraud_sessions', 'volume'),\n",
    "        'decision_error_rate': ('decision_error_sessions', 'qa_reviewed'),\n",
    "        'extraction_error_rate': ('extraction_error_sessions', 'qa_reviewed'),\n",
    "        'false_approve_rate': ('false_approves', 'declinable_sessions'),\n",
    "        'false_decline_rate': ('false_declines', 'approvable_sessions'),\n",
    "        'missed_fraud_rate': ('missed_fraud_sessions', 'fraud_sessions'),\n",
    "        'decline_rate_in_qa': ('declinable_sessions', 'qa_reviewed'),\n",
    "        'approve_rate_in_qa': ('approvable_sessions', 'qa_reviewed'),\n",
    "        'fraud_rate_in_qa': ('fraud_sessions', 'qa_reviewed'),\n",
    "    }\n",
    "\n",
    "    for rate_name, (numerator_col, denominator_col) in rates_to_calculate.items():\n",
    "        if numerator_col in df_copy.columns and denominator_col in df_copy.columns:\n",
    "            df_copy.loc[:, rate_name] = df_copy[numerator_col] / df_copy[denominator_col].replace(0, np.nan)\n",
    "        else:\n",
    "            warnings.warn(f\"Columns '{numerator_col}' or '{denominator_col}' not found for calculating '{rate_name}'. Skipping rate calculation.\")\n",
    "            df_copy.loc[:, rate_name] = np.nan\n",
    "            \n",
    "    df_copy.fillna(0, inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "def aggregate_population_df(df, reporting_dimensions, metric_properties):\n",
    "    agg_dict = {col: 'sum' for props in metric_properties.values() for col in [props['numerator_col'], props['denominator_col'], props['freq_denominator_base_col']] if col in df.columns}\n",
    "    agg_dict['volume'] = 'sum'\n",
    "    return df.groupby(reporting_dimensions, as_index=False).agg(agg_dict)\n",
    "\n",
    "def prepare_population_data(df: pd.DataFrame, selected_metric: str, metric_properties: Dict[str, Dict[str, str]], z_score: float, reporting_dimensions: list = None) -> Tuple[pd.DataFrame, Dict[str, float], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Prepares the aggregated population data by calculating all rates\n",
    "    and global average metrics/frequencies.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Aggregate by reporting dimensions if provided\n",
    "    if reporting_dimensions is not None and len(reporting_dimensions) > 0:\n",
    "        df = aggregate_population_df(df, reporting_dimensions, metric_properties)\n",
    "    df = calculate_all_rates(df, metric_properties)\n",
    "\n",
    "    global_avg_metrics = {}\n",
    "    for metric_name, props in metric_properties.items():\n",
    "        num_col = props['numerator_col']\n",
    "        den_col = props['denominator_col']\n",
    "        if num_col in df.columns and den_col in df.columns:\n",
    "            total_numerator = df[num_col].sum()\n",
    "            total_denominator = df[den_col].sum()\n",
    "            if total_denominator > 0:\n",
    "                global_avg_metrics[metric_name] = calculate_adjusted_proportion_agresti_coull(total_numerator, total_denominator, z_score)\n",
    "            else:\n",
    "                global_avg_metrics[metric_name] = 0\n",
    "        else:\n",
    "            global_avg_metrics[metric_name] = 0\n",
    "\n",
    "    overall_avg_frequencies_in_qa = {}\n",
    "    for metric_name, props in metric_properties.items():\n",
    "        base_col = props['freq_denominator_base_col']\n",
    "        den_col = props['denominator_col']\n",
    "        if base_col in df.columns and den_col in df.columns:\n",
    "            base_col_sum = df[base_col].sum()\n",
    "            denominator_col_sum = df[den_col].sum()\n",
    "            overall_avg_frequencies_in_qa[metric_name] = (denominator_col_sum / base_col_sum) if base_col_sum > 0 else 0\n",
    "        else:\n",
    "            overall_avg_frequencies_in_qa[metric_name] = 0\n",
    "\n",
    "    return df, global_avg_metrics, overall_avg_frequencies_in_qa\n",
    "\n",
    "def calculate_weighted_metric(df: pd.DataFrame, metric: str, weight_col: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates weighted average for a metric using provided weights.\n",
    "    \"\"\"\n",
    "    props = METRIC_PROPERTIES[metric]\n",
    "    num = df[props['numerator_col']]\n",
    "    den = df[props['denominator_col']]\n",
    "    weights = df[weight_col]\n",
    "    weighted_num = (num * weights).sum()\n",
    "    weighted_den = (den * weights).sum()\n",
    "    return weighted_num / weighted_den if weighted_den > 0 else np.nan\n",
    "\n",
    "# def prepare_population_data(df: pd.DataFrame, selected_metric: str, metric_properties: Dict[str, Dict[str, str]], z_score: float) -> Tuple[pd.DataFrame, Dict[str, float], Dict[str, float]]:\n",
    "#     \"\"\"\n",
    "#     Prepares the aggregated population data by calculating all rates\n",
    "#     and global average metrics/frequencies.\n",
    "#     \"\"\"\n",
    "#     df = df.copy()\n",
    "#     df = df.\n",
    "#     df = calculate_all_rates(df, metric_properties)\n",
    "\n",
    "#     global_avg_metrics = {}\n",
    "#     for metric_name, props in metric_properties.items():\n",
    "#         num_col = props['numerator_col']\n",
    "#         den_col = props['denominator_col']\n",
    "#         if num_col in df.columns and den_col in df.columns:\n",
    "#             total_numerator = df[num_col].sum()\n",
    "#             total_denominator = df[den_col].sum()\n",
    "#             if total_denominator > 0:\n",
    "#                 global_avg_metrics[metric_name] = calculate_adjusted_proportion_agresti_coull(total_numerator, total_denominator, z_score)\n",
    "#             else:\n",
    "#                 global_avg_metrics[metric_name] = 0\n",
    "#         else:\n",
    "#             global_avg_metrics[metric_name] = 0\n",
    "    \n",
    "#     overall_avg_frequencies_in_qa = {}\n",
    "#     for metric_name, props in metric_properties.items():\n",
    "#         base_col = props['freq_denominator_base_col']\n",
    "#         den_col = props['denominator_col']\n",
    "#         if base_col in df.columns and den_col in df.columns:\n",
    "#             base_col_sum = df[base_col].sum()\n",
    "#             denominator_col_sum = df[den_col].sum()\n",
    "#             overall_avg_frequencies_in_qa[metric_name] = (denominator_col_sum / base_col_sum) if base_col_sum > 0 else 0\n",
    "#         else:\n",
    "#             overall_avg_frequencies_in_qa[metric_name] = 0\n",
    "    \n",
    "#     return df, global_avg_metrics, overall_avg_frequencies_in_qa\n",
    "\n",
    "def get_frequency_of_denominator(df: pd.DataFrame, metric: str, metric_properties: Dict[str, Dict[str, str]], overall_avg_frequencies_in_qa: Dict[str, float]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the frequency of the denominator for a given metric within the DataFrame.\n",
    "    \"\"\"\n",
    "    props = metric_properties[metric]\n",
    "    freq_base_col = props['freq_denominator_base_col']\n",
    "    den_col = props['denominator_col']\n",
    "\n",
    "    if freq_base_col not in df.columns or den_col not in df.columns:\n",
    "        warnings.warn(f\"Frequency base column '{freq_base_col}' or denominator column '{den_col}' not found in DataFrame for metric '{metric}'. Using global frequency as fallback for all rows.\")\n",
    "        return np.full(len(df), overall_avg_frequencies_in_qa.get(metric, 0))\n",
    "\n",
    "    freq = df[den_col] / df[freq_base_col].replace(0, np.nan)\n",
    "    freq = freq.fillna(0.0)\n",
    "\n",
    "    MIN_FREQ = 0.01  # Set a reasonable minimum frequency (e.g., 1%)\n",
    "    global_freq_val = max(overall_avg_frequencies_in_qa.get(metric, MIN_FREQ), MIN_FREQ)\n",
    "    freq = np.where(freq < MIN_FREQ, global_freq_val, freq)\n",
    "    freq = np.maximum(freq, MIN_FREQ)\n",
    "\n",
    "    return freq\n",
    "\n",
    "\n",
    "# def calculate_required_sessions(\n",
    "#     df: pd.DataFrame,\n",
    "#     metric: str,\n",
    "#     global_rates: Dict[str, float],\n",
    "#     global_freq: Dict[str, float],\n",
    "#     error_margin_logic: str,\n",
    "#     absolute_error_margin_value: float,\n",
    "#     relative_error_margin_value: float,\n",
    "#     min_absolute_error_margin: float,\n",
    "#     confidence_level: float,\n",
    "#     z_score: float\n",
    "# ) -> Tuple[List[Union[int, float]], List[Union[int, float]], np.ndarray]:\n",
    "#     \"\"\"\n",
    "#     Calculates the required QA reviewed sessions for each group (row in df) to achieve\n",
    "#     the target error margin for the selected metric.\n",
    "#     If denominator is zero, estimate denominator as group volume * global_freq,\n",
    "#     and numerator as estimated_denominator * global_rate.\n",
    "#     Returns: (required_sample_size, extra_reviews, adj_error_margin)\n",
    "#     \"\"\"\n",
    "#     props = METRIC_PROPERTIES[metric]\n",
    "#     num_col = props['numerator_col']\n",
    "#     den_col = props['denominator_col']\n",
    "#     freq_base_col = props['freq_denominator_base_col']\n",
    "\n",
    "#     if den_col not in df.columns or num_col not in df.columns:\n",
    "#         warnings.warn(f\"Numerator '{num_col}' or Denominator '{den_col}' column not found in input DataFrame for `calculate_required_sessions`. Cannot proceed.\")\n",
    "#         return [float('inf')] * len(df), [float('inf')] * len(df), np.full(len(df), np.nan)\n",
    "\n",
    "#     denominator_n_series = df[den_col].astype(float)\n",
    "#     numerator_series = df[num_col].astype(float)\n",
    "#     volume_series = df['volume'].astype(float) if 'volume' in df.columns else denominator_n_series\n",
    "\n",
    "#     global_rate = global_rates.get(metric, 0)\n",
    "#     global_freq_val = global_freq.get(metric, 0)\n",
    "\n",
    "#     adjusted_p_estimates = []\n",
    "#     population_size_series = []\n",
    "\n",
    "#     for i in range(len(df)):\n",
    "#         if denominator_n_series.iloc[i] == 0:\n",
    "#             safe_freq = max(global_freq_val, 0.01)\n",
    "#             est_denominator = max(1, int(volume_series.iloc[i] * safe_freq))\n",
    "#             est_numerator = int(est_denominator * global_rate)\n",
    "#             p_val = global_rate\n",
    "#             group_population_size = volume_series.iloc[i]\n",
    "#         else:\n",
    "#             est_denominator = denominator_n_series.iloc[i]\n",
    "#             est_numerator = numerator_series.iloc[i]\n",
    "#             p_val = calculate_adjusted_proportion_agresti_coull(est_numerator, est_denominator, z_score)\n",
    "#             group_population_size = volume_series.iloc[i]\n",
    "\n",
    "#         adjusted_p_estimates.append(p_val)\n",
    "#         population_size_series.append(group_population_size)\n",
    "\n",
    "#     adjusted_p_estimates = np.array(adjusted_p_estimates)\n",
    "#     population_size_series = np.array(population_size_series)\n",
    "\n",
    "#     if error_margin_logic == 'Relative Error Margin':\n",
    "#         adj_error_margin = np.maximum(relative_error_margin_value * adjusted_p_estimates, min_absolute_error_margin)\n",
    "#     else:\n",
    "#         adj_error_margin = np.full_like(adjusted_p_estimates, max(absolute_error_margin_value, min_absolute_error_margin))\n",
    "\n",
    "#     req_denom_sessions = []\n",
    "#     for i in range(len(df)):\n",
    "#         p_val = adjusted_p_estimates[i]\n",
    "#         e_val = adj_error_margin[i]\n",
    "#         group_population_size = population_size_series[i]\n",
    "#         req_denom_sessions.append(\n",
    "#             calculate_required_sample_size_proportion(\n",
    "#                 p_val, e_val, confidence_level, group_population_size, z_score\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     freq = get_frequency_of_denominator(df, metric, METRIC_PROPERTIES, global_freq)\n",
    "\n",
    "#     if freq_base_col not in df.columns:\n",
    "#         warnings.warn(f\"Warning: '{freq_base_col}' (frequency base column) not found in DataFrame for current reviews. Assuming 0 for all current reviews.\")\n",
    "#         current_qa_reviewed_sum = np.zeros(len(df))\n",
    "#     else:\n",
    "#         current_qa_reviewed_sum = df[freq_base_col].values\n",
    "\n",
    "#     req_sample_size = []\n",
    "#     extra_reviews = []\n",
    "\n",
    "#     for r_denom, f_val, c_reviews in zip(req_denom_sessions, freq, current_qa_reviewed_sum):\n",
    "#         if not np.isfinite(r_denom) or not np.isfinite(f_val) or f_val <= 0:\n",
    "#             req_qa_sessions = float('inf') if r_denom > 0 else 0\n",
    "#         else:\n",
    "#             req_qa_sessions = math.ceil(r_denom / f_val)\n",
    "\n",
    "#         if np.isinf(req_qa_sessions):\n",
    "#             req_sample_size.append(np.inf)\n",
    "#             extra_reviews.append(np.inf)\n",
    "#         else:\n",
    "#             req_sample_size.append(max(1, int(req_qa_sessions)))\n",
    "#             extra_reviews.append(max(0, req_sample_size[-1] - c_reviews))\n",
    "\n",
    "#     return req_sample_size, extra_reviews, adj_error_margin\n",
    "\n",
    "def calculate_required_sessions(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    global_rates: Dict[str, float],\n",
    "    global_freq: Dict[str, float],\n",
    "    error_margin_logic: str,\n",
    "    absolute_error_margin_value: float,\n",
    "    relative_error_margin_value: float,\n",
    "    min_absolute_error_margin: float,\n",
    "    confidence_level: float,\n",
    "    z_score: float\n",
    ") -> Tuple[List[Union[int, float]], List[Union[int, float]], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates the required QA reviewed sessions for each group (row in df) to achieve\n",
    "    the target error margin for the selected metric.\n",
    "    If denominator is zero, numerator is zero, or metric rate is very small (<=0.001),\n",
    "    estimate denominator as group volume * global_freq, and numerator as estimated_denominator * global_rate.\n",
    "    Returns: (required_sample_size, extra_reviews, adj_error_margin)\n",
    "    \"\"\"\n",
    "    props = METRIC_PROPERTIES[metric]\n",
    "    num_col = props['numerator_col']\n",
    "    den_col = props['denominator_col']\n",
    "    freq_base_col = props['freq_denominator_base_col']\n",
    "\n",
    "    if den_col not in df.columns or num_col not in df.columns:\n",
    "        warnings.warn(f\"Numerator '{num_col}' or Denominator '{den_col}' column not found in input DataFrame for `calculate_required_sessions`. Cannot proceed.\")\n",
    "        return [float('inf')] * len(df), [float('inf')] * len(df), np.full(len(df), np.nan)\n",
    "\n",
    "    denominator_n_series = df[den_col].astype(float)\n",
    "    numerator_series = df[num_col].astype(float)\n",
    "    volume_series = df['volume'].astype(float) if 'volume' in df.columns else denominator_n_series\n",
    "\n",
    "    global_rate = global_rates.get(metric, 0)\n",
    "    global_freq_val = global_freq.get(metric, 0)\n",
    "\n",
    "    adjusted_p_estimates = []\n",
    "    population_size_series = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        den = denominator_n_series.iloc[i]\n",
    "        num = numerator_series.iloc[i]\n",
    "        vol = volume_series.iloc[i]\n",
    "        # Calculate observed metric rate\n",
    "        metric_rate = num / den if den > 0 else 0\n",
    "\n",
    "        # Use global approximation if no data or metric rate is very small\n",
    "        if (den == 0) or (num == 0) or (metric_rate <= 0.001):\n",
    "            safe_freq = max(global_freq_val, 0.01)\n",
    "            est_denominator = max(1, int(vol * safe_freq))\n",
    "            est_numerator = int(est_denominator * global_rate)\n",
    "            p_val = global_rate\n",
    "            group_population_size = vol\n",
    "        else:\n",
    "            est_denominator = den\n",
    "            est_numerator = num\n",
    "            p_val = calculate_adjusted_proportion_agresti_coull(est_numerator, est_denominator, z_score)\n",
    "            group_population_size = vol\n",
    "\n",
    "        adjusted_p_estimates.append(p_val)\n",
    "        population_size_series.append(group_population_size)\n",
    "\n",
    "    adjusted_p_estimates = np.array(adjusted_p_estimates)\n",
    "    population_size_series = np.array(population_size_series)\n",
    "\n",
    "    if error_margin_logic == 'Relative Error Margin':\n",
    "        adj_error_margin = np.maximum(relative_error_margin_value * adjusted_p_estimates, min_absolute_error_margin)\n",
    "    else:\n",
    "        adj_error_margin = np.full_like(adjusted_p_estimates, max(absolute_error_margin_value, min_absolute_error_margin))\n",
    "\n",
    "    req_denom_sessions = []\n",
    "    for i in range(len(df)):\n",
    "        p_val = adjusted_p_estimates[i]\n",
    "        e_val = adj_error_margin[i]\n",
    "        group_population_size = population_size_series[i]\n",
    "        req_denom_sessions.append(\n",
    "            calculate_required_sample_size_proportion(\n",
    "                p_val, e_val, confidence_level, group_population_size, z_score\n",
    "            )\n",
    "        )\n",
    "\n",
    "    freq = get_frequency_of_denominator(df, metric, METRIC_PROPERTIES, global_freq)\n",
    "\n",
    "    if freq_base_col not in df.columns:\n",
    "        warnings.warn(f\"Warning: '{freq_base_col}' (frequency base column) not found in DataFrame for current reviews. Assuming 0 for all current reviews.\")\n",
    "        current_qa_reviewed_sum = np.zeros(len(df))\n",
    "    else:\n",
    "        current_qa_reviewed_sum = df[freq_base_col].values\n",
    "\n",
    "    req_sample_size = []\n",
    "    extra_reviews = []\n",
    "\n",
    "    for r_denom, f_val, c_reviews in zip(req_denom_sessions, freq, current_qa_reviewed_sum):\n",
    "        if not np.isfinite(r_denom) or not np.isfinite(f_val) or f_val <= 0:\n",
    "            req_qa_sessions = float('inf') if r_denom > 0 else 0\n",
    "        else:\n",
    "            req_qa_sessions = math.ceil(r_denom / f_val)\n",
    "\n",
    "        if np.isinf(req_qa_sessions):\n",
    "            req_sample_size.append(np.inf)\n",
    "            extra_reviews.append(np.inf)\n",
    "        else:\n",
    "            req_sample_size.append(max(1, int(req_qa_sessions)))\n",
    "            extra_reviews.append(max(0, req_sample_size[-1] - c_reviews))\n",
    "\n",
    "    return req_sample_size, extra_reviews, adj_error_margin\n",
    "\n",
    "\n",
    "# def calculate_sample_counts(population_df: pd.DataFrame, total_target_sessions: int, groupby_dimensions: List[str],\n",
    "#                             sampling_type: str, exponent: float, volume_col_for_sampling: str,\n",
    "#                             selected_metric: str, global_rates: Dict[str, float], overall_avg_frequencies_in_qa: Dict[str, float],\n",
    "#                             z_score: float, min_volume: int = 100000, min_den_n: int = 100, min_required_n_for_margin: int = 10) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Simulates sampling sessions from aggregated population data and calculates\n",
    "#     simulated numerator, denominator counts, and error margins for the SELECTED_METRIC.\n",
    "    \n",
    "#     New Params:\n",
    "#     - min_volume: Minimum original volume for a group to be included (filters small groups upstream).\n",
    "#     - min_den_n: Minimum simulated_den_n for viability (used in fallback and post-filter).\n",
    "#     - min_required_n_for_margin: Passed to calculate_margin_wilson to force high margin if n < this.\n",
    "#     \"\"\"\n",
    "#     props = METRIC_PROPERTIES[selected_metric]\n",
    "#     num_col_original = props['numerator_col']\n",
    "#     den_col_original = props['denominator_col']\n",
    "\n",
    "#     df_for_sim = population_df.copy()\n",
    "\n",
    "#     if num_col_original not in df_for_sim.columns or den_col_original not in df_for_sim.columns:\n",
    "#         warnings.warn(f\"Original numerator '{num_col_original}' or denominator '{den_col_original}' column not found for selected metric '{selected_metric}'. Simulation cannot proceed accurately.\")\n",
    "#         empty_cols = list(set(groupby_dimensions + ['simulated_total_sessions', 'simulated_num_x', 'simulated_den_n', 'margin', 'margin_rel']))\n",
    "#         return pd.DataFrame(columns=empty_cols)\n",
    "\n",
    "#     # --- NEW: Pre-filter small groups based on original volume to avoid simulating them ---\n",
    "#     if 'volume' in df_for_sim.columns:\n",
    "#         df_for_sim = df_for_sim[df_for_sim['volume'] >= min_volume].copy()\n",
    "#         if df_for_sim.empty:\n",
    "#             warnings.warn(\"All groups filtered out due to min_volume threshold. Returning empty DataFrame.\")\n",
    "#             return pd.DataFrame(columns=groupby_dimensions + ['simulated_total_sessions', 'simulated_num_x', 'simulated_den_n', 'margin', 'margin_rel'])\n",
    "\n",
    "#     df_for_sim['original_metric_rate'] = df_for_sim[num_col_original] / df_for_sim[den_col_original].replace(0, np.nan)\n",
    "    \n",
    "#     # --- IMPROVED: Enhance fallback to use global rate if original den < min_den_n (prevents unreliable small-n rates) ---\n",
    "#     is_small_den = (df_for_sim[den_col_original] < min_den_n) | (df_for_sim[den_col_original] == 0) | (df_for_sim['original_metric_rate'].fillna(0) == 0)\n",
    "#     df_for_sim['original_metric_rate'] = np.where(\n",
    "#         is_small_den,\n",
    "#         global_rates[selected_metric],\n",
    "#         calculate_adjusted_proportion_agresti_coull(df_for_sim[num_col_original], df_for_sim[den_col_original], z_score)\n",
    "#     )\n",
    "\n",
    "#     df_for_sim['original_freq_den'] = get_frequency_of_denominator(df_for_sim, selected_metric, METRIC_PROPERTIES, overall_avg_frequencies_in_qa)\n",
    "    \n",
    "#     # Simulation logic (unchanged)\n",
    "#     if sampling_type == 'random':\n",
    "#         total_sampling_volume = df_for_sim[volume_col_for_sampling].sum()\n",
    "#         if total_sampling_volume > 0:\n",
    "#             df_for_sim['simulated_total_sessions'] = (df_for_sim[volume_col_for_sampling] / total_sampling_volume * total_target_sessions)\n",
    "#         else:\n",
    "#             df_for_sim['simulated_total_sessions'] = 0 \n",
    "#     elif sampling_type == 'biased':\n",
    "#         df_for_sim['rand'] = np.random.random(len(df_for_sim))\n",
    "#         safe_volume = np.maximum(df_for_sim[volume_col_for_sampling], 1e-9) \n",
    "        \n",
    "#         df_for_sim['bias_score'] = -np.log(df_for_sim['rand']) * (safe_volume ** exponent)\n",
    "        \n",
    "#         total_bias_score = df_for_sim['bias_score'].sum()\n",
    "#         if total_bias_score > 0:\n",
    "#             df_for_sim['simulated_total_sessions'] = (df_for_sim['bias_score'] / total_bias_score * total_target_sessions)\n",
    "#         else:\n",
    "#             df_for_sim['simulated_total_sessions'] = 0\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown sampling_type: {sampling_type}. Must be 'random' or 'biased'.\")\n",
    "    \n",
    "#     df_for_sim['simulated_total_sessions'] = df_for_sim['simulated_total_sessions'].round().astype(int)\n",
    "    \n",
    "#     df_for_sim['simulated_den_n'] = (df_for_sim['simulated_total_sessions'] * df_for_sim['original_freq_den'])\n",
    "#     df_for_sim['simulated_den_n'] = df_for_sim['simulated_den_n'].round().astype(int)\n",
    "\n",
    "#     df_for_sim['simulated_num_x'] = (df_for_sim['simulated_den_n'] * df_for_sim['original_metric_rate'])\n",
    "#     df_for_sim['simulated_num_x'] = df_for_sim['simulated_num_x'].round().astype(int)\n",
    "\n",
    "#     # --- IMPROVED: Set fallback if simulated_den_n < min_den_n (catches post-simulation small groups) ---\n",
    "#     df_for_sim['used_global_rate_fallback'] = (\n",
    "#         (df_for_sim['simulated_den_n'] < min_den_n) |\n",
    "#         (df_for_sim['simulated_den_n'] == 0) |\n",
    "#         (df_for_sim['original_metric_rate'] == global_rates[selected_metric])\n",
    "#     )\n",
    "\n",
    "#     valid_groupby_dimensions = [dim for dim in groupby_dimensions if dim in df_for_sim.columns]\n",
    "\n",
    "#     if valid_groupby_dimensions:\n",
    "#         # --- NEW: Propagate 'volume' in aggregation for downstream filtering ---\n",
    "#         agg_dict = {\n",
    "#             'simulated_total_sessions': ('simulated_total_sessions', 'sum'),\n",
    "#             'simulated_num_x': ('simulated_num_x', 'sum'),\n",
    "#             'simulated_den_n': ('simulated_den_n', 'sum'),\n",
    "#             'volume': ('volume', 'sum')  # Propagate total original volume for the group\n",
    "#         }\n",
    "#         sim_agg_df = df_for_sim.groupby(valid_groupby_dimensions).agg(**agg_dict).reset_index()\n",
    "#     else:\n",
    "#         sim_agg_df = pd.DataFrame([{\n",
    "#             'simulated_total_sessions': df_for_sim['simulated_total_sessions'].sum(),\n",
    "#             'simulated_num_x': df_for_sim['simulated_num_x'].sum(),\n",
    "#             'simulated_den_n': df_for_sim['simulated_den_n'].sum(),\n",
    "#             'volume': df_for_sim['volume'].sum()  # Propagate total volume\n",
    "#         }])\n",
    "#         for dim in REPORTING_DIMENSIONS + ['primary_product', 'customer']:\n",
    "#             if dim not in sim_agg_df.columns:\n",
    "#                 sim_agg_df[dim] = 'Overall'\n",
    "\n",
    "#     sim_agg_df['simulated_num_x'] = np.maximum(sim_agg_df['simulated_num_x'], 0)\n",
    "#     sim_agg_df['simulated_den_n'] = np.maximum(sim_agg_df['simulated_den_n'], 0)\n",
    "\n",
    "#     # --- NEW: Post-aggregation filter: Drop aggregates with low simulated_den_n or volume ---\n",
    "#     if 'volume' in sim_agg_df.columns and 'simulated_den_n' in sim_agg_df.columns:\n",
    "#         sim_agg_df = sim_agg_df[(sim_agg_df['volume'] >= min_volume) & (sim_agg_df['simulated_den_n'] >= min_den_n)].copy()\n",
    "#         if sim_agg_df.empty:\n",
    "#             warnings.warn(\"All aggregated groups filtered out due to min_volume or min_den_n thresholds. Returning empty DataFrame.\")\n",
    "#             return pd.DataFrame(columns=groupby_dimensions + ['simulated_total_sessions', 'simulated_num_x', 'simulated_den_n', 'margin', 'margin_rel'])\n",
    "\n",
    "#     # --- IMPROVED: Pass min_required_n to enforce in margin calculation ---\n",
    "#     sim_agg_df = get_margins_df(sim_agg_df, 'simulated_num_x', 'simulated_den_n', z_score, min_required_n=min_required_n_for_margin)\n",
    "    \n",
    "#     return sim_agg_df\n",
    "\n",
    "def calculate_sample_counts(population_df: pd.DataFrame, total_target_sessions: int, groupby_dimensions: List[str],\n",
    "                            sampling_type: str, exponent: float, volume_col_for_sampling: str,\n",
    "                            selected_metric: str, global_rates: Dict[str, float], overall_avg_frequencies_in_qa: Dict[str, float],\n",
    "                            z_score: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulates sampling sessions from aggregated population data and calculates\n",
    "    simulated numerator, denominator counts, and error margins for the SELECTED_METRIC.\n",
    "    \"\"\"\n",
    "    props = METRIC_PROPERTIES[selected_metric]\n",
    "    num_col_original = props['numerator_col']\n",
    "    den_col_original = props['denominator_col']\n",
    "\n",
    "    df_for_sim = population_df.copy()\n",
    "\n",
    "    if num_col_original not in df_for_sim.columns or den_col_original not in df_for_sim.columns:\n",
    "        warnings.warn(f\"Original numerator '{num_col_original}' or denominator '{den_col_original}' column not found for selected metric '{selected_metric}'. Simulation cannot proceed accurately.\")\n",
    "        empty_cols = list(set(groupby_dimensions + ['simulated_total_sessions', 'simulated_num_x', 'simulated_den_n', 'margin', 'margin_rel']))\n",
    "        return pd.DataFrame(columns=empty_cols)\n",
    "\n",
    "    df_for_sim['original_metric_rate'] = df_for_sim[num_col_original] / df_for_sim[den_col_original].replace(0, np.nan)\n",
    "    df_for_sim['original_metric_rate'] = np.where(\n",
    "        (df_for_sim['original_metric_rate'].fillna(0) == 0) | (df_for_sim[den_col_original] == 0),\n",
    "        global_rates[selected_metric],\n",
    "        calculate_adjusted_proportion_agresti_coull(df_for_sim[num_col_original], df_for_sim[den_col_original], z_score)\n",
    "    )\n",
    "\n",
    "    df_for_sim['original_freq_den'] = get_frequency_of_denominator(df_for_sim, selected_metric, METRIC_PROPERTIES, overall_avg_frequencies_in_qa)\n",
    "    \n",
    "    if sampling_type == 'random':\n",
    "        total_sampling_volume = df_for_sim[volume_col_for_sampling].sum()\n",
    "        if total_sampling_volume > 0:\n",
    "            df_for_sim['simulated_total_sessions'] = (df_for_sim[volume_col_for_sampling] / total_sampling_volume * total_target_sessions)\n",
    "        else:\n",
    "            df_for_sim['simulated_total_sessions'] = 0 \n",
    "    elif sampling_type == 'biased':\n",
    "        df_for_sim['rand'] = np.random.random(len(df_for_sim))\n",
    "        safe_volume = np.maximum(df_for_sim[volume_col_for_sampling], 1e-9) \n",
    "        df_for_sim['bias_score'] = -np.log(df_for_sim['rand']) * (safe_volume ** exponent)\n",
    "        total_bias_score = df_for_sim['bias_score'].sum()\n",
    "        if total_bias_score > 0:\n",
    "            df_for_sim['simulated_total_sessions'] = (df_for_sim['bias_score'] / total_bias_score * total_target_sessions)\n",
    "        else:\n",
    "            df_for_sim['simulated_total_sessions'] = 0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown sampling_type: {sampling_type}. Must be 'random' or 'biased'.\")\n",
    "\n",
    "    # --- PATCH: Cap simulated_total_sessions to not exceed available volume for each group ---\n",
    "    if 'volume' in df_for_sim.columns:\n",
    "        df_for_sim['simulated_total_sessions'] = np.minimum(df_for_sim['simulated_total_sessions'].round().astype(int), df_for_sim['volume'].astype(int))\n",
    "    else:\n",
    "        df_for_sim['simulated_total_sessions'] = df_for_sim['simulated_total_sessions'].round().astype(int)\n",
    "\n",
    "    df_for_sim['simulated_den_n'] = (df_for_sim['simulated_total_sessions'] * df_for_sim['original_freq_den'])\n",
    "    df_for_sim['simulated_den_n'] = df_for_sim['simulated_den_n'].round().astype(int)\n",
    "\n",
    "    df_for_sim['simulated_num_x'] = (df_for_sim['simulated_den_n'] * df_for_sim['original_metric_rate'])\n",
    "    df_for_sim['simulated_num_x'] = df_for_sim['simulated_num_x'].round().astype(int)\n",
    "\n",
    "    df_for_sim['used_global_rate_fallback'] = (\n",
    "        (df_for_sim['simulated_den_n'] == 0) |\n",
    "        (df_for_sim['original_metric_rate'] == global_rates[selected_metric])\n",
    "    )\n",
    "\n",
    "    valid_groupby_dimensions = [dim for dim in groupby_dimensions if dim in df_for_sim.columns]\n",
    "\n",
    "    if valid_groupby_dimensions:\n",
    "        sim_agg_df = df_for_sim.groupby(valid_groupby_dimensions).agg(\n",
    "            simulated_total_sessions=('simulated_total_sessions', 'sum'),\n",
    "            simulated_num_x=('simulated_num_x', 'sum'),\n",
    "            simulated_den_n=('simulated_den_n', 'sum')\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        sim_agg_df = pd.DataFrame([{\n",
    "            'simulated_total_sessions': df_for_sim['simulated_total_sessions'].sum(),\n",
    "            'simulated_num_x': df_for_sim['simulated_num_x'].sum(),\n",
    "            'simulated_den_n': df_for_sim['simulated_den_n'].sum()\n",
    "        }])\n",
    "        for dim in REPORTING_DIMENSIONS + ['primary_product', 'customer']:\n",
    "            if dim not in sim_agg_df.columns:\n",
    "                sim_agg_df[dim] = 'Overall'\n",
    "\n",
    "    sim_agg_df['simulated_num_x'] = np.maximum(sim_agg_df['simulated_num_x'], 0)\n",
    "    sim_agg_df['simulated_den_n'] = np.maximum(sim_agg_df['simulated_den_n'], 0)\n",
    "\n",
    "    sim_agg_df = get_margins_df(sim_agg_df, 'simulated_num_x', 'simulated_den_n', z_score)\n",
    "    \n",
    "    return sim_agg_df\n",
    "\n",
    "# def calculate_sample_counts(population_df: pd.DataFrame, total_target_sessions: int, groupby_dimensions: List[str],\n",
    "#                             sampling_type: str, exponent: float, volume_col_for_sampling: str,\n",
    "#                             selected_metric: str, global_rates: Dict[str, float], overall_avg_frequencies_in_qa: Dict[str, float],\n",
    "#                             z_score: float) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Simulates sampling sessions from aggregated population data and calculates\n",
    "#     simulated numerator, denominator counts, and error margins for the SELECTED_METRIC.\n",
    "#     \"\"\"\n",
    "#     props = METRIC_PROPERTIES[selected_metric]\n",
    "#     num_col_original = props['numerator_col']\n",
    "#     den_col_original = props['denominator_col']\n",
    "\n",
    "#     df_for_sim = population_df.copy()\n",
    "\n",
    "#     if num_col_original not in df_for_sim.columns or den_col_original not in df_for_sim.columns:\n",
    "#         warnings.warn(f\"Original numerator '{num_col_original}' or denominator '{den_col_original}' column not found for selected metric '{selected_metric}'. Simulation cannot proceed accurately.\")\n",
    "#         empty_cols = list(set(groupby_dimensions + ['simulated_total_sessions', 'simulated_num_x', 'simulated_den_n', 'margin', 'margin_rel']))\n",
    "#         return pd.DataFrame(columns=empty_cols)\n",
    "\n",
    "\n",
    "#     df_for_sim['original_metric_rate'] = df_for_sim[num_col_original] / df_for_sim[den_col_original].replace(0, np.nan)\n",
    "#     df_for_sim['original_metric_rate'] = np.where(\n",
    "#         (df_for_sim['original_metric_rate'].fillna(0) == 0) | (df_for_sim[den_col_original] == 0),\n",
    "#         global_rates[selected_metric],\n",
    "#         calculate_adjusted_proportion_agresti_coull(df_for_sim[num_col_original], df_for_sim[den_col_original], z_score)\n",
    "#     )\n",
    "\n",
    "#     df_for_sim['original_freq_den'] = get_frequency_of_denominator(df_for_sim, selected_metric, METRIC_PROPERTIES, overall_avg_frequencies_in_qa)\n",
    "    \n",
    "#     if sampling_type == 'random':\n",
    "#         total_sampling_volume = df_for_sim[volume_col_for_sampling].sum()\n",
    "#         if total_sampling_volume > 0:\n",
    "#             df_for_sim['simulated_total_sessions'] = (df_for_sim[volume_col_for_sampling] / total_sampling_volume * total_target_sessions)\n",
    "#         else:\n",
    "#             df_for_sim['simulated_total_sessions'] = 0 \n",
    "#     elif sampling_type == 'biased':\n",
    "#         df_for_sim['rand'] = np.random.random(len(df_for_sim))\n",
    "#         safe_volume = np.maximum(df_for_sim[volume_col_for_sampling], 1e-9) \n",
    "        \n",
    "#         df_for_sim['bias_score'] = -np.log(df_for_sim['rand']) * (safe_volume ** exponent)\n",
    "        \n",
    "#         total_bias_score = df_for_sim['bias_score'].sum()\n",
    "#         if total_bias_score > 0:\n",
    "#             df_for_sim['simulated_total_sessions'] = (df_for_sim['bias_score'] / total_bias_score * total_target_sessions)\n",
    "#         else:\n",
    "#             df_for_sim['simulated_total_sessions'] = 0\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown sampling_type: {sampling_type}. Must be 'random' or 'biased'.\")\n",
    "    \n",
    "#     df_for_sim['simulated_total_sessions'] = df_for_sim['simulated_total_sessions'].round().astype(int)\n",
    "    \n",
    "#     df_for_sim['simulated_den_n'] = (df_for_sim['simulated_total_sessions'] * df_for_sim['original_freq_den'])\n",
    "#     df_for_sim['simulated_den_n'] = df_for_sim['simulated_den_n'].round().astype(int)\n",
    "\n",
    "#     df_for_sim['simulated_num_x'] = (df_for_sim['simulated_den_n'] * df_for_sim['original_metric_rate'])\n",
    "#     df_for_sim['simulated_num_x'] = df_for_sim['simulated_num_x'].round().astype(int)\n",
    "\n",
    "#     df_for_sim['used_global_rate_fallback'] = (\n",
    "#     (df_for_sim['simulated_den_n'] == 0) |\n",
    "#     (df_for_sim['original_metric_rate'] == global_rates[selected_metric])\n",
    "#     )\n",
    "\n",
    "#     valid_groupby_dimensions = [dim for dim in groupby_dimensions if dim in df_for_sim.columns]\n",
    "\n",
    "#     if valid_groupby_dimensions:\n",
    "#         sim_agg_df = df_for_sim.groupby(valid_groupby_dimensions).agg(\n",
    "#             simulated_total_sessions=('simulated_total_sessions', 'sum'),\n",
    "#             simulated_num_x=('simulated_num_x', 'sum'),\n",
    "#             simulated_den_n=('simulated_den_n', 'sum')\n",
    "#         ).reset_index()\n",
    "#     else:\n",
    "#         sim_agg_df = pd.DataFrame([{\n",
    "#             'simulated_total_sessions': df_for_sim['simulated_total_sessions'].sum(),\n",
    "#             'simulated_num_x': df_for_sim['simulated_num_x'].sum(),\n",
    "#             'simulated_den_n': df_for_sim['simulated_den_n'].sum()\n",
    "#         }])\n",
    "#         # Add all expected dimensions as 'Overall'\n",
    "#         for dim in REPORTING_DIMENSIONS + ['primary_product', 'customer']:\n",
    "#             if dim not in sim_agg_df.columns:\n",
    "#                 sim_agg_df[dim] = 'Overall'\n",
    "\n",
    "#     # if groupby_dimensions and all(dim in df_for_sim.columns for dim in groupby_dimensions):\n",
    "#     #     sim_agg_df = df_for_sim.groupby(groupby_dimensions).agg(\n",
    "#     #         simulated_total_sessions=('simulated_total_sessions', 'sum'),\n",
    "#     #         simulated_num_x=('simulated_num_x', 'sum'),\n",
    "#     #         simulated_den_n=('simulated_den_n', 'sum')\n",
    "#     #     ).reset_index()\n",
    "#     # else:\n",
    "#     #     sim_agg_df = pd.DataFrame([{\n",
    "#     #         'simulated_total_sessions': df_for_sim['simulated_total_sessions'].sum(),\n",
    "#     #         'simulated_num_x': df_for_sim['simulated_num_x'].sum(),\n",
    "#     #         'simulated_den_n': df_for_sim['simulated_den_n'].sum()\n",
    "#     #     }])\n",
    "#     #     for dim in REPORTING_DIMENSIONS + ['primary_product', 'customer']:\n",
    "#     #         if dim not in sim_agg_df.columns:\n",
    "#     #             sim_agg_df[dim] = 'Overall' \n",
    "\n",
    "#     sim_agg_df['simulated_num_x'] = np.maximum(sim_agg_df['simulated_num_x'], 0)\n",
    "#     sim_agg_df['simulated_den_n'] = np.maximum(sim_agg_df['simulated_den_n'], 0)\n",
    "\n",
    "#     sim_agg_df = get_margins_df(sim_agg_df, 'simulated_num_x', 'simulated_den_n', z_score)\n",
    "    \n",
    "#     return sim_agg_df\n",
    "\n",
    "#--- Helper Functions for Aggregation and Plotting ---\n",
    "\n",
    "def _aggregate_top_n_for_table_and_plot(df: pd.DataFrame, group_col: str, n: int, sort_col: str, \n",
    "                                        value_cols: List[str], rest_agg_logic: Dict[str, str],\n",
    "                                        recalculate_margin_func: Callable[[pd.DataFrame], float]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates data to show top N items by a sorting column, with the rest grouped as \"Rest\".\n",
    "    This version is enhanced to handle specific aggregation types for different columns in the 'Rest' group,\n",
    "    and recalculates the margin for the 'Rest' group from underlying counts for statistical accuracy.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[group_col] + value_cols)\n",
    "\n",
    "    sort_col_effective = sort_col\n",
    "    if sort_col not in df.columns or not pd.api.types.is_numeric_dtype(df[sort_col]):\n",
    "        if 'volume' in df.columns and pd.api.types.is_numeric_dtype(df['volume']):\n",
    "            sort_col_effective = 'volume'\n",
    "            warnings.warn(f\"Sorting column '{sort_col}' not found or not numeric. Falling back to 'volume' for sorting.\")\n",
    "        elif 'simulated_total_sessions' in df.columns and pd.api.types.is_numeric_dtype(df['simulated_total_sessions']):\n",
    "            sort_col_effective = 'simulated_total_sessions'\n",
    "            warnings.warn(f\"Sorting column '{sort_col}' not found or not numeric. Falling back to 'simulated_total_sessions' for sorting.\")\n",
    "        elif group_col in df.columns:\n",
    "            warnings.warn(f\"No suitable numeric sorting column found. Sorting by '{group_col}' (alphabetically).\")\n",
    "            df = df.sort_values(group_col)\n",
    "            sort_col_effective = group_col\n",
    "        else:\n",
    "            warnings.warn(f\"Cannot determine a suitable column for sorting. Returning original DataFrame.\")\n",
    "            return df\n",
    "\n",
    "    if sort_col_effective in df.columns and pd.api.types.is_numeric_dtype(df[sort_col_effective]):\n",
    "        df_sorted = df.sort_values(sort_col_effective, ascending=False).copy()\n",
    "    else:\n",
    "        df_sorted = df.copy()\n",
    "\n",
    "    numeric_value_cols = [col for col in value_cols if col in df_sorted.columns and pd.api.types.is_numeric_dtype(df_sorted[col])]\n",
    "    \n",
    "    for col in numeric_value_cols:\n",
    "        df_sorted[col] = df_sorted[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if len(df_sorted) > n:\n",
    "        top_n_df = df_sorted.head(n).copy()\n",
    "        rest_df = df_sorted.iloc[n:].copy()\n",
    "        rest_row_data = {group_col: 'Rest'}\n",
    "        for col, agg_method in rest_agg_logic.items():\n",
    "            if col in rest_df.columns:\n",
    "                if agg_method == 'sum':\n",
    "                    sum_val = rest_df[col].sum(min_count=1)\n",
    "                    rest_row_data[col] = sum_val if not pd.isna(sum_val) else 0.0\n",
    "                elif agg_method == 'mean':\n",
    "                    rest_row_data[col] = rest_df[col].mean()\n",
    "                elif agg_method == 'recalculate_margin' and recalculate_margin_func is not None:\n",
    "                    rest_row_data[col] = recalculate_margin_func(rest_df)\n",
    "            else:\n",
    "                rest_row_data[col] = 0.0\n",
    "        # Always recalculate margin from aggregate counts\n",
    "        if 'Current Error Margin (%)' in value_cols:\n",
    "            total_k = rest_df['numerator_x'].sum() if 'numerator_x' in rest_df.columns else 0\n",
    "            total_n = rest_df['denominator_n'].sum() if 'denominator_n' in rest_df.columns else 0\n",
    "            rest_row_data['Current Error Margin (%)'] = calculate_margin_wilson(total_k, total_n, Z_SCORE)['margin'] * 100 if total_n > 0 and total_k > 0 else np.nan\n",
    "\n",
    "        if 'Meets Target Margin' in value_cols:\n",
    "            if 'Current Error Margin (%)' in rest_row_data and not pd.isna(rest_row_data['Current Error Margin (%)']):\n",
    "                if ERROR_MARGIN_LOGIC == 'Absolute Error Margin':\n",
    "                    rest_row_data['Meets Target Margin'] = (\n",
    "                        'Yes' if rest_row_data['Current Error Margin (%)'] <= ABSOLUTE_ERROR_MARGIN_VALUE * 100 else 'No'\n",
    "                    )\n",
    "                else:  # Relative Error Margin\n",
    "                    # You need to have the current relative error margin available, e.g. 'Current Relative Error Margin (%)'\n",
    "                    rel_margin = rest_row_data.get('Current Relative Error Margin (%)')\n",
    "                    if rel_margin is not None and not pd.isna(rel_margin):\n",
    "                        rest_row_data['Meets Target Margin'] = (\n",
    "                            'Yes' if rel_margin <= RELATIVE_ERROR_MARGIN_VALUE * 100 else 'No'\n",
    "                        )\n",
    "                    else:\n",
    "                        rest_row_data['Meets Target Margin'] = 'N/A'\n",
    "            else:\n",
    "                rest_row_data['Meets Target Margin'] = 'N/A'\n",
    "\n",
    "        rest_row_df = pd.DataFrame([rest_row_data])\n",
    "        aggregated_df = pd.concat([top_n_df, rest_row_df], ignore_index=True)\n",
    "    else:\n",
    "        aggregated_df = df_sorted\n",
    "    \n",
    "    return aggregated_df\n",
    "\n",
    "def _aggregate_top_n_for_table_and_plot(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    n: int,\n",
    "    sort_col: str,\n",
    "    value_cols: List[str],\n",
    "    rest_agg_logic: Dict[str, str],\n",
    "    recalculate_margin_func: Callable[[pd.DataFrame], float]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates data to show top N items by a sorting column, with the rest grouped as \"Rest\".\n",
    "    Ensures grouping by group_col only, so each group is unique.\n",
    "    Handles 'recalculate_margin' as a post-aggregation step.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[group_col] + value_cols)\n",
    "\n",
    "    # Only use valid pandas agg methods for groupby\n",
    "    valid_agg_dict = {}\n",
    "    postprocess_cols = []\n",
    "    for col in value_cols:\n",
    "        if col == group_col:\n",
    "            continue\n",
    "        agg_method = rest_agg_logic.get(col, 'sum')\n",
    "        if agg_method in ['sum', 'mean', 'min', 'max', 'first', 'last', 'median', 'std', 'var']:\n",
    "            valid_agg_dict[col] = agg_method\n",
    "        elif agg_method == 'recalculate_margin':\n",
    "            # We'll handle this after aggregation\n",
    "            valid_agg_dict[col] = 'sum'  # placeholder, will overwrite later\n",
    "            postprocess_cols.append(col)\n",
    "        else:\n",
    "            valid_agg_dict[col] = 'sum'\n",
    "\n",
    "    grouped = df.groupby(group_col, as_index=False).agg(valid_agg_dict)\n",
    "\n",
    "    # --- Sorting logic ---\n",
    "    sort_col_effective = sort_col\n",
    "    if sort_col not in grouped.columns or not pd.api.types.is_numeric_dtype(grouped[sort_col]):\n",
    "        if 'volume' in grouped.columns and pd.api.types.is_numeric_dtype(grouped['volume']):\n",
    "            sort_col_effective = 'volume'\n",
    "            warnings.warn(f\"Sorting column '{sort_col}' not found or not numeric. Falling back to 'volume' for sorting.\")\n",
    "        elif 'simulated_total_sessions' in grouped.columns and pd.api.types.is_numeric_dtype(grouped['simulated_total_sessions']):\n",
    "            sort_col_effective = 'simulated_total_sessions'\n",
    "            warnings.warn(f\"Sorting column '{sort_col}' not found or not numeric. Falling back to 'simulated_total_sessions' for sorting.\")\n",
    "        elif group_col in grouped.columns:\n",
    "            warnings.warn(f\"No suitable numeric sorting column found. Sorting by '{group_col}' (alphabetically).\")\n",
    "            grouped = grouped.sort_values(group_col)\n",
    "            sort_col_effective = group_col\n",
    "        else:\n",
    "            warnings.warn(f\"Cannot determine a suitable column for sorting. Returning original DataFrame.\")\n",
    "            return grouped\n",
    "\n",
    "    if sort_col_effective in grouped.columns and pd.api.types.is_numeric_dtype(grouped[sort_col_effective]):\n",
    "        df_sorted = grouped.sort_values(sort_col_effective, ascending=False).copy()\n",
    "    else:\n",
    "        df_sorted = grouped.copy()\n",
    "\n",
    "    numeric_value_cols = [col for col in value_cols if col in df_sorted.columns and pd.api.types.is_numeric_dtype(df_sorted[col])]\n",
    "    for col in numeric_value_cols:\n",
    "        df_sorted[col] = df_sorted[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if len(df_sorted) > n:\n",
    "        top_n_df = df_sorted.head(n).copy()\n",
    "        rest_df = df_sorted.iloc[n:].copy()\n",
    "        rest_row_data = {group_col: 'Rest'}\n",
    "        for col, agg_method in rest_agg_logic.items():\n",
    "            if col in rest_df.columns:\n",
    "                if agg_method == 'sum':\n",
    "                    sum_val = rest_df[col].sum(min_count=1)\n",
    "                    rest_row_data[col] = sum_val if not pd.isna(sum_val) else 0.0\n",
    "                elif agg_method == 'mean':\n",
    "                    rest_row_data[col] = rest_df[col].mean()\n",
    "                elif agg_method == 'recalculate_margin' and recalculate_margin_func is not None:\n",
    "                    rest_row_data[col] = recalculate_margin_func(rest_df)\n",
    "            else:\n",
    "                rest_row_data[col] = 0.0\n",
    "        # Post-process any columns that need recalculate_margin\n",
    "        for col in postprocess_cols:\n",
    "            if col in rest_row_data and recalculate_margin_func is not None:\n",
    "                rest_row_data[col] = recalculate_margin_func(rest_df)\n",
    "        rest_row_df = pd.DataFrame([rest_row_data])\n",
    "        aggregated_df = pd.concat([top_n_df, rest_row_df], ignore_index=True)\n",
    "    else:\n",
    "        aggregated_df = df_sorted\n",
    "\n",
    "    return aggregated_df\n",
    "\n",
    "# --- Plotting Functions ---\n",
    "\n",
    "# def create_bar_chart(df: pd.DataFrame, x_col_or_index_values: Union[str, pd.Series, pd.Index], y_cols: List[str], title: str,\n",
    "#                      y_axis_title: str, x_axis_title: str, barmode: str = 'group',\n",
    "#                      line_data: Dict[str, Any] = None, filename_suffix: str = \"\", save_plots_to_html: bool = False):\n",
    "#     \"\"\"\n",
    "#     Generates a Plotly bar chart with optional line data.\n",
    "#     \"\"\"\n",
    "#     fig = go.Figure()\n",
    "\n",
    "#     if isinstance(x_col_or_index_values, str):\n",
    "#         x_values = df[x_col_or_index_values]\n",
    "#         num_categories = len(df[x_col_or_index_values].unique())\n",
    "#     else:\n",
    "#         x_values = x_col_or_index_values\n",
    "#         if isinstance(x_values, pd.Index):\n",
    "#             num_categories = len(x_values.unique())\n",
    "#         else:\n",
    "#             num_categories = len(np.unique(x_values))\n",
    "\n",
    "#     for y_col in y_cols:\n",
    "#         name_label = y_col.replace('_plot', '')\n",
    "#         fig.add_trace(go.Bar(name=name_label, x=x_values, y=df[y_col]))\n",
    "\n",
    "#     if line_data:\n",
    "#         fig.add_shape(type=\"line\", x0=-0.5, y0=line_data['y_value'], x1=num_categories - 0.5, y1=line_data['y_value'],\n",
    "#                       line=dict(color=line_data['color'], width=line_data['width'], dash=line_data['dash']),\n",
    "#                       name=line_data['name'])\n",
    "        \n",
    "#     all_y_values = pd.concat([df[col].replace([np.inf, -np.inf], np.nan).dropna() for col in y_cols if col in df.columns])\n",
    "#     y_max = all_y_values.max() * 1.1 if not all_y_values.empty else 100\n",
    "#     y_min = all_y_values.min() * 0.9 if not all_y_values.empty and all_y_values.min() < 0 else 0\n",
    "    \n",
    "#     for y_col in y_cols:\n",
    "#         if y_col in df.columns and np.isinf(df[y_col]).any():\n",
    "#             inf_rows = df[df[y_col] == np.inf]\n",
    "#             for idx, row in inf_rows.iterrows():\n",
    "#                 x_val_for_annotation = row[x_col_or_index_values] if isinstance(x_col_or_index_values, str) else idx\n",
    "                \n",
    "#                 fig.add_annotation(\n",
    "#                     x=x_val_for_annotation, y=y_max,\n",
    "#                     text=\"Inf.\",\n",
    "#                     showarrow=False,\n",
    "#                     yshift=10,\n",
    "#                     font=dict(color=\"red\", size=10)\n",
    "#                 )\n",
    "\n",
    "#     fig.update_layout(\n",
    "#         title=title,\n",
    "#         yaxis_title=y_axis_title,\n",
    "#         xaxis_title=x_axis_title,\n",
    "#         barmode=barmode,\n",
    "#         xaxis_tickangle=45,\n",
    "#         yaxis_range=[y_min, y_max],\n",
    "#         legend_title_text='Metric'\n",
    "#     )\n",
    "    \n",
    "#     if save_plots_to_html and filename_suffix:\n",
    "#         fig.write_html(f\"{filename_suffix}.html\")\n",
    "#         print(f\"Plot saved to {filename_suffix}.html\")\n",
    "    \n",
    "#     fig.show()\n",
    "\n",
    "def create_bar_chart(\n",
    "    df: pd.DataFrame,\n",
    "    x_col_or_index_values: Union[str, pd.Series, pd.Index],\n",
    "    y_cols: List[str],\n",
    "    title: str,\n",
    "    y_axis_title: str,\n",
    "    x_axis_title: str,\n",
    "    barmode: str = 'group',\n",
    "    line_data: Dict[str, Any] = None,\n",
    "    filename_suffix: str = \"\",\n",
    "    save_plots_to_html: bool = False,\n",
    "    text_labels: Dict[str, List[str]] = None  # <-- NEW ARGUMENT\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a Plotly bar chart with optional line data and value labels.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if isinstance(x_col_or_index_values, str):\n",
    "        x_values = df[x_col_or_index_values]\n",
    "        num_categories = len(df[x_col_or_index_values].unique())\n",
    "    else:\n",
    "        x_values = x_col_or_index_values\n",
    "        if isinstance(x_values, pd.Index):\n",
    "            num_categories = len(x_values.unique())\n",
    "        else:\n",
    "            num_categories = len(np.unique(x_values))\n",
    "\n",
    "    for y_col in y_cols:\n",
    "        name_label = y_col.replace('_plot', '')\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=name_label,\n",
    "            x=x_values,\n",
    "            y=df[y_col],\n",
    "            text=text_labels[y_col] if text_labels and y_col in text_labels else None,\n",
    "            textposition='auto' if text_labels and y_col in text_labels else None\n",
    "        ))\n",
    "\n",
    "    if line_data:\n",
    "        fig.add_shape(type=\"line\", x0=-0.5, y0=line_data['y_value'], x1=num_categories - 0.5, y1=line_data['y_value'],\n",
    "                      line=dict(color=line_data['color'], width=line_data['width'], dash=line_data['dash']),\n",
    "                      name=line_data['name'])\n",
    "\n",
    "    all_y_values = pd.concat([df[col].replace([np.inf, -np.inf], np.nan).dropna() for col in y_cols if col in df.columns])\n",
    "    y_max = all_y_values.max() * 1.1 if not all_y_values.empty else 100\n",
    "    y_min = all_y_values.min() * 0.9 if not all_y_values.empty and all_y_values.min() < 0 else 0\n",
    "\n",
    "    for y_col in y_cols:\n",
    "        if y_col in df.columns and np.isinf(df[y_col]).any():\n",
    "            inf_rows = df[df[y_col] == np.inf]\n",
    "            for idx, row in inf_rows.iterrows():\n",
    "                x_val_for_annotation = row[x_col_or_index_values] if isinstance(x_col_or_index_values, str) else idx\n",
    "                fig.add_annotation(\n",
    "                    x=x_val_for_annotation, y=y_max,\n",
    "                    text=\"Inf.\",\n",
    "                    showarrow=False,\n",
    "                    yshift=10,\n",
    "                    font=dict(color=\"red\", size=10)\n",
    "                )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        yaxis_title=y_axis_title,\n",
    "        xaxis_title=x_axis_title,\n",
    "        barmode=barmode,\n",
    "        xaxis_tickangle=45,\n",
    "        yaxis_range=[y_min, y_max],\n",
    "        legend_title_text='Metric'\n",
    "    )\n",
    "\n",
    "    if save_plots_to_html and filename_suffix:\n",
    "        fig.write_html(f\"{filename_suffix}.html\")\n",
    "        print(f\"Plot saved to {filename_suffix}.html\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def create_heatmap(df: pd.DataFrame, title: str, filename_suffix: str = \"\", save_plots_to_html: bool = False):\n",
    "    \"\"\"\n",
    "    Generates a Plotly heatmap.\n",
    "    \"\"\"\n",
    "    fig = px.imshow(df, text_auto=True, color_continuous_scale='Viridis',\n",
    "                     title=title,\n",
    "                     labels=dict(x='Simulation', y='Dimension', color='Count'))\n",
    "    fig.update_layout(xaxis_tickangle=45)\n",
    "    if save_plots_to_html and filename_suffix:\n",
    "        fig.write_html(f\"{filename_suffix}.html\")\n",
    "        print(f\"Plot saved to {filename_suffix}.html\")\n",
    "    fig.show()\n",
    "\n",
    "def create_combined_bar_line_chart(df: pd.DataFrame, bar_cols: List[str], line_col: str,\n",
    "                                   title: str, y1_axis_title: str, y2_axis_title: str,\n",
    "                                   x_col_or_index_values: Union[str, pd.Series, pd.Index] = None, filename_suffix: str = \"\", save_plots_to_html: bool = False):\n",
    "    \"\"\"\n",
    "    Generates a Plotly chart with bars and an overlaid line.\n",
    "    \"\"\"\n",
    "    if x_col_or_index_values is None:\n",
    "        x_values = df.index\n",
    "    elif isinstance(x_col_or_index_values, str):\n",
    "        x_values = df[x_col_or_index_values]\n",
    "    else:\n",
    "        x_values = x_col_or_index_values\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for b_col in bar_cols:\n",
    "        fig.add_trace(go.Bar(name=b_col.replace('_coverage', ' Coverage'), x=x_values, y=df[b_col]))\n",
    "    \n",
    "    if line_col in df.columns and df[line_col].dropna().any():\n",
    "        fig.add_trace(go.Scatter(name=line_col.replace('_perc', ' (%)').replace('_', ' ').title(),\n",
    "                                 x=x_values, y=df[line_col], mode='lines+markers', yaxis='y2', line=dict(color='orange')))\n",
    "        fig.update_layout(yaxis2=dict(title=y2_axis_title, overlaying='y', side='right'))\n",
    "\n",
    "    fig.update_layout(title=title,\n",
    "                      yaxis=dict(title=y1_axis_title),\n",
    "                      xaxis_title='Simulation',\n",
    "                      xaxis_tickangle=45,\n",
    "                      barmode='group',\n",
    "                      legend_title_text='Metrics & Coverage',\n",
    "                      legend=dict(\n",
    "                        orientation=\"h\",  \n",
    "                        yanchor=\"top\",    \n",
    "                        y=1.08,            \n",
    "                        xanchor=\"center\",\n",
    "                        x=0.5\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    if save_plots_to_html and filename_suffix:\n",
    "        fig.write_html(f\"{filename_suffix}.html\")\n",
    "        print(f\"Plot saved to {filename_suffix}.html\")\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "def display_metric_with_error_bars(\n",
    "    population_df: pd.DataFrame,\n",
    "    selected_metric: str,\n",
    "    metric_properties: Dict[str, Dict[str, str]],\n",
    "    reporting_dimensions: list,\n",
    "    top_n: int,\n",
    "    global_avg_metrics: Dict[str, float],\n",
    "    z_score: float,\n",
    "    save_plots_to_html: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Displays a point plot (scatter) with error bars (confidence intervals) for the metric per group in each reporting dimension.\n",
    "    Error bars are clipped so the lower bound is never below zero.\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    analysis_dimensions = list(set(reporting_dimensions + ['primary_product', 'customer']))\n",
    "\n",
    "    for dim_name in analysis_dimensions:\n",
    "        if dim_name not in population_df.columns:\n",
    "            print(f\"Warning: Dimension '{dim_name}' not found in population data. Skipping visualization for this dimension.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Metric with Error Bars for Dimension: {dim_name} ---\")\n",
    "\n",
    "        num_col = metric_properties[selected_metric]['numerator_col']\n",
    "        den_col = metric_properties[selected_metric]['denominator_col']\n",
    "\n",
    "        if num_col not in population_df.columns or den_col not in population_df.columns:\n",
    "            warnings.warn(f\"Required columns for selected metric not found in DataFrame for dimension '{dim_name}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        current_dim_agg = population_df.groupby(dim_name).agg(\n",
    "            numerator_x=(num_col, 'sum'),\n",
    "            denominator_n=(den_col, 'sum'),\n",
    "            volume_sum=('volume', 'sum')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Calculate metric rate and error margin for each group\n",
    "        current_dim_agg['metric_rate'] = current_dim_agg['numerator_x'] / current_dim_agg['denominator_n'].replace(0, np.nan)\n",
    "        current_dim_agg['metric_rate'] = current_dim_agg['metric_rate'].fillna(global_avg_metrics[selected_metric])\n",
    "\n",
    "        current_margins = get_margins_df(current_dim_agg, 'numerator_x', 'denominator_n', z_score)\n",
    "        current_dim_agg['error_margin'] = current_margins['margin']\n",
    "\n",
    "        # Top N + Rest, recalculate metric and margin for Rest group from summed numerators/denominators\n",
    "        def recalc_metric_and_margin(rest_df):\n",
    "            total_num = rest_df['numerator_x'].sum()\n",
    "            total_den = rest_df['denominator_n'].sum()\n",
    "            metric = total_num / total_den if total_den > 0 else 0\n",
    "            margin = calculate_margin_wilson(total_num, total_den, z_score)['margin']\n",
    "            return pd.Series({'metric_rate': metric, 'error_margin': margin, 'numerator_x': total_num, 'denominator_n': total_den, 'volume_sum': rest_df['volume_sum'].sum()})\n",
    "\n",
    "        if len(current_dim_agg) > top_n:\n",
    "            top_n_df = current_dim_agg.sort_values('volume_sum', ascending=False).head(top_n).copy()\n",
    "            rest_df = current_dim_agg.sort_values('volume_sum', ascending=False).iloc[top_n:]\n",
    "            rest_row = recalc_metric_and_margin(rest_df)\n",
    "            rest_row[dim_name] = 'Rest'\n",
    "            agg_metric_for_plot = pd.concat([top_n_df, pd.DataFrame([rest_row])], ignore_index=True)\n",
    "        else:\n",
    "            agg_metric_for_plot = current_dim_agg.copy()\n",
    "\n",
    "        # Round metrics to 3 decimals\n",
    "        agg_metric_for_plot['metric_rate'] = pd.to_numeric(agg_metric_for_plot['metric_rate'], errors='coerce').round(3)\n",
    "        agg_metric_for_plot['error_margin'] = pd.to_numeric(agg_metric_for_plot['error_margin'], errors='coerce').round(3)\n",
    "        \n",
    "\n",
    "        # # Calculate lower and upper bounds, clip lower at 0 - CHANGE IF YOU WANT TO SHOW DECIMALS\n",
    "        # y = agg_metric_for_plot['metric_rate']\n",
    "        # error = agg_metric_for_plot['error_margin']\n",
    "        # lower = (y - error).clip(lower=0)\n",
    "        # upper = y + error\n",
    "        # error_array = upper - y\n",
    "        # error_array_minus = y - lower\n",
    "\n",
    "        # Convert to percent\n",
    "        agg_metric_for_plot['metric_rate_perc'] = agg_metric_for_plot['metric_rate'] * 100\n",
    "        agg_metric_for_plot['error_margin_perc'] = agg_metric_for_plot['error_margin'] * 100\n",
    "\n",
    "        y = agg_metric_for_plot['metric_rate_perc']\n",
    "        error = agg_metric_for_plot['error_margin_perc']\n",
    "        lower = (y - error).clip(lower=0)\n",
    "        upper = y + error\n",
    "        error_array = upper - y\n",
    "        error_array_minus = y - lower\n",
    "\n",
    "        # Hover text as percent\n",
    "        hover_text = agg_metric_for_plot.apply(\n",
    "            lambda row: f\"{row[dim_name]}<br>{row['metric_rate_perc']:.1f}% ± {row['error_margin_perc']:.1f}%\", axis=1\n",
    "        )\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=agg_metric_for_plot[dim_name],\n",
    "            y=y,\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                array=error_array,\n",
    "                arrayminus=error_array_minus,\n",
    "                visible=True,\n",
    "                color='black',\n",
    "                thickness=2,\n",
    "                width=8\n",
    "            ),\n",
    "            mode='markers',\n",
    "            marker=dict(size=8, color='royalblue', line=dict(width=1, color='black')),\n",
    "            name=f\"{selected_metric} (with CI)\",\n",
    "            text=hover_text,\n",
    "            hoverinfo='text'\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{selected_metric} per {dim_name} (Top {top_n} by Volume + Rest) with Error Bars\",\n",
    "            yaxis_title=f\"{selected_metric} (%)\",\n",
    "            xaxis_title=dim_name,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "\n",
    "        if save_plots_to_html:\n",
    "            filename = f\"metric_with_errorbars_{dim_name}.html\"\n",
    "            fig.write_html(filename)\n",
    "            print(f\"Plot saved to {filename}\")\n",
    "\n",
    "        fig.show()\n",
    "        \n",
    "# --- Main Report Generation Functions ---\n",
    "\n",
    "def display_current_state_analysis(population_df: pd.DataFrame, selected_metric: str, metric_properties: Dict[str, Dict[str, str]],\n",
    "                                   predefined_total_cost_per_review: float, target_overall_error_percent: float, z_score: float,\n",
    "                                   save_plots_to_html: bool) -> None:\n",
    "    \"\"\"\n",
    "    Displays a summary of the current state of error margins and cost for the selected metric.\n",
    "    Now includes the overall metric rate.\n",
    "    \"\"\"\n",
    "    current_qa_reviewed_sum = population_df['qa_reviewed'].sum()\n",
    "    \n",
    "    selected_num_sum = population_df[metric_properties[selected_metric]['numerator_col']].sum()\n",
    "    selected_den_sum = population_df[metric_properties[selected_metric]['denominator_col']].sum()\n",
    "    \n",
    "    current_overall_margins = calculate_margin_wilson(selected_num_sum, selected_den_sum, z_score)\n",
    "    overall_margin_selected_metric = current_overall_margins['margin']\n",
    "    overall_metric_rate = selected_num_sum / selected_den_sum if selected_den_sum > 0 else np.nan\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'Metric': [selected_metric],\n",
    "        'Overall Metric Rate (%)': [round(overall_metric_rate * 100, 2)],\n",
    "        'Overall Error Margin (%)': [round(overall_margin_selected_metric * 100, 2)]\n",
    "    })\n",
    "    print(f\"\\n## Current State Summary Table: {selected_metric} (Overall)\")\n",
    "    display(summary)\n",
    "\n",
    "    cost_current = predefined_total_cost_per_review * current_qa_reviewed_sum if current_qa_reviewed_sum > 0 else np.inf\n",
    "    print(f\"\\nTotal Cost (Current State): ${cost_current:.2f} EUR\")\n",
    "    \n",
    "    plot_df = pd.DataFrame({\n",
    "        'Metric': [selected_metric],\n",
    "        'Current Error Margin (%)': [overall_margin_selected_metric * 100],\n",
    "        'Target Overall Error Margin (%)': [target_overall_error_percent]\n",
    "    })\n",
    "    \n",
    "    # Prepare text labels rounded to 0.00\n",
    "    text_labels = {\n",
    "        'Current Error Margin (%)': plot_df['Current Error Margin (%)'].apply(lambda x: f\"{x:.2f}\"),\n",
    "        'Target Overall Error Margin (%)': plot_df['Target Overall Error Margin (%)'].apply(lambda x: f\"{x:.2f}\")\n",
    "    }\n",
    "\n",
    "    create_bar_chart(\n",
    "        df=plot_df,\n",
    "        x_col_or_index_values='Metric',\n",
    "        y_cols=['Current Error Margin (%)', 'Target Overall Error Margin (%)'],\n",
    "        title=f'Overall Error Margin vs Target for {selected_metric} (Current State)',\n",
    "        y_axis_title='Error Margin (%)',\n",
    "        x_axis_title='Metric',\n",
    "        filename_suffix='current_state_overall_error_margin',\n",
    "        text_labels=text_labels,\n",
    "        save_plots_to_html=save_plots_to_html\n",
    "    )\n",
    "\n",
    "def display_current_dimension_metrics_and_margins(population_df: pd.DataFrame, selected_metric: str, metric_properties: Dict[str, Dict[str, str]],\n",
    "                                                  reporting_dimensions: List[str], top_n: int, target_overall_error_percent: float,\n",
    "                                                  global_avg_metrics: Dict[str, float], z_score: float, save_plots_to_html: bool) -> None:\n",
    "    \"\"\"\n",
    "    Displays visualizations of current metric values and error margins per reporting dimension and primary product.\n",
    "    \"\"\"\n",
    "    analysis_dimensions = list(set(reporting_dimensions + ['primary_product', 'customer']))\n",
    "\n",
    "    for dim_name in analysis_dimensions:\n",
    "        if dim_name not in population_df.columns:\n",
    "            print(f\"Warning: Dimension '{dim_name}' not found in population data. Skipping visualization for this dimension.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Current Metric and Error Margin for Dimension: {dim_name} ---\")\n",
    "\n",
    "        num_col = metric_properties[selected_metric]['numerator_col']\n",
    "        den_col = metric_properties[selected_metric]['denominator_col']\n",
    "        \n",
    "        if num_col not in population_df.columns or den_col not in population_df.columns:\n",
    "            warnings.warn(f\"Required columns for selected metric not found in DataFrame for dimension '{dim_name}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        current_dim_agg = population_df.groupby(dim_name).agg(\n",
    "            numerator_x=(num_col, 'sum'),\n",
    "            denominator_n=(den_col, 'sum'),\n",
    "            volume_sum=('volume', 'sum')\n",
    "        ).reset_index()\n",
    "\n",
    "        current_dim_agg['original_metric_rate'] = current_dim_agg['numerator_x'] / current_dim_agg['denominator_n'].replace(0, np.nan)\n",
    "        current_dim_agg['original_metric_rate'] = current_dim_agg['original_metric_rate'].fillna(global_avg_metrics[selected_metric])\n",
    "\n",
    "        current_margins = get_margins_df(current_dim_agg, 'numerator_x', 'denominator_n', z_score)\n",
    "        current_dim_agg['error_margin_perc'] = current_margins['margin'] * 100\n",
    "\n",
    "        agg_metric_for_plot = _aggregate_top_n_for_table_and_plot(\n",
    "            current_dim_agg, dim_name, top_n, 'volume_sum',\n",
    "            value_cols=['original_metric_rate', 'numerator_x', 'denominator_n', 'volume_sum'],\n",
    "            rest_agg_logic={'original_metric_rate': 'mean', 'numerator_x': 'sum', 'denominator_n': 'sum', 'volume_sum': 'sum'},\n",
    "            recalculate_margin_func=lambda rest_df_group: rest_df_group['numerator_x'].sum() / rest_df_group['denominator_n'].sum() if rest_df_group['denominator_n'].sum() > 0 else np.nan\n",
    "        )\n",
    "\n",
    "        create_bar_chart(\n",
    "            df=agg_metric_for_plot,\n",
    "            x_col_or_index_values=dim_name,\n",
    "            y_cols=['original_metric_rate'],\n",
    "            title=f'Current {selected_metric} per {dim_name} (Top {top_n} by Volume + Rest)',\n",
    "            y_axis_title=f'Current {selected_metric}',\n",
    "            x_axis_title=dim_name,\n",
    "            filename_suffix=f'current_metric_{dim_name}',\n",
    "            save_plots_to_html=save_plots_to_html\n",
    "        )\n",
    "\n",
    "        agg_margin_for_plot = _aggregate_top_n_for_table_and_plot(\n",
    "            current_dim_agg, dim_name, top_n, 'volume_sum',\n",
    "            value_cols=['error_margin_perc', 'numerator_x', 'denominator_n', 'volume_sum'],\n",
    "            rest_agg_logic={'error_margin_perc': 'mean', 'numerator_x': 'sum', 'denominator_n': 'sum', 'volume_sum': 'sum'},\n",
    "            recalculate_margin_func=lambda rest_df_group: calculate_margin_wilson(rest_df_group['numerator_x'].sum(), rest_df_group['denominator_n'].sum(), z_score)['margin'] * 100\n",
    "        )\n",
    "        create_bar_chart(\n",
    "            df=agg_margin_for_plot,\n",
    "            x_col_or_index_values=dim_name,\n",
    "            y_cols=['error_margin_perc'],\n",
    "            title=f'Current Error Margin (%) for {selected_metric} per {dim_name} (Top {top_n} by Volume + Rest)',\n",
    "            y_axis_title='Error Margin (%)',\n",
    "            x_axis_title=dim_name,\n",
    "            line_data={\n",
    "                'y_value': target_overall_error_percent,\n",
    "                'name': f'Target {target_overall_error_percent}%',\n",
    "                'color': \"Red\", 'width': 2, 'dash': \"dash\"\n",
    "            },\n",
    "            filename_suffix=f'current_error_margin_{dim_name}',\n",
    "            save_plots_to_html=save_plots_to_html\n",
    "        )\n",
    "\n",
    "def display_sample_size_calculations(\n",
    "    population_df: pd.DataFrame, selected_metric: str, metric_properties: Dict[str, Dict[str, str]],\n",
    "    reporting_dimensions: List[str], error_margin_logic: str, absolute_error_margin_value: float,\n",
    "    relative_error_margin_value: float, min_absolute_error_margin: float,\n",
    "    top_n: int, global_rates: Dict[str, float], overall_avg_frequencies_in_qa: Dict[str, float],\n",
    "    confidence_level: float, z_score: float, save_plots_to_html: bool\n",
    ") -> None:\n",
    "    print(f\"\\n## Required Sample Size Calculations (Current State for Target Error Margin for {selected_metric})\")\n",
    "\n",
    "    num_col = metric_properties[selected_metric]['numerator_col']\n",
    "    den_col = metric_properties[selected_metric]['denominator_col']\n",
    "    freq_base_col = metric_properties[selected_metric]['freq_denominator_base_col']\n",
    "\n",
    "    analysis_dimensions = list(set(reporting_dimensions + ['customer', 'primary_product']))\n",
    "\n",
    "    for dim_name in analysis_dimensions:\n",
    "        if dim_name not in population_df.columns:\n",
    "            print(f\"Warning: Dimension '{dim_name}' not found in population data. Skipping sample size calculation for this dimension.\")\n",
    "            continue\n",
    "\n",
    "        aggregation_cols = {\n",
    "            num_col: (num_col, 'sum'),\n",
    "            den_col: (den_col, 'sum'),\n",
    "            freq_base_col: (freq_base_col, 'sum'),\n",
    "            'volume': ('volume', 'sum'),\n",
    "            'qa_reviewed': ('qa_reviewed', 'sum')\n",
    "        }\n",
    "        existing_aggregation_cols = {k: v for k, v in aggregation_cols.items() if k in population_df.columns}\n",
    "\n",
    "        if not existing_aggregation_cols:\n",
    "            print(f\"Warning: No relevant columns found for aggregation for dimension '{dim_name}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        temp_df_all_groups = population_df.groupby(dim_name).agg(\n",
    "            **existing_aggregation_cols\n",
    "        ).reset_index()\n",
    "\n",
    "        if temp_df_all_groups.empty:\n",
    "            print(f\"No data after grouping by '{dim_name}'. Skipping sample size calculation for this dimension.\")\n",
    "            continue\n",
    "\n",
    "        rest_agg_logic_for_raw_counts = {\n",
    "            num_col: 'sum',\n",
    "            den_col: 'sum',\n",
    "            freq_base_col: 'sum',\n",
    "            'volume': 'sum',\n",
    "            'qa_reviewed': 'sum'\n",
    "        }\n",
    "\n",
    "        aggregated_df_for_calculations = _aggregate_top_n_for_table_and_plot(\n",
    "            temp_df_all_groups,\n",
    "            group_col=dim_name,\n",
    "            n=top_n,\n",
    "            sort_col='volume',\n",
    "            value_cols=[num_col, den_col, freq_base_col, 'volume', 'qa_reviewed'],\n",
    "            rest_agg_logic=rest_agg_logic_for_raw_counts,\n",
    "            recalculate_margin_func= lambda df: np.nan\n",
    "        )\n",
    "\n",
    "        original_metric_rate = aggregated_df_for_calculations[num_col] / aggregated_df_for_calculations[den_col].round(3).replace(0, np.nan)\n",
    "        global_metric_rate = global_rates[selected_metric]\n",
    "        # Use global rate if numerator==0 or denominator==0 or metric_rate==0\n",
    "        metric_rate = np.where(\n",
    "            (aggregated_df_for_calculations[den_col] == 0) | \n",
    "            (aggregated_df_for_calculations[num_col] == 0) | \n",
    "            (original_metric_rate.fillna(0) == 0),\n",
    "            global_metric_rate,\n",
    "            original_metric_rate\n",
    "        )\n",
    "\n",
    "        # Add this line to indicate if global rate was used:\n",
    "        aggregated_df_for_calculations['Used Global Rate'] = (\n",
    "            (aggregated_df_for_calculations[den_col] == 0) | \n",
    "            (aggregated_df_for_calculations[num_col] == 0) | \n",
    "            (original_metric_rate.fillna(0) == 0)\n",
    "        )\n",
    "\n",
    "        aggregated_df_for_calculations['Metric Rate'] = original_metric_rate.round(2)\n",
    "        aggregated_df_for_calculations['Global Metric Rate'] = global_metric_rate.round(2)\n",
    "\n",
    "        # Use adj_error_margin from calculate_required_sessions\n",
    "        req_size_list, extra_reviews_list, adj_error_margin = calculate_required_sessions(\n",
    "            aggregated_df_for_calculations, selected_metric, global_rates, overall_avg_frequencies_in_qa,\n",
    "            error_margin_logic, absolute_error_margin_value, relative_error_margin_value, min_absolute_error_margin,\n",
    "            confidence_level, z_score\n",
    "        )\n",
    "        aggregated_df_for_calculations['Target Error Margin'] = adj_error_margin\n",
    "\n",
    "        aggregated_df_for_calculations['Required QA Reviewed Sessions'] = req_size_list\n",
    "        aggregated_df_for_calculations['Extra Reviews Needed'] = extra_reviews_list\n",
    "\n",
    "        current_margins = get_margins_df(aggregated_df_for_calculations, num_col, den_col, z_score)\n",
    "        aggregated_df_for_calculations['Current Error Margin'] = current_margins['margin']\n",
    "\n",
    "        aggregated_df_for_calculations['Meets Target Margin'] = (\n",
    "            aggregated_df_for_calculations['Current Error Margin'] <= aggregated_df_for_calculations['Target Error Margin']\n",
    "        )\n",
    "\n",
    "        if freq_base_col in aggregated_df_for_calculations.columns:\n",
    "        # Use the reporting column for current QA reviewed sessions\n",
    "            aggregated_df_for_calculations['Current QA Reviewed Sessions'] = aggregated_df_for_calculations['qa_reviewed']\n",
    "        else:\n",
    "            warnings.warn(f\"'{freq_base_col}' not found in aggregated_df_for_calculations. 'Current QA Reviewed Sessions' will be 0.\")\n",
    "            aggregated_df_for_calculations['Current QA Reviewed Sessions'] = 0.0\n",
    "\n",
    "        # if freq_base_col in aggregated_df_for_calculations.columns:\n",
    "        #     aggregated_df_for_calculations.rename(columns={freq_base_col: 'Current QA Reviewed Sessions'}, inplace=True)\n",
    "        # else:\n",
    "        #     warnings.warn(f\"'{freq_base_col}' not found in aggregated_df_for_calculations. 'Current QA Reviewed Sessions' will be 0.\")\n",
    "        #     aggregated_df_for_calculations['Current QA Reviewed Sessions'] = 0.0\n",
    "        display_cols = [\n",
    "            dim_name, 'Metric Rate', 'Used Global Rate', 'Global Metric Rate', 'Target Error Margin', 'Current QA Reviewed Sessions',\n",
    "            'Required QA Reviewed Sessions', 'Extra Reviews Needed', 'Current Error Margin', 'Meets Target Margin'\n",
    "        ]\n",
    "        existing_display_cols = [col for col in display_cols if col in aggregated_df_for_calculations.columns]\n",
    "\n",
    "         # Calculate totals for the relevant columns\n",
    "        total_current_sessions = aggregated_df_for_calculations['Current QA Reviewed Sessions'].sum()\n",
    "        total_required_sessions = aggregated_df_for_calculations['Required QA Reviewed Sessions'].sum()\n",
    "        total_extra_reviews = aggregated_df_for_calculations['Extra Reviews Needed'].sum()\n",
    "\n",
    "        # Prepare a total row (fill other columns with empty or suitable values)\n",
    "        total_row = {col: '' for col in existing_display_cols}\n",
    "        if 'Current QA Reviewed Sessions' in existing_display_cols:\n",
    "            total_row['Current QA Reviewed Sessions'] = total_current_sessions\n",
    "        if 'Required QA Reviewed Sessions' in existing_display_cols:\n",
    "            total_row['Required QA Reviewed Sessions'] = total_required_sessions\n",
    "        if 'Extra Reviews Needed' in existing_display_cols:\n",
    "            total_row['Extra Reviews Needed'] = total_extra_reviews\n",
    "        # For the dimension column, label as 'Total'\n",
    "        if dim_name in existing_display_cols:\n",
    "            total_row[dim_name] = 'Total'\n",
    "\n",
    "        # Append the total row to the DataFrame for display\n",
    "        display_df = aggregated_df_for_calculations[existing_display_cols].round(3)\n",
    "        display_df = pd.concat([display_df, pd.DataFrame([total_row])], ignore_index=True)\n",
    "\n",
    "        print(f\"\\n--- Required Sample Size per Group for {dim_name} (Top {top_n} by Volume + Rest) ---\")\n",
    "        display(display_df)\n",
    "\n",
    "        # print(f\"\\n--- Required Sample Size per Group for {dim_name} (Top {top_n} by Volume + Rest) ---\")\n",
    "        # display(aggregated_df_for_calculations[existing_display_cols].round(3))\n",
    "\n",
    "        plot_df = aggregated_df_for_calculations[[dim_name, 'Required QA Reviewed Sessions', 'Current QA Reviewed Sessions']].copy()\n",
    "\n",
    "        create_bar_chart(\n",
    "            df=plot_df,\n",
    "            x_col_or_index_values=dim_name,\n",
    "            y_cols=['Required QA Reviewed Sessions', 'Current QA Reviewed Sessions'],\n",
    "            title=f'Required vs. Current QA Reviewed Sessions for {selected_metric} per {dim_name} (Top {top_n} by Volume + Rest)',\n",
    "            y_axis_title='Number of Sessions',\n",
    "            x_axis_title=dim_name,\n",
    "            filename_suffix=f'required_vs_current_sessions_{dim_name}',\n",
    "            save_plots_to_html=save_plots_to_html\n",
    "        )\n",
    "        \n",
    "def _prepare_volume_stratified_df(df_input: pd.DataFrame, target_sessions_for_strat: int) -> pd.DataFrame:\n",
    "    \"\"\"Helper for Volume-Based Stratified Sampling.\"\"\"\n",
    "    df_copy = df_input.copy()\n",
    "    try:\n",
    "        num_quantiles = 5\n",
    "        if len(df_copy['volume'].unique()) < num_quantiles:\n",
    "            num_quantiles = len(df_copy['volume'].unique())\n",
    "            if num_quantiles < 2: raise ValueError(\"Not enough unique values for stratification.\")\n",
    "\n",
    "        df_copy['strata'] = pd.qcut(df_copy['volume'], q=num_quantiles, labels=False, duplicates='drop')\n",
    "        \n",
    "        if 'strata' in df_copy.columns:\n",
    "            strata_volumes_sum_per_row = df_copy.groupby('strata')['volume'].transform('sum')\n",
    "            total_pop_volume = df_copy['volume'].sum()\n",
    "            \n",
    "            df_copy['temp_sampling_volume_strat'] = df_copy.apply(\n",
    "                lambda row: (row['volume'] / strata_volumes_sum_per_row.loc[row.name]) * ((df_copy.groupby('strata')['volume'].sum().loc[row['strata']] / total_pop_volume) * target_sessions_for_strat)\n",
    "                if strata_volumes_sum_per_row.loc[row.name] > 0 and total_pop_volume > 0 else 0, axis=1\n",
    "            )\n",
    "            df_copy['temp_sampling_volume_strat'] = df_copy['temp_sampling_volume_strat'].fillna(0)\n",
    "        else:\n",
    "            warnings.warn(\"Could not create 'strata' for volume stratification. Falling back to simple random sampling.\")\n",
    "            df_copy['temp_sampling_volume_strat'] = df_copy['volume']\n",
    "    except (ValueError, Exception) as e:\n",
    "        warnings.warn(f\"Volume stratification error: {e}. Falling back to simple random sampling.\")\n",
    "        df_copy['temp_sampling_volume_strat'] = df_copy['volume']\n",
    "    return df_copy\n",
    "\n",
    "# def _run_single_simulation_scenario(population_df: pd.DataFrame, target_sessions: int, sampling_type: str, exponent: float,\n",
    "#                                     volume_col_for_sampling: str, selected_metric: str, global_avg_metrics: Dict[str, float],\n",
    "#                                     overall_avg_frequencies_in_qa: Dict[str, float], reporting_dimensions: List[str],\n",
    "#                                     error_margin_logic: str, absolute_error_margin_value: float, relative_error_margin_value: float,\n",
    "#                                     z_score: float, population_df_original_for_metrics: pd.DataFrame) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Helper function to run a single simulation scenario and calculate its results.\n",
    "#     Returns a dictionary of results, does not print.\n",
    "#     \"\"\"\n",
    "#     scenario_results = {}\n",
    "\n",
    "#     def get_coverage_and_list(sim_df: pd.DataFrame, groupby_dims: List[str], margin_type: str, threshold: float) -> Tuple[int, str]:\n",
    "#         \"\"\"\n",
    "#         Calculates the number of groups whose error margin is below the target threshold.\n",
    "#         Also returns a comma-separated list of these groups.\n",
    "#         \"\"\"\n",
    "#         if sim_df.empty:\n",
    "#             return 0, \"\"\n",
    "            \n",
    "#         if margin_type not in sim_df.columns:\n",
    "#             warnings.warn(f\"Warning: Margin type '{margin_type}' not found in simulation DataFrame for coverage calculation. Coverage will be 0.\")\n",
    "#             return 0, \"\"\n",
    "\n",
    "#         condition_met = (sim_df[margin_type] <= threshold)\n",
    "#         coverage = sim_df[condition_met].shape[0] \n",
    "        \n",
    "#         if not groupby_dims: \n",
    "#             return coverage, \"\"\n",
    "\n",
    "#         existing_groupby_dims = [dim for dim in groupby_dims if dim in sim_df.columns]\n",
    "#         if not existing_groupby_dims:\n",
    "#             return coverage, \"\" \n",
    "\n",
    "#         values_list = sim_df.loc[condition_met, existing_groupby_dims].apply(\n",
    "#             lambda x: ':: '.join(map(str, x)) if isinstance(x.values[0], (tuple, list)) else str(x.values[0]), axis=1\n",
    "#         ).tolist()\n",
    "#         values_str = ', '.join(values_list)\n",
    "#         return coverage, values_str\n",
    "    \n",
    "#     overall_sim_df = calculate_sample_counts(\n",
    "#         population_df=population_df, total_target_sessions=target_sessions, \n",
    "#         groupby_dimensions=[], sampling_type=sampling_type, exponent=exponent, \n",
    "#         volume_col_for_sampling=volume_col_for_sampling,\n",
    "#         selected_metric=selected_metric, global_rates=global_avg_metrics, \n",
    "#         overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa, z_score=z_score\n",
    "#     )\n",
    "#     scenario_results['overall_margin'] = overall_sim_df['margin'].iloc[0] if not overall_sim_df.empty else np.inf\n",
    "#     scenario_results['total_sampled'] = target_sessions\n",
    "#     cost_total = REVIEW_COST_PER_SESSION * target_sessions if target_sessions > 0 else np.inf\n",
    "#     scenario_results['cost_total'] = cost_total\n",
    "\n",
    "#     margin_type_for_coverage = 'margin' if error_margin_logic == 'Absolute Error Margin' else 'margin_rel'\n",
    "#     threshold_for_coverage = absolute_error_margin_value if error_margin_logic == 'Absolute Error Margin' else relative_error_margin_value\n",
    "\n",
    "#     sim_dfs_for_metrics = {}\n",
    "    \n",
    "#     # Use population_df_original_for_metrics for calculating top_3_customers and strategic_clients\n",
    "#     top_3_customers = population_df_original_for_metrics.groupby('customer')['volume'].sum().nlargest(3).index.tolist()\n",
    "#     strategic_clients = population_df_original_for_metrics[population_df_original_for_metrics['customer_priority'] == 'Tier Platinum']['customer'].unique().tolist()\n",
    "#     total_strategic_clients_in_population = len(strategic_clients)\n",
    "\n",
    "#     for group_dim_list, key_suffix in [\n",
    "#         (reporting_dimensions, 'dimension'),\n",
    "#         (['customer'], 'customer'),\n",
    "#         (['primary_product'], 'product'),\n",
    "#         (['industry'], 'industry'),\n",
    "#         (['document_country'], 'document_country'),\n",
    "#         (['region'], 'region')\n",
    "#     ]:\n",
    "#         if all(dim in population_df.columns for dim in group_dim_list):\n",
    "#             sim_df = calculate_sample_counts(\n",
    "#                 population_df=population_df, total_target_sessions=target_sessions, \n",
    "#                 groupby_dimensions=group_dim_list, sampling_type=sampling_type, exponent=exponent, \n",
    "#                 volume_col_for_sampling=volume_col_for_sampling,\n",
    "#                 selected_metric=selected_metric, global_rates=global_avg_metrics, \n",
    "#                 overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa, z_score=z_score\n",
    "#             )\n",
    "#             coverage, values_str = get_coverage_and_list(sim_df, group_dim_list, margin_type_for_coverage, threshold_for_coverage)\n",
    "#             scenario_results[f\"{key_suffix}_coverage\"] = coverage\n",
    "#             scenario_results[f\"{key_suffix}_list\"] = values_str\n",
    "#             sim_dfs_for_metrics[key_suffix] = sim_df\n",
    "#         else:\n",
    "#             scenario_results[f\"{key_suffix}_coverage\"] = 0\n",
    "#             scenario_results[f\"{key_suffix}_list\"] = \"\"\n",
    "#             sim_dfs_for_metrics[key_suffix] = pd.DataFrame(columns=group_dim_list + ['margin', 'margin_rel', 'simulated_total_sessions'])\n",
    "\n",
    "#     if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "#         top_3_customers_in_sample_df = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'].isin(top_3_customers)]\n",
    "#         share_top_3_customers = top_3_customers_in_sample_df['simulated_total_sessions'].sum() / target_sessions if target_sessions > 0 else 0\n",
    "#     else: share_top_3_customers = 0\n",
    "#     scenario_results['share_top_3_customers'] = share_top_3_customers * 100\n",
    "\n",
    "#     if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "#         strategic_clients_in_sample_df = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'].isin(strategic_clients)]\n",
    "#         strategic_clients_met_threshold = strategic_clients_in_sample_df[strategic_clients_in_sample_df[margin_type_for_coverage] <= threshold_for_coverage].shape[0]\n",
    "#         share_strategic_clients_met_sla = (strategic_clients_met_threshold / total_strategic_clients_in_population) * 100 if total_strategic_clients_in_population > 0 else 0\n",
    "#     else: share_strategic_clients_met_sla = 0\n",
    "#     scenario_results['share_strategic_clients_met_sla'] = share_strategic_clients_met_sla\n",
    "\n",
    "#     total_coverage_for_cost = scenario_results['customer_coverage'] + scenario_results['product_coverage'] + scenario_results['industry_coverage'] + scenario_results['document_country_coverage'] + scenario_results['region_coverage']\n",
    "#     total_cost = REVIEW_COST_PER_SESSION * target_sessions if target_sessions > 0 else np.nan\n",
    "#     if total_coverage_for_cost > 0:\n",
    "#         cost_over_coverage = total_cost / total_coverage_for_cost\n",
    "#     else:\n",
    "#         cost_over_coverage = total_cost  # Full price if nothing is covered\n",
    "#     scenario_results['cost_over_coverage'] = cost_over_coverage\n",
    "\n",
    "#     return scenario_results\n",
    "\n",
    "def _run_single_simulation_scenario(\n",
    "    population_df: pd.DataFrame,\n",
    "    target_sessions: int,\n",
    "    sampling_type: str,\n",
    "    exponent: float,\n",
    "    volume_col_for_sampling: str,\n",
    "    selected_metric: str,\n",
    "    global_avg_metrics: Dict[str, float],\n",
    "    overall_avg_frequencies_in_qa: Dict[str, float],\n",
    "    reporting_dimensions: List[str],\n",
    "    error_margin_logic: str,\n",
    "    absolute_error_margin_value: float,\n",
    "    relative_error_margin_value: float,\n",
    "    min_absolute_error_margin: float,\n",
    "    z_score: float,\n",
    "    population_df_original_for_metrics: pd.DataFrame\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Helper function to run a single simulation scenario and calculate its results.\n",
    "    Returns a dictionary of results, does not print.\n",
    "    \"\"\"\n",
    "    scenario_results = {}\n",
    "\n",
    "    TOP_N_CUSTOMERS = 4  # or 5, as needed\n",
    "    top_customers = population_df_original_for_metrics.groupby('customer')['volume'].sum().nlargest(TOP_N_CUSTOMERS).index.tolist()\n",
    "\n",
    "    def aggregate_and_calc_margin(sim_df, group_col, num_col, den_col, z_score):\n",
    "        \"\"\"Aggregate by group_col and recalculate margin.\"\"\"\n",
    "        agg = sim_df.groupby(group_col).agg(\n",
    "            total_num=(num_col, 'sum'),\n",
    "            total_den=(den_col, 'sum')\n",
    "        ).reset_index()\n",
    "        margins = calculate_margin_wilson(agg['total_num'], agg['total_den'], z_score)\n",
    "        agg['margin'] = margins['margin']\n",
    "        agg['margin_rel'] = margins['margin_rel']\n",
    "        return agg\n",
    "\n",
    "    def get_coverage_and_list(sim_df: pd.DataFrame, groupby_dims: list, margin_type: str, threshold: float, error_margin_logic: str = 'Absolute Error Margin', relative_error_margin_value: float = 0.5, min_absolute_error_margin: float = 0.02, top_customers: list = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Calculates the number of groups whose error margin is below the target threshold.\n",
    "        For customers, applies top/rest logic. For others, uses final_thresholds (absolute/relative).\n",
    "        \"\"\"\n",
    "        import warnings\n",
    "        import numpy as np\n",
    "\n",
    "        if top_customers is None:\n",
    "            top_customers = []\n",
    "\n",
    "        # --- Compute per-row threshold ---\n",
    "        if error_margin_logic == 'Relative Error Margin' and 'metric_rate' in sim_df.columns:\n",
    "            rel_thresholds = np.maximum(relative_error_margin_value * sim_df['metric_rate'], min_absolute_error_margin)\n",
    "        else:\n",
    "            rel_thresholds = np.full(len(sim_df), threshold)\n",
    "\n",
    "        # --- Customer logic ---\n",
    "        if 'customer' in sim_df.columns or 'customer_top_20' in sim_df.columns:\n",
    "            target_margin_top = threshold\n",
    "            target_margin_rest = 0.1\n",
    "            customer_thresholds = sim_df['customer'].apply(\n",
    "                lambda x: target_margin_top if x in top_customers else target_margin_rest\n",
    "            ).values\n",
    "            # Use customer thresholds for customer groups, rel_thresholds for others\n",
    "            final_thresholds = np.where(sim_df['customer'].notna(), customer_thresholds, rel_thresholds)\n",
    "        else:\n",
    "            final_thresholds = rel_thresholds\n",
    "\n",
    "        sim_df['target_margin'] = final_thresholds\n",
    "\n",
    "        if sim_df.empty:\n",
    "            return 0, \"\"\n",
    "\n",
    "        if margin_type not in sim_df.columns:\n",
    "            warnings.warn(f\"Warning: Margin type '{margin_type}' not found in simulation DataFrame for coverage calculation. Coverage will be 0.\")\n",
    "            return 0, \"\"\n",
    "\n",
    "        mask = sim_df['margin'] <= sim_df['target_margin']\n",
    "\n",
    "        if 'used_global_rate_fallback' in sim_df.columns:\n",
    "            mask &= ~sim_df['used_global_rate_fallback']\n",
    "\n",
    "        if 'simulated_den_n' in sim_df.columns:\n",
    "            mask &= sim_df['simulated_den_n'] > 10\n",
    "\n",
    "        if 'customer_top_20' in sim_df.columns:\n",
    "            mask &= sim_df['customer_top_20'] != 'Other'\n",
    "\n",
    "        coverage = mask.sum()\n",
    "\n",
    "        if not groupby_dims:\n",
    "            return coverage, \"\"\n",
    "\n",
    "        existing_groupby_dims = [dim for dim in groupby_dims if dim in sim_df.columns]\n",
    "        if not existing_groupby_dims:\n",
    "            return coverage, \"\"\n",
    "\n",
    "        selected = sim_df.loc[mask, existing_groupby_dims].astype(str)\n",
    "        if selected.empty:\n",
    "            values_list = []\n",
    "        elif len(existing_groupby_dims) == 1:\n",
    "            values_list = selected.iloc[:, 0].tolist()\n",
    "        else:\n",
    "            values_list = selected.apply(lambda row: ' & '.join(row.values), axis=1).tolist()\n",
    "        values_str = ', '.join(values_list)\n",
    "        return coverage, values_str\n",
    "    \n",
    "    # OLD ONE\n",
    "    # def get_coverage_and_list(sim_df: pd.DataFrame, groupby_dims: List[str], margin_type: str, threshold: float) -> Tuple[int, str]:\n",
    "    #     \"\"\"\n",
    "    #     Calculates the number of groups whose error margin is below the target threshold.\n",
    "    #     Returns a comma-separated list of these groups, excluding those not in customer_top_20 if applicable.\n",
    "    #     \"\"\"\n",
    "    #     import warnings\n",
    "\n",
    "    #     target_margin_top = threshold  # e.g., 1% for top customers\n",
    "    #     target_margin_rest = 0.1 # e.g., 3% for others\n",
    "\n",
    "    #     if 'customer' in sim_df.columns or 'customer_top_20' in sim_df.columns:\n",
    "    #         sim_df['target_margin'] = sim_df['customer'].apply(\n",
    "    #             lambda x: target_margin_top if x in top_customers else target_margin_rest\n",
    "    #         )\n",
    "    #     else:\n",
    "    #         sim_df['target_margin'] = threshold  # fallback for non-customer groups\n",
    "\n",
    "    #     if sim_df.empty:\n",
    "    #         return 0, \"\"\n",
    "    #     if margin_type not in sim_df.columns:\n",
    "    #         warnings.warn(f\"Warning: Margin type '{margin_type}' not found in simulation DataFrame for coverage calculation. Coverage will be 0.\")\n",
    "    #         return 0, \"\"\n",
    "\n",
    "    #     #mask = sim_df[margin_type] <= threshold\n",
    "    #     mask = sim_df[margin_type] <= sim_df['target_margin']\n",
    "\n",
    "    #     # Exclude global rate fallback if column exists\n",
    "    #     if 'used_global_rate_fallback' in sim_df.columns:\n",
    "    #         mask &= ~sim_df['used_global_rate_fallback']\n",
    "\n",
    "    #     # Exclude groups with zero denominator if column exists\n",
    "    #     if 'simulated_den_n' in sim_df.columns:\n",
    "    #         mask &= sim_df['simulated_den_n'] > 10\n",
    "\n",
    "    #     # Exclude customers not in customer_top_20 if applicable\n",
    "    #     if 'customer_top_20' in sim_df.columns:\n",
    "    #         mask &= sim_df['customer_top_20'] != 'Other'\n",
    "\n",
    "    #     coverage = mask.sum()\n",
    "\n",
    "    #     if not groupby_dims:\n",
    "    #         return coverage, \"\"\n",
    "\n",
    "    #     existing_groupby_dims = [dim for dim in groupby_dims if dim in sim_df.columns]\n",
    "    #     if not existing_groupby_dims:\n",
    "    #         return coverage, \"\"\n",
    "\n",
    "    #     selected = sim_df.loc[mask, existing_groupby_dims].astype(str)\n",
    "    #     if selected.empty:\n",
    "    #         values_list = []\n",
    "    #     elif len(existing_groupby_dims) == 1:\n",
    "    #         values_list = selected.iloc[:, 0].tolist()\n",
    "    #     else:\n",
    "    #         values_list = selected.apply(lambda row: ' & '.join(row.values), axis=1).tolist()\n",
    "    #     values_str = ', '.join(values_list)\n",
    "    #     return coverage, values_str\n",
    "\n",
    "    \n",
    "    overall_sim_df = calculate_sample_counts(\n",
    "        population_df=population_df, total_target_sessions=target_sessions, \n",
    "        groupby_dimensions=[], sampling_type=sampling_type, exponent=exponent, \n",
    "        volume_col_for_sampling=volume_col_for_sampling,\n",
    "        selected_metric=selected_metric, global_rates=global_avg_metrics, \n",
    "        overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa, z_score=z_score\n",
    "    )\n",
    "    scenario_results['overall_margin'] = overall_sim_df['margin'].iloc[0] if not overall_sim_df.empty else np.inf\n",
    "    scenario_results['total_sampled'] = target_sessions\n",
    "    cost_total = REVIEW_COST_PER_SESSION * target_sessions if target_sessions > 0 else np.inf\n",
    "    scenario_results['cost_total'] = cost_total\n",
    "\n",
    "    margin_type_for_coverage = 'margin' if error_margin_logic == 'Absolute Error Margin' else 'margin_rel'\n",
    "    threshold_for_coverage = absolute_error_margin_value if error_margin_logic == 'Absolute Error Margin' else relative_error_margin_value\n",
    "\n",
    "    sim_dfs_for_metrics = {}\n",
    "\n",
    "    # Use population_df_original_for_metrics for calculating top_3_customers and strategic_clients\n",
    "    top_3_customers = population_df_original_for_metrics.groupby('customer')['volume'].sum().nlargest(3).index.tolist()\n",
    "\n",
    "    if 'customer_priority' in population_df_original_for_metrics.columns:\n",
    "        strategic_clients = population_df_original_for_metrics[population_df_original_for_metrics['customer_priority'] == 'Tier Platinum']['customer'].unique().tolist()\n",
    "    else:\n",
    "        print(\"WARNING: 'customer_priority' column missing in population_df_original_for_metrics\")\n",
    "        strategic_clients = []\n",
    "    \n",
    "    total_strategic_clients_in_population = len(strategic_clients)\n",
    "\n",
    "    # For each dimension, aggregate and recalculate margin for coverage\n",
    "    dimension_configs = [\n",
    "        (reporting_dimensions, 'dimension'),\n",
    "        (['customer'], 'customer'),\n",
    "        (['primary_product'], 'product'),\n",
    "        (['industry'], 'industry'),\n",
    "        (['document_country'], 'document_country'),\n",
    "        (['region'], 'region')\n",
    "    ]\n",
    "    for group_dim_list, key_suffix in dimension_configs:\n",
    "        if all(dim in population_df.columns for dim in group_dim_list):\n",
    "            sim_df = calculate_sample_counts(\n",
    "                population_df=population_df, total_target_sessions=target_sessions, \n",
    "                groupby_dimensions=group_dim_list, sampling_type=sampling_type, exponent=exponent, \n",
    "                volume_col_for_sampling=volume_col_for_sampling,\n",
    "                selected_metric=selected_metric, global_rates=global_avg_metrics, \n",
    "                overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa, z_score=z_score\n",
    "            )\n",
    "            # If group_dim_list is a single main group, aggregate by that group (to avoid slice double-counting)\n",
    "            if len(group_dim_list) == 1:\n",
    "                main_group = group_dim_list\n",
    "                sim_df_agg = aggregate_and_calc_margin(\n",
    "                    sim_df, group_col=main_group, num_col='simulated_num_x', den_col='simulated_den_n', z_score=z_score\n",
    "                )\n",
    "                # Add back simulated_total_sessions if possible\n",
    "                if 'simulated_total_sessions' in sim_df.columns:\n",
    "                    total_sessions = sim_df.groupby(main_group)['simulated_total_sessions'].sum().reset_index()\n",
    "                    sim_df_agg = sim_df_agg.merge(total_sessions, on=main_group, how='left')\n",
    "                else:\n",
    "                    sim_df_agg['simulated_total_sessions'] = np.nan\n",
    "                coverage, values_str = get_coverage_and_list(\n",
    "                    sim_df_agg, main_group, \n",
    "                    margin_type_for_coverage, \n",
    "                    threshold_for_coverage,\n",
    "                    error_margin_logic=error_margin_logic,\n",
    "                    relative_error_margin_value=relative_error_margin_value,\n",
    "                    min_absolute_error_margin=min_absolute_error_margin,\n",
    "                    top_customers=top_customers\n",
    "                )\n",
    "                sim_dfs_for_metrics[key_suffix] = sim_df_agg\n",
    "            else:\n",
    "                # For multi-dim slices, keep as is\n",
    "                coverage, values_str = get_coverage_and_list(\n",
    "                    sim_df, group_dim_list, \n",
    "                    margin_type_for_coverage,\n",
    "                    threshold_for_coverage,\n",
    "                    error_margin_logic=error_margin_logic,\n",
    "                    relative_error_margin_value=relative_error_margin_value,\n",
    "                    min_absolute_error_margin=min_absolute_error_margin,\n",
    "                    top_customers=top_customers\n",
    "                )\n",
    "                sim_dfs_for_metrics[key_suffix] = sim_df\n",
    "            scenario_results[f\"{key_suffix}_coverage\"] = coverage\n",
    "            scenario_results[f\"{key_suffix}_list\"] = values_str\n",
    "        else:\n",
    "            scenario_results[f\"{key_suffix}_coverage\"] = 0\n",
    "            scenario_results[f\"{key_suffix}_list\"] = \"\"\n",
    "            sim_dfs_for_metrics[key_suffix] = pd.DataFrame(columns=group_dim_list + ['margin', 'margin_rel', 'simulated_total_sessions'])\n",
    "\n",
    "    if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "        top_3_customers_in_sample_df = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'].isin(top_3_customers)]\n",
    "        share_top_3_customers = top_3_customers_in_sample_df['simulated_total_sessions'].sum() / target_sessions if target_sessions > 0 else 0\n",
    "    else:\n",
    "        share_top_3_customers = 0\n",
    "    scenario_results['share_top_3_customers'] = share_top_3_customers * 100\n",
    "\n",
    "    if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "        strategic_clients_in_sample_df = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'].isin(strategic_clients)]\n",
    "        strategic_clients_met_threshold = strategic_clients_in_sample_df[strategic_clients_in_sample_df[margin_type_for_coverage] <= threshold_for_coverage].shape[0]\n",
    "        share_strategic_clients_met_sla = (strategic_clients_met_threshold / total_strategic_clients_in_population) * 100 if total_strategic_clients_in_population > 0 else 0\n",
    "    else:\n",
    "        share_strategic_clients_met_sla = 0\n",
    "    scenario_results['share_strategic_clients_met_sla'] = share_strategic_clients_met_sla\n",
    "\n",
    "    uber_total_volume = 0\n",
    "    uber_simulated_sessions = 0\n",
    "    if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "        df_uber = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'] == 'Uber Main Account']\n",
    "        if not df_uber.empty:\n",
    "            # Get total volume from original population_df\n",
    "            uber_total_volume = population_df[population_df['customer'] == 'Uber Main Account']['volume'].sum()\n",
    "            # Get simulated sessions from simulation\n",
    "            uber_simulated_sessions = int(df_uber['simulated_total_sessions'].sum())\n",
    "\n",
    "    scenario_results['uber_main_account_total_volume'] = uber_total_volume\n",
    "    scenario_results['uber_main_account_simulated_sessions'] = uber_simulated_sessions\n",
    "\n",
    "    # Calculate 1% of total volume, subtract simulated sessions\n",
    "    uber_target_sessions = int(uber_total_volume * 0.01)\n",
    "    uber_left_to_cover = max(uber_target_sessions - uber_simulated_sessions, 0)\n",
    "    scenario_results['uber_main_account_left_to_cover'] = uber_left_to_cover\n",
    "\n",
    "    total_coverage_for_cost = (\n",
    "        scenario_results['customer_coverage'] +\n",
    "        scenario_results['product_coverage'] +\n",
    "        scenario_results['industry_coverage'] +\n",
    "        scenario_results['document_country_coverage'] +\n",
    "        scenario_results['region_coverage']\n",
    "    )\n",
    "    total_cost = REVIEW_COST_PER_SESSION * target_sessions if target_sessions > 0 else np.nan\n",
    "    if total_coverage_for_cost > 0:\n",
    "        cost_over_coverage = total_cost / total_coverage_for_cost\n",
    "    else:\n",
    "        cost_over_coverage = total_cost  # Full price if nothing is covered\n",
    "    scenario_results['cost_over_coverage'] = cost_over_coverage\n",
    "\n",
    "    return scenario_results\n",
    "\n",
    "# def _run_single_simulation_scenario(\n",
    "#     population_df: pd.DataFrame,\n",
    "#     target_sessions: int,\n",
    "#     sampling_type: str,\n",
    "#     exponent: float,\n",
    "#     volume_col_for_sampling: str,\n",
    "#     selected_metric: str,\n",
    "#     global_avg_metrics: dict,\n",
    "#     overall_avg_frequencies_in_qa: dict,\n",
    "#     reporting_dimensions: list,\n",
    "#     error_margin_logic: str,\n",
    "#     absolute_error_margin_value: float,\n",
    "#     relative_error_margin_value: float,\n",
    "#     z_score: float,\n",
    "#     population_df_original_for_metrics: pd.DataFrame\n",
    "# ) -> dict:\n",
    "#     \"\"\"\n",
    "#     Run a single simulation scenario, calculating overall margin and per-dimension coverage.\n",
    "#     \"\"\"\n",
    "#     scenario_results = {}\n",
    "\n",
    "#     # 1. Calculate overall margin (no grouping)\n",
    "#     overall_sim_df = calculate_sample_counts(\n",
    "#         population_df=population_df,\n",
    "#         total_target_sessions=target_sessions,\n",
    "#         groupby_dimensions=[],\n",
    "#         sampling_type=sampling_type,\n",
    "#         exponent=exponent,\n",
    "#         volume_col_for_sampling=volume_col_for_sampling,\n",
    "#         selected_metric=selected_metric,\n",
    "#         global_rates=global_avg_metrics,\n",
    "#         overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa,\n",
    "#         z_score=z_score\n",
    "#     )\n",
    "#     scenario_results['overall_margin'] = overall_sim_df['margin'].iloc[0] if not overall_sim_df.empty else np.inf\n",
    "#     scenario_results['total_sampled'] = target_sessions\n",
    "#     cost_total = REVIEW_COST_PER_SESSION * target_sessions if target_sessions > 0 else np.inf\n",
    "#     scenario_results['cost_total'] = cost_total\n",
    "\n",
    "#     margin_type_for_coverage = 'margin' if error_margin_logic == 'Absolute Error Margin' else 'margin_rel'\n",
    "#     threshold_for_coverage = absolute_error_margin_value if error_margin_logic == 'Absolute Error Margin' else relative_error_margin_value\n",
    "\n",
    "#     sim_dfs_for_metrics = {}\n",
    "\n",
    "#     # 2. Prepare dimension list for per-dimension coverage\n",
    "#     dimension_list = [\n",
    "#         (reporting_dimensions, 'dimension'),\n",
    "#         (['customer'], 'customer'),\n",
    "#         (['primary_product'], 'product'),\n",
    "#         (['industry'], 'industry'),\n",
    "#         (['document_country'], 'document_country'),\n",
    "#         (['region'], 'region')\n",
    "#     ]\n",
    "\n",
    "#     for dims, key_suffix in dimension_list:\n",
    "#         available_dims = [dim for dim in dims if dim in population_df.columns]\n",
    "#         if dims and available_dims:\n",
    "#             # Calculate sample counts for this dimension\n",
    "#             sim_df = calculate_sample_counts(\n",
    "#                 population_df=population_df,\n",
    "#                 total_target_sessions=target_sessions,\n",
    "#                 groupby_dimensions=available_dims,\n",
    "#                 sampling_type=sampling_type,\n",
    "#                 exponent=exponent,\n",
    "#                 volume_col_for_sampling=volume_col_for_sampling,\n",
    "#                 selected_metric=selected_metric,\n",
    "#                 global_rates=global_avg_metrics,\n",
    "#                 overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa,\n",
    "#                 z_score=z_score\n",
    "#             )\n",
    "#             # Coverage mask\n",
    "#             mask = (sim_df[margin_type_for_coverage] <= threshold_for_coverage)\n",
    "#             if 'used_global_rate_fallback' in sim_df.columns:\n",
    "#                 mask &= ~sim_df['used_global_rate_fallback']\n",
    "#             if 'simulated_den_n' in sim_df.columns:\n",
    "#                 mask &= sim_df['simulated_den_n'] > 0\n",
    "#             if 'customer_top_20' in sim_df.columns and 'customer' in available_dims:\n",
    "#                 mask &= sim_df['customer_top_20'] != 'Rest'\n",
    "#             mask &= sim_df['simulated_den_n'] >= 50\n",
    "\n",
    "#             coverage = mask.sum()\n",
    "#             # List of covered values\n",
    "#             if len(available_dims) == 1:\n",
    "#                 values_str = ', '.join(sim_df.loc[mask, available_dims[0]].astype(str).tolist())\n",
    "#             else:\n",
    "#                 values_str = ', '.join(sim_df.loc[mask, available_dims].astype(str).agg('::'.join, axis=1).tolist())\n",
    "#             scenario_results[f\"{key_suffix}_coverage\"] = coverage\n",
    "#             scenario_results[f\"{key_suffix}_list\"] = values_str\n",
    "#             sim_dfs_for_metrics[key_suffix] = sim_df\n",
    "#         else:\n",
    "#             scenario_results[f\"{key_suffix}_coverage\"] = 0\n",
    "#             scenario_results[f\"{key_suffix}_list\"] = \"\"\n",
    "#             sim_dfs_for_metrics[key_suffix] = pd.DataFrame(columns=dims + ['margin', 'margin_rel', 'simulated_total_sessions'])\n",
    "\n",
    "#     # 3. Top customers and strategic clients logic (unchanged)\n",
    "#     top_3_customers = population_df_original_for_metrics.groupby('customer')['volume'].sum().nlargest(3).index.tolist()\n",
    "#     if 'customer_priority' in population_df_original_for_metrics.columns:\n",
    "#         strategic_clients = population_df_original_for_metrics[population_df_original_for_metrics['customer_priority'] == 'Tier Platinum']['customer'].unique().tolist()\n",
    "#     else:\n",
    "#         print(\"WARNING: 'customer_priority' column missing in population_df_original_for_metrics\")\n",
    "#         strategic_clients = []\n",
    "#     total_strategic_clients_in_population = len(strategic_clients)\n",
    "\n",
    "#     # Share of top 3 customers\n",
    "#     if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "#         top_3_customers_in_sample_df = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'].isin(top_3_customers)]\n",
    "#         share_top_3_customers = top_3_customers_in_sample_df['simulated_total_sessions'].sum() / target_sessions if target_sessions > 0 else 0\n",
    "#     else:\n",
    "#         share_top_3_customers = 0\n",
    "#     scenario_results['share_top_3_customers'] = share_top_3_customers * 100\n",
    "\n",
    "#     # Share of strategic clients meeting SLA\n",
    "#     if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "#         strategic_clients_in_sample_df = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'].isin(strategic_clients)]\n",
    "#         strategic_clients_met_threshold = strategic_clients_in_sample_df[strategic_clients_in_sample_df[margin_type_for_coverage] <= threshold_for_coverage].shape[0]\n",
    "#         share_strategic_clients_met_sla = (strategic_clients_met_threshold / total_strategic_clients_in_population) * 100 if total_strategic_clients_in_population > 0 else 0\n",
    "#     else:\n",
    "#         share_strategic_clients_met_sla = 0\n",
    "#     scenario_results['share_strategic_clients_met_sla'] = share_strategic_clients_met_sla\n",
    "\n",
    "#     # Cost per coverage\n",
    "#     total_coverage_for_cost = (\n",
    "#         scenario_results.get('customer_coverage', 0) +\n",
    "#         scenario_results.get('product_coverage', 0) +\n",
    "#         scenario_results.get('industry_coverage', 0) +\n",
    "#         scenario_results.get('document_country_coverage', 0) +\n",
    "#         scenario_results.get('region_coverage', 0)\n",
    "#     )\n",
    "#     total_cost = REVIEW_COST_PER_SESSION * target_sessions if target_sessions > 0 else np.nan\n",
    "#     if total_coverage_for_cost > 0:\n",
    "#         cost_over_coverage = total_cost / total_coverage_for_cost\n",
    "#     else:\n",
    "#         cost_over_coverage = total_cost\n",
    "#     scenario_results['cost_over_coverage'] = cost_over_coverage\n",
    "\n",
    "#     return scenario_results\n",
    "\n",
    "\n",
    "def _calculate_proportional_stratified_allocation(\n",
    "    df: pd.DataFrame, \n",
    "    stratify_by_dimension: List[str], \n",
    "    total_qa_sessions: int, \n",
    "    min_per_stratum: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates sample allocation weights for proportional stratified sampling.\n",
    "\n",
    "    This corrected logic performs a two-step allocation:\n",
    "    1. Allocates the total QA budget to each stratum proportional to its volume.\n",
    "    2. Applies a minimum sample count per stratum and re-normalizes to stay within budget.\n",
    "    3. Distributes the final stratum allocation to each row within it, proportional to the row's volume.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The population DataFrame.\n",
    "        stratify_by_dimension (List[str]): The dimensions to stratify by.\n",
    "        total_qa_sessions (int): The total number of QA sessions to allocate.\n",
    "        min_per_stratum (int): The minimum number of sessions to allocate to any stratum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an added column 'temp_sampling_volume_stratified_proportional'.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Step 1: Create a unique 'strata' identifier\n",
    "    valid_stratify_dims = [col for col in stratify_by_dimension if col in df_copy.columns]\n",
    "    if not valid_stratify_dims:\n",
    "        warnings.warn(\"Stratification dimensions not found. Falling back to volume.\")\n",
    "        df_copy['temp_sampling_volume_stratified_proportional'] = df_copy['volume']\n",
    "        return df_copy\n",
    "        \n",
    "    df_copy['strata'] = df_copy[valid_stratify_dims].apply(lambda row: tuple(row), axis=1)\n",
    "\n",
    "    # Step 2: Calculate initial allocation at the STRATUM level\n",
    "    strata_summary = df_copy.groupby('strata').agg(\n",
    "        stratum_volume=('volume', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    total_volume = df_copy['volume'].sum()\n",
    "    if total_volume == 0:\n",
    "        df_copy['temp_sampling_volume_stratified_proportional'] = 0\n",
    "        return df_copy\n",
    "        \n",
    "    # Proportional allocation\n",
    "    strata_summary['initial_allocation'] = (strata_summary['stratum_volume'] / total_volume) * total_qa_sessions\n",
    "    \n",
    "    # Step 3: Apply minimum per stratum and re-normalize to respect the budget\n",
    "    strata_summary['floored_allocation'] = np.maximum(strata_summary['initial_allocation'], min_per_stratum)\n",
    "    \n",
    "    total_floored_allocation = strata_summary['floored_allocation'].sum()\n",
    "    if total_floored_allocation > 0:\n",
    "        # Scale down allocations so their sum equals the total QA budget\n",
    "        strata_summary['final_allocation'] = (strata_summary['floored_allocation'] / total_floored_allocation) * total_qa_sessions\n",
    "    else:\n",
    "        strata_summary['final_allocation'] = 0\n",
    "        \n",
    "    # Step 4: Map the final STRATUM allocation back to each ROW\n",
    "    df_copy = df_copy.merge(strata_summary[['strata', 'final_allocation', 'stratum_volume']], on='strata', how='left')\n",
    "    \n",
    "    # Step 5: Distribute the stratum's allocation to rows within it, proportional to row volume\n",
    "    # This correctly calculates the weight for each individual row.\n",
    "    df_copy['row_allocation_weight'] = (df_copy['volume'] / df_copy['stratum_volume'].replace(0, 1)) * df_copy['final_allocation']\n",
    "    \n",
    "    # Final step: Ensure a row is not allocated more samples than its total volume\n",
    "    df_copy['temp_sampling_volume_stratified_proportional'] = np.minimum(df_copy['row_allocation_weight'], df_copy['volume'])\n",
    "    \n",
    "    return df_copy.drop(columns=['strata', 'final_allocation', 'stratum_volume', 'row_allocation_weight'])\n",
    "\n",
    "def _calculate_equal_stratified_allocation(\n",
    "    df: pd.DataFrame, \n",
    "    stratify_by_dimension: List[str], \n",
    "    total_qa_sessions: int, \n",
    "    min_per_stratum: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates sample allocation weights for equal stratified sampling.\n",
    "\n",
    "    This logic allocates the total QA budget to each stratum equally,\n",
    "    applies a minimum sample count per stratum, re-normalizes to stay within budget,\n",
    "    and distributes the final stratum allocation to each row within it, proportional to the row's volume.\n",
    "    It also ensures that no row is allocated more samples than its natural volume.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The population DataFrame.\n",
    "        stratify_by_dimension (List[str]): The dimensions to stratify by.\n",
    "        total_qa_sessions (int): The total number of QA sessions to allocate.\n",
    "        min_per_stratum (int): The minimum number of sessions to allocate to any stratum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an added column 'temp_sampling_volume_stratified_equal'.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Step 1: Create a unique 'strata' identifier\n",
    "    valid_stratify_dims = [col for col in stratify_by_dimension if col in df_copy.columns]\n",
    "    if not valid_stratify_dims:\n",
    "        warnings.warn(\"Stratification dimensions not found. Falling back to volume.\")\n",
    "        df_copy['temp_sampling_volume_stratified_equal'] = df_copy['volume']\n",
    "        return df_copy\n",
    "        \n",
    "    df_copy['strata'] = df_copy[valid_stratify_dims].apply(lambda row: tuple(row), axis=1)\n",
    "\n",
    "    # Step 2: Calculate initial allocation at the STRATUM level\n",
    "    strata_summary = df_copy.groupby('strata').agg(\n",
    "        stratum_volume=('volume', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    num_strata = len(strata_summary)\n",
    "    if num_strata == 0:\n",
    "        df_copy['temp_sampling_volume_stratified_equal'] = 0\n",
    "        return df_copy\n",
    "\n",
    "    # Equal allocation\n",
    "    strata_summary['initial_allocation'] = total_qa_sessions / num_strata\n",
    "    \n",
    "    # Step 3: Apply minimum per stratum and re-normalize to respect the budget\n",
    "    strata_summary['floored_allocation'] = np.maximum(strata_summary['initial_allocation'], min_per_stratum)\n",
    "    \n",
    "    total_floored_allocation = strata_summary['floored_allocation'].sum()\n",
    "    if total_floored_allocation > 0:\n",
    "        # Scale down allocations so their sum equals the total QA budget\n",
    "        strata_summary['final_allocation'] = (strata_summary['floored_allocation'] / total_floored_allocation) * total_qa_sessions\n",
    "    else:\n",
    "        strata_summary['final_allocation'] = 0\n",
    "        \n",
    "    # Step 4: Map the final STRATUM allocation back to each ROW\n",
    "    df_copy = df_copy.merge(strata_summary[['strata', 'final_allocation', 'stratum_volume']], on='strata', how='left')\n",
    "    \n",
    "    # Step 5: Distribute the stratum's allocation to rows within it, proportional to row volume\n",
    "    # Ensure a row is not allocated more samples than its natural volume\n",
    "    df_copy['row_allocation_weight'] = (df_copy['volume'] / df_copy['stratum_volume'].replace(0, 1)) * df_copy['final_allocation']\n",
    "    df_copy['temp_sampling_volume_stratified_equal'] = np.minimum(df_copy['row_allocation_weight'], df_copy['volume'])\n",
    "\n",
    "    # Additionally, ensure that the sum of allocations for each customer does not exceed their total volume\n",
    "    if 'customer' in df_copy.columns:\n",
    "        customer_total_volume = df_copy.groupby('customer')['volume'].transform('sum')\n",
    "        customer_alloc_sum = df_copy.groupby('customer')['temp_sampling_volume_stratified_equal'].transform('sum')\n",
    "        excess = np.maximum(customer_alloc_sum - customer_total_volume, 0)\n",
    "        # If excess exists, proportionally reduce allocations for that customer\n",
    "        if excess.any():\n",
    "            reduction_factor = np.where(customer_alloc_sum > 0, \n",
    "                                       (customer_total_volume / customer_alloc_sum), \n",
    "                                       1)\n",
    "            df_copy['temp_sampling_volume_stratified_equal'] = df_copy['temp_sampling_volume_stratified_equal'] * reduction_factor\n",
    "\n",
    "    return df_copy.drop(columns=['strata', 'final_allocation', 'stratum_volume', 'row_allocation_weight'])\n",
    "\n",
    "\n",
    "# def _calculate_equal_stratified_allocation(\n",
    "#     df: pd.DataFrame, \n",
    "#     stratify_by_dimension: List[str], \n",
    "#     total_qa_sessions: int, \n",
    "#     min_per_stratum: int\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Calculates sample allocation weights for equal stratified sampling.\n",
    "\n",
    "#     This logic allocates the total QA budget to each stratum equally,\n",
    "#     applies a minimum sample count per stratum, re-normalizes to stay within budget,\n",
    "#     and distributes the final stratum allocation to each row within it, proportional to the row's volume.\n",
    "#     It also ensures that no row is allocated more samples than its natural volume.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): The population DataFrame.\n",
    "#         stratify_by_dimension (List[str]): The dimensions to stratify by.\n",
    "#         total_qa_sessions (int): The total number of QA sessions to allocate.\n",
    "#         min_per_stratum (int): The minimum number of sessions to allocate to any stratum.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: The original DataFrame with an added column 'temp_sampling_volume_stratified_equal'.\n",
    "#     \"\"\"\n",
    "#     df_copy = df.copy()\n",
    "\n",
    "#     # Step 1: Create a unique 'strata' identifier\n",
    "#     valid_stratify_dims = [col for col in stratify_by_dimension if col in df_copy.columns]\n",
    "#     if not valid_stratify_dims:\n",
    "#         warnings.warn(\"Stratification dimensions not found. Falling back to volume.\")\n",
    "#         df_copy['temp_sampling_volume_stratified_equal'] = df_copy['volume']\n",
    "#         return df_copy\n",
    "        \n",
    "#     df_copy['strata'] = df_copy[valid_stratify_dims].apply(lambda row: tuple(row), axis=1)\n",
    "\n",
    "#     # Step 2: Calculate initial allocation at the STRATUM level\n",
    "#     strata_summary = df_copy.groupby('strata').agg(\n",
    "#         stratum_volume=('volume', 'sum')\n",
    "#     ).reset_index()\n",
    "    \n",
    "#     num_strata = len(strata_summary)\n",
    "#     if num_strata == 0:\n",
    "#         df_copy['temp_sampling_volume_stratified_equal'] = 0\n",
    "#         return df_copy\n",
    "\n",
    "#     # Equal allocation\n",
    "#     strata_summary['initial_allocation'] = total_qa_sessions / num_strata\n",
    "    \n",
    "#     # Step 3: Apply minimum per stratum and re-normalize to respect the budget\n",
    "#     strata_summary['floored_allocation'] = np.maximum(strata_summary['initial_allocation'], min_per_stratum)\n",
    "    \n",
    "#     total_floored_allocation = strata_summary['floored_allocation'].sum()\n",
    "#     if total_floored_allocation > 0:\n",
    "#         # Scale down allocations so their sum equals the total QA budget\n",
    "#         strata_summary['final_allocation'] = (strata_summary['floored_allocation'] / total_floored_allocation) * total_qa_sessions\n",
    "#     else:\n",
    "#         strata_summary['final_allocation'] = 0\n",
    "        \n",
    "#     # Step 4: Map the final STRATUM allocation back to each ROW\n",
    "#     df_copy = df_copy.merge(strata_summary[['strata', 'final_allocation', 'stratum_volume']], on='strata', how='left')\n",
    "    \n",
    "#     # Step 5: Distribute the stratum's allocation to rows within it, proportional to row volume\n",
    "#     df_copy['row_allocation_weight'] = (df_copy['volume'] / df_copy['stratum_volume'].replace(0, 1)) * df_copy['final_allocation']\n",
    "    \n",
    "#     # Final step: Ensure a row is not allocated more samples than its total volume\n",
    "#     df_copy['temp_sampling_volume_stratified_equal'] = np.minimum(df_copy['row_allocation_weight'], df_copy['volume'])\n",
    "    \n",
    "#     return df_copy.drop(columns=['strata', 'final_allocation', 'stratum_volume', 'row_allocation_weight'])\n",
    "\n",
    "def run_simulations(population_df: pd.DataFrame, global_avg_metrics: Dict[str, float], overall_avg_frequencies_in_qa: Dict[str, float],\n",
    "                    selected_metric: str, reporting_dimensions: List[str],\n",
    "                    error_margin_logic: str, absolute_error_margin_value: float,\n",
    "                    relative_error_margin_value: float, min_absolute_error_margin: float,\n",
    "                    target_overall_error_percent: float, target_dimension_error_percent: float,\n",
    "                    weight_factor_small_customers: float,\n",
    "                    small_customer_volume_quantile_threshold: float, stratify_by_dimension: List[str],\n",
    "                    sampling_types_to_run: List[str], top_n: int, z_score: float, min_per_stratum: int = 500, verbose = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Runs various sampling simulations to evaluate different strategies\n",
    "    for achieving target error margins and cost efficiency.\n",
    "    \"\"\"\n",
    "    simulation_results = {}\n",
    "    current_total_qa = population_df['qa_reviewed_for_reporting'].sum()\n",
    "    \n",
    "    def filter_existing_columns(df, columns):\n",
    "        return [col for col in columns if col in df.columns]\n",
    "\n",
    "    SCENARIO_CONFIGS = {\n",
    "        # 'Random': {\n",
    "        #     'type': 'random_multiples',\n",
    "        #     'scales': [0.3, 0.5, 0.7, 1, 1.5, 2, 3]\n",
    "        # },\n",
    "        'Current Random': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1, 0.3, 0.5, 0.7, 2, 3],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'qa_reviewed',\n",
    "            'df_modifier_func': lambda df: df\n",
    "        },\n",
    "        'True Random': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1, 0.3, 0.5, 0.7, 2, 3],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'qa_reviewed_for_reporting',\n",
    "            'df_modifier_func': lambda df: df\n",
    "        },\n",
    "        'Oversample Small Traditional': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'temp_sampling_volume_small',\n",
    "            'df_modifier_func': lambda df: (\n",
    "                df.merge(\n",
    "                    df.groupby('customer')['volume'].sum().rename('customer_total_volume'),\n",
    "                    left_on='customer', right_index=True\n",
    "                ).assign(\n",
    "                    temp_sampling_volume_small=lambda x: np.where(\n",
    "                        x['customer_total_volume'] < x['customer_total_volume'].quantile(small_customer_volume_quantile_threshold),\n",
    "                        x['volume'] * weight_factor_small_customers,\n",
    "                        x['volume']\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        },\n",
    "        'Client-Tier Weighting': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'temp_sampling_volume_tier',\n",
    "            # 'df_modifier_func': lambda df: df.assign(\n",
    "            #     temp_sampling_volume_tier=df['customer_priority'].map(\n",
    "            #         {'Tier Platinum': 3, 'Tier 1': 2, 'Tier 2': 1, 'Tier 3': 1, 'Tier 4': 1, 'Tier 5': 1, '': 1}\n",
    "            #     ).fillna(1) * df['volume']\n",
    "            #)\n",
    "            'df_modifier_func': lambda df: (\n",
    "                lambda weights: df.assign(\n",
    "                    temp_sampling_volume_tier=df['customer_priority'].map(weights).fillna(1) * df['volume']\n",
    "                )\n",
    "            )(get_tier_weights_by_target_proportion(df, TARGET_QA_PROPORTIONS))\n",
    "        },\n",
    "        'Proportional Stratified Sampling': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1,2],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'temp_sampling_volume_stratified_proportional',\n",
    "            'df_modifier_func': lambda df: _calculate_proportional_stratified_allocation(\n",
    "                df=df,\n",
    "                stratify_by_dimension=stratify_by_dimension,\n",
    "                total_qa_sessions=current_total_qa,\n",
    "                min_per_stratum=min_per_stratum\n",
    "            )\n",
    "            # 'df_modifier_func': lambda df, stratify_by_dimension=stratify_by_dimension: (\n",
    "            #     (lambda _df, strata_volumes, total_pop_volume, min_per_stratum, customer_total_volumes: \n",
    "            #         _df\n",
    "            #         .groupby('strata', group_keys=False)\n",
    "            #         .apply(lambda g: (\n",
    "            #             g.assign(\n",
    "            #                 temp_sampling_volume_stratified_proportional=(\n",
    "            #                     np.minimum(\n",
    "            #                         np.maximum(\n",
    "            #                             g['volume'] / g['volume'].sum() * (strata_volumes[g.name] / total_pop_volume * current_total_qa)\n",
    "            #                             if g['volume'].sum() > 0 and total_pop_volume > 0 else 0,\n",
    "            #                             min_per_stratum / len(g) if len(g) > 0 else 0\n",
    "            #                         ),\n",
    "            #                         g['volume']\n",
    "            #                     )\n",
    "            #                 )\n",
    "            #             )\n",
    "            #             # Cap at customer's total volume (across all strata)\n",
    "            #             .assign(\n",
    "            #                 temp_sampling_volume_stratified_proportional=lambda x: np.minimum(\n",
    "            #                     x['temp_sampling_volume_stratified_proportional'],\n",
    "            #                     x['customer'].map(customer_total_volumes)\n",
    "            #                 )\n",
    "            #             )\n",
    "            #         ))\n",
    "            #         .reset_index(drop=True)\n",
    "            #     )(\n",
    "            #         df.assign(strata=df[filter_existing_columns(df, stratify_by_dimension)].apply(lambda row: tuple(row), axis=1)),\n",
    "            #         df.groupby(df[filter_existing_columns(df, stratify_by_dimension)].apply(lambda row: tuple(row), axis=1))['volume'].sum(),\n",
    "            #         df['volume'].sum(),\n",
    "            #         min_per_stratum,\n",
    "            #         df.groupby('customer')['volume'].sum().to_dict()\n",
    "            #     )\n",
    "            # )\n",
    "        },\n",
    "        'Equal Stratified Sampling': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1,2],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'temp_sampling_volume_stratified_equal',\n",
    "            'df_modifier_func': lambda df: _calculate_equal_stratified_allocation(\n",
    "                df=df,\n",
    "                stratify_by_dimension=stratify_by_dimension,\n",
    "                total_qa_sessions=current_total_qa,\n",
    "                min_per_stratum=min_per_stratum\n",
    "            )\n",
    "            # 'df_modifier_func': lambda df, stratify_by_dimension=stratify_by_dimension: (\n",
    "            #     (lambda _df, n_strata, min_per_stratum, customer_total_volumes:\n",
    "            #         _df\n",
    "            #         .groupby('strata', group_keys=False)\n",
    "            #         .apply(lambda g: (\n",
    "            #             g.assign(\n",
    "            #                 temp_sampling_volume_stratified_equal=(\n",
    "            #                     np.minimum(\n",
    "            #                         np.maximum(\n",
    "            #                             (current_total_qa // n_strata) / len(g) if len(g) > 0 else 0,\n",
    "            #                             min_per_stratum / len(g) if len(g) > 0 else 0\n",
    "            #                         ),\n",
    "            #                         g['volume']\n",
    "            #                     )\n",
    "            #                 )\n",
    "            #             )\n",
    "            #             .assign(\n",
    "            #                 temp_sampling_volume_stratified_equal=lambda x: np.minimum(\n",
    "            #                     x['temp_sampling_volume_stratified_equal'],\n",
    "            #                     x['customer'].map(customer_total_volumes)\n",
    "            #                 )\n",
    "            #             )\n",
    "            #         ))\n",
    "            #         .reset_index(drop=True)\n",
    "            #     )(\n",
    "            #         df.assign(strata=df[filter_existing_columns(df, stratify_by_dimension)].apply(lambda row: tuple(row), axis=1)),\n",
    "            #         df[filter_existing_columns(df, stratify_by_dimension)].apply(lambda row: tuple(row), axis=1).nunique(),\n",
    "            #         min_per_stratum,\n",
    "            #         df.groupby('customer')['volume'].sum().to_dict()\n",
    "            #     )\n",
    "            # )\n",
    "        },\n",
    "        'Volume-Based Stratified Sampling': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'temp_sampling_volume_strat',\n",
    "            'df_modifier_func': _prepare_volume_stratified_df\n",
    "        },\n",
    "        'Biased with Exponents': {\n",
    "            'type': 'biased_multiples',\n",
    "            'scales': [1],\n",
    "            'exponents': [0.3, 0.7, 0.8, 1.5]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for scenario_name in sampling_types_to_run:\n",
    "        if scenario_name not in SCENARIO_CONFIGS:\n",
    "            if verbose:\n",
    "                print(f\"Warning: Unknown simulation scenario '{scenario_name}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        config = SCENARIO_CONFIGS[scenario_name]\n",
    "        \n",
    "        if config['type'] == 'random_multiples':\n",
    "            if verbose:\n",
    "                print(f'\\n--- Running {scenario_name} SIMULATIONS ---')\n",
    "            for multiplier in config['scales']:\n",
    "                target_sessions = int(current_total_qa * multiplier)\n",
    "                if target_sessions == 0 and current_total_qa != 0:\n",
    "                    continue\n",
    "                name = f'{scenario_name} {multiplier:.1f}x' if current_total_qa > 0 else f'{scenario_name} {target_sessions}'\n",
    "                if verbose:\n",
    "                    print(f\"  - Simulation: {name} (Target Sessions: {target_sessions})\")\n",
    "\n",
    "                scenario_results = _run_single_simulation_scenario(\n",
    "                    population_df, target_sessions, 'random', 1, 'volume',\n",
    "                    selected_metric, global_avg_metrics, overall_avg_frequencies_in_qa, reporting_dimensions,\n",
    "                    error_margin_logic, absolute_error_margin_value, relative_error_margin_value, min_absolute_error_margin,\n",
    "                    z_score, population_df # Pass original population_df for metrics\n",
    "                )\n",
    "                simulation_results[name] = scenario_results\n",
    "\n",
    "        elif config['type'] == 'single_run':\n",
    "            # print(f'\\n--- Running {scenario_name} SIMULATION ---')\n",
    "            # target_sessions = current_total_qa \n",
    "            for multiplier in config['scales']:\n",
    "                target_sessions = int(current_total_qa * multiplier)\n",
    "                name = f'{scenario_name} {multiplier:.1f}x' if current_total_qa > 0 else f'{scenario_name} {target_sessions}'\n",
    "                if target_sessions == 0: \n",
    "                    if verbose:\n",
    "                        print(f\"  - Skipping '{scenario_name}' simulation due to 0 target sessions.\")\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print(f\"  - Simulation: {scenario_name} (Target Sessions: {target_sessions})\")\n",
    "\n",
    "                if scenario_name == 'Volume-Based Stratified Sampling':\n",
    "                    df_modified = config['df_modifier_func'](population_df.copy(), target_sessions)\n",
    "                elif scenario_name in ['Proportional Stratified Sampling', 'Equal Stratified Sampling']:\n",
    "                    if verbose:\n",
    "                        print(\"Stratified by:\", stratify_by_dimension)\n",
    "                    df_modified = config['df_modifier_func'](population_df.copy())\n",
    "                #     df_modified = config['df_modifier_func'](population_df.copy(), stratify_by_dimension)\n",
    "                else:\n",
    "                    df_modified = config['df_modifier_func'](population_df.copy())\n",
    "                \n",
    "                scenario_results = _run_single_simulation_scenario(\n",
    "                    df_modified, target_sessions, config['sampling_type'], config['exponent'], \n",
    "                    config['volume_col_modifier'],\n",
    "                    selected_metric, global_avg_metrics, overall_avg_frequencies_in_qa, reporting_dimensions,\n",
    "                    error_margin_logic, absolute_error_margin_value, relative_error_margin_value,\n",
    "                    min_absolute_error_margin, z_score, population_df# Pass original population_df for metrics\n",
    "                )\n",
    "                simulation_results[name] = scenario_results\n",
    "\n",
    "        elif config['type'] == 'biased_multiples':\n",
    "            if verbose:\n",
    "                print(f'\\n--- Running {scenario_name} SIMULATIONS ---')\n",
    "            for exp in config['exponents']:\n",
    "                target_sessions = current_total_qa\n",
    "                if target_sessions == 0: continue\n",
    "                name = f'Biased Exp={exp}'\n",
    "                if verbose:\n",
    "                    print(f\"  - Simulation: {name} (Target Sessions: {target_sessions}, Exponent: {exp})\")\n",
    "\n",
    "                scenario_results = _run_single_simulation_scenario(\n",
    "                    population_df, target_sessions, 'biased', exp, 'volume',\n",
    "                    selected_metric, global_avg_metrics, overall_avg_frequencies_in_qa, reporting_dimensions,\n",
    "                    error_margin_logic, absolute_error_margin_value, relative_error_margin_value,\n",
    "                    min_absolute_error_margin, z_score, population_df # Pass original population_df for metrics\n",
    "                )\n",
    "                simulation_results[name] = scenario_results\n",
    "    \n",
    "\n",
    "    # # User-defined constraints and weights (could be UI-driven)\n",
    "    # CONSTRAINTS = {\n",
    "    #     'overall_margin': lambda x: x <= TARGET_OVERALL_ERROR_PERCENT / 100,\n",
    "    #     'customer_coverage': lambda x: x >= 1,  # e.g., at least 1 customer below threshold\n",
    "    #     'product_coverage': lambda x: x >= 1,  # e.g., at least 1 product below threshold\n",
    "    #     'industry_coverage': lambda x: x >= 1,  # e.g., at least 1 industry below threshold\n",
    "    #     'document_country_coverage': lambda x: x >= 1,  # e.g., at least 1 document country below threshold\n",
    "    #     'region_coverage': lambda x: x >= 1,  # e.g., at least 1 region below threshold\n",
    "    #     'share_strategic_clients_met_sla': lambda x: x >= 0.2,  # e.g., at least 20% of strategic clients met SLA\n",
    "    # }\n",
    "    # WEIGHTS = {\n",
    "    #     'cost_total': -1,  # minimize\n",
    "    #     'customer_coverage': 2,  # maximize\n",
    "    #     'overall_margin': -1,  # minimize\n",
    "    #     'share_strategic_clients_met_sla': 1,  # maximize\n",
    "    #     'product_coverage': 1,  # maximize\n",
    "    #     'industry_coverage': 1,  # maximize\n",
    "    #     'document_country_coverage': 1,  # maximize\n",
    "    #     'region_coverage': 1,  # maximize\n",
    "    #     'share_top_3_customers': 1,  # maximize\n",
    "    #     'total_sampled': -1  # minimize total sampled sessions\n",
    "    # }\n",
    "\n",
    "    # def score_simulation(res):\n",
    "    #     score = 0\n",
    "    #     for k, w in WEIGHTS.items():\n",
    "    #         v = res.get(k, 0)\n",
    "    #         score += w * (v if pd.notnull(v) else 0)\n",
    "    #     return score\n",
    "\n",
    "    # # Filter by constraints\n",
    "    # filtered = [res for res in simulation_results.values() if all(f(res.get(k, np.nan)) for k, f in CONSTRAINTS.items())]\n",
    "    # if filtered:\n",
    "    #     best = max(filtered, key=score_simulation)\n",
    "    # else:\n",
    "    #     best = max(simulation_results.values(), key=score_simulation)\n",
    "    # print(\"Best scenario by multi-objective score:\", best.get('strategy_name', 'N/A'))\n",
    "\n",
    "    # --- Optimal Solution Determination ---\n",
    "    optimal_size_info = None\n",
    "    max_coverage = -1\n",
    "    \n",
    "    for sim_name, res in simulation_results.items():\n",
    "        if res['overall_margin'] == np.inf: continue\n",
    "\n",
    "        overall_margin_percent = res['overall_margin'] * 100\n",
    "        current_total_coverage = res['customer_coverage'] + res['product_coverage'] + res['industry_coverage'] + res['document_country_coverage'] + res['region_coverage']\n",
    "\n",
    "        if overall_margin_percent <= target_overall_error_percent:\n",
    "            if optimal_size_info is None or current_total_coverage > max_coverage:\n",
    "                max_coverage = current_total_coverage\n",
    "                optimal_size_info = res\n",
    "                optimal_size_info['strategy_name'] = sim_name\n",
    "            elif current_total_coverage == max_coverage:\n",
    "                if res['total_sampled'] < optimal_size_info['total_sampled']:\n",
    "                    optimal_size_info = res\n",
    "                    optimal_size_info['strategy_name'] = sim_name\n",
    "\n",
    "    if optimal_size_info is None:\n",
    "        if verbose:\n",
    "            print(\"\\nNo simulation met the overall error margin target. Finding the simulation with the highest coverage regardless of overall margin.\")\n",
    "        max_coverage_any_margin = -1\n",
    "        for sim_name, res in simulation_results.items():\n",
    "            if res['overall_margin'] == np.inf: continue\n",
    "            current_total_coverage = res['customer_coverage'] + res['product_coverage'] + res['industry_coverage'] + res['document_country_coverage'] + res['region_coverage']\n",
    "            if optimal_size_info is None or current_total_coverage > max_coverage_any_margin:\n",
    "                max_coverage_any_margin = current_total_coverage\n",
    "                optimal_size_info = res\n",
    "                optimal_size_info['strategy_name'] = sim_name\n",
    "            elif current_total_coverage == max_coverage_any_margin:\n",
    "                if res['total_sampled'] < optimal_size_info['total_sampled']:\n",
    "                    optimal_size_info = res\n",
    "                    optimal_size_info['strategy_name'] = sim_name\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n## Optimal Solution Summary')\n",
    "        if optimal_size_info:\n",
    "            # Show which threshold was used\n",
    "            if ERROR_MARGIN_LOGIC == 'Absolute Error Margin':\n",
    "                threshold_type = \"Absolute Error Margin\"\n",
    "                threshold_value = ABSOLUTE_ERROR_MARGIN_VALUE * 100\n",
    "                threshold_unit = \"%\"\n",
    "            else:\n",
    "                threshold_type = \"Relative Error Margin\"\n",
    "                threshold_value = RELATIVE_ERROR_MARGIN_VALUE * 100\n",
    "                threshold_unit = \"% of group rate\"\n",
    "\n",
    "            print(f\"Threshold logic used: {threshold_type} (Threshold: {threshold_value:.2f}{threshold_unit})\")\n",
    "            optimal_size = optimal_size_info['total_sampled']\n",
    "            print(f\"Optimal sample size: {optimal_size} sessions (Strategy: {optimal_size_info['strategy_name']})\")\n",
    "            print(f\"  - Overall error margin for this strategy: {optimal_size_info['overall_margin']*100:.2f}%\")\n",
    "            print(f\"  - Covers {optimal_size_info['customer_coverage']} customers below threshold.\")\n",
    "            print(f\"  - Customers meeting target: {optimal_size_info['customer_list']}\")\n",
    "            print(f\"  - Covers {optimal_size_info['product_coverage']} products below threshold.\")\n",
    "            print(f\"  - Products meeting target: {optimal_size_info['product_list']}\")\n",
    "            print(f\"  - Covers {optimal_size_info['industry_coverage']} industries below threshold.\")\n",
    "            print(f\"  - Industries meeting target: {optimal_size_info['industry_list']}\")\n",
    "            print(f\"  - Covers {optimal_size_info['document_country_coverage']} document countries below threshold.\")\n",
    "            print(f\"  - Document countries meeting target: {optimal_size_info['document_country_list']}\")\n",
    "            print(f\"  - Covers {optimal_size_info['region_coverage']} regions below threshold.\")\n",
    "            print(f\"  - Regions meeting target: {optimal_size_info['region_list']}\")\n",
    "            print(f\"  - Share of top 3 customers: {optimal_size_info['share_top_3_customers']:.2f}%\")\n",
    "            print(f\"  - Share of strategic clients meeting SLA: {optimal_size_info['share_strategic_clients_met_sla']:.2f}%\")\n",
    "            print(f\"  - Total Cost for this strategy: ${optimal_size_info['cost_total']:.2f} EUR\")\n",
    "        else:\n",
    "            print(\"Could not determine an optimal sample size. Defaulting to current total QA reviewed sessions for plotting.\")\n",
    "\n",
    "    if verbose:\n",
    "        # Plotly Visualizations\n",
    "        summary_df = pd.DataFrame.from_dict(simulation_results, orient='index')\n",
    "        if summary_df.empty:\n",
    "            print(\"\\nNo simulation results to plot.\")\n",
    "            return simulation_results # Return empty results\n",
    "\n",
    "        summary_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        summary_df['overall_margin_perc'] = summary_df['overall_margin'] * 100\n",
    "\n",
    "        # Only include dimension_coverage if reporting_dimensions is not ['customer'] or ['product']\n",
    "        if REPORTING_DIMENSIONS == ['customer'] or REPORTING_DIMENSIONS == ['product']:\n",
    "            coverage_cols = ['customer_coverage', 'product_coverage', 'industry_coverage', 'document_country_coverage', 'region_coverage']\n",
    "        else:\n",
    "            coverage_cols = ['dimension_coverage', 'customer_coverage', 'product_coverage', 'industry_coverage', 'document_country_coverage', 'region_coverage']\n",
    "\n",
    "        if all(col in summary_df.columns for col in coverage_cols):\n",
    "            create_heatmap(\n",
    "                df=summary_df[coverage_cols].T,\n",
    "                title='Coverage Below Target Margin per Simulation',\n",
    "                filename_suffix='simulation_coverage_heatmap',\n",
    "                save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Missing coverage columns for imshow plot.\")\n",
    "\n",
    "        if all(col in summary_df.columns for col in coverage_cols + ['overall_margin_perc']):\n",
    "            create_combined_bar_line_chart(\n",
    "                df=summary_df,\n",
    "                x_col_or_index_values=summary_df.index,\n",
    "                bar_cols=coverage_cols,\n",
    "                line_col='overall_margin_perc',\n",
    "                title='Coverage and Overall Error Margins Across Simulations',\n",
    "                y1_axis_title='Coverage Count',\n",
    "                y2_axis_title='Overall Error Margin (%)',\n",
    "                filename_suffix='simulation_coverage_and_margin_bar',\n",
    "                save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Missing required columns for Coverage and Overall Error Margins plot.\")\n",
    "\n",
    "        if 'cost_total' in summary_df.columns and summary_df['cost_total'].dropna().any():\n",
    "            create_bar_chart(\n",
    "                df=summary_df,\n",
    "                x_col_or_index_values=summary_df.index,\n",
    "                y_cols=['cost_total'],\n",
    "                title='Total Cost Across Strategies',\n",
    "                y_axis_title='Total Cost (EUR)',\n",
    "                x_axis_title='Simulation',\n",
    "                filename_suffix='simulation_cost_total',\n",
    "                save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Missing 'cost_total' column or all values are NaN/inf for plotting.\")\n",
    "\n",
    "        print('\\n## Detailed Plots for Optimal Solution')\n",
    "\n",
    "        if optimal_size_info:\n",
    "            optimal_strategy_name = optimal_size_info['strategy_name']\n",
    "            optimal_target_sessions = optimal_size_info['total_sampled']\n",
    "\n",
    "            plot_sampling_type = 'random'\n",
    "            plot_exponent = 1.0\n",
    "            plot_volume_col = 'volume'\n",
    "            plot_df_source = population_df.copy()\n",
    "\n",
    "            if 'Oversample Small' in optimal_strategy_name:\n",
    "                plot_df_source = SCENARIO_CONFIGS['Oversample Small Traditional']['df_modifier_func'](plot_df_source)\n",
    "                plot_volume_col = SCENARIO_CONFIGS['Oversample Small Traditional']['volume_col_modifier']\n",
    "            elif 'Client-Tier Weighting' in optimal_strategy_name:\n",
    "                plot_df_source = SCENARIO_CONFIGS['Client-Tier Weighting']['df_modifier_func'](plot_df_source)\n",
    "                plot_volume_col = SCENARIO_CONFIGS['Client-Tier Weighting']['volume_col_modifier']\n",
    "            elif 'Proportional Stratified' in optimal_strategy_name:\n",
    "                plot_df_source = SCENARIO_CONFIGS['Proportional Stratified Sampling']['df_modifier_func'](plot_df_source)\n",
    "                plot_volume_col = SCENARIO_CONFIGS['Proportional Stratified Sampling']['volume_col_modifier']\n",
    "            elif 'Equal Stratified' in optimal_strategy_name:\n",
    "                plot_df_source = SCENARIO_CONFIGS['Equal Stratified Sampling']['df_modifier_func'](plot_df_source)\n",
    "                plot_volume_col = SCENARIO_CONFIGS['Equal Stratified Sampling']['volume_col_modifier']\n",
    "            elif 'Volume-Based Stratified' in optimal_strategy_name:\n",
    "                plot_df_source = SCENARIO_CONFIGS['Volume-Based Stratified Sampling']['df_modifier_func'](plot_df_source, optimal_target_sessions)\n",
    "                plot_volume_col = SCENARIO_CONFIGS['Volume-Based Stratified Sampling']['volume_col_modifier']\n",
    "            elif 'Biased Exp=' in optimal_strategy_name:\n",
    "                plot_sampling_type = 'biased'\n",
    "                plot_exponent = float(optimal_strategy_name.split('=')[1])\n",
    "                plot_volume_col = 'volume'\n",
    "\n",
    "            optimal_sim_df_for_plots = calculate_sample_counts(plot_df_source, total_target_sessions=optimal_target_sessions, \n",
    "                                                                groupby_dimensions=REPORTING_DIMENSIONS, \n",
    "                                                                sampling_type=plot_sampling_type, exponent=plot_exponent, \n",
    "                                                                volume_col_for_sampling=plot_volume_col,\n",
    "                                                                selected_metric=selected_metric, global_rates=global_avg_metrics,\n",
    "                                                                overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa, z_score=z_score)\n",
    "\n",
    "            sort_col_for_plots = 'simulated_total_sessions'\n",
    "\n",
    "            if 'customer' in optimal_sim_df_for_plots.columns:\n",
    "                # Aggregate by customer, sum numerators/denominators, recalculate margin\n",
    "                if 'used_global_rate_fallback' not in optimal_sim_df_for_plots.columns:\n",
    "                    optimal_sim_df_for_plots['used_global_rate_fallback'] = False\n",
    "\n",
    "                agg_customers = (\n",
    "                    optimal_sim_df_for_plots\n",
    "                    .groupby('customer', as_index=False)\n",
    "                    .agg({\n",
    "                        'simulated_num_x': 'sum',\n",
    "                        'simulated_den_n': 'sum',\n",
    "                        'simulated_total_sessions': 'sum',\n",
    "                        'used_global_rate_fallback': 'any'\n",
    "                    })\n",
    "                )\n",
    "                margins = calculate_margin_wilson(\n",
    "                    agg_customers['simulated_num_x'],\n",
    "                    agg_customers['simulated_den_n'],\n",
    "                    z_score\n",
    "                )\n",
    "                agg_customers['margin'] = margins['margin']\n",
    "\n",
    "                # Top N + Rest logic (optional, similar to your _aggregate_top_n_for_table_and_plot)\n",
    "                if len(agg_customers) > top_n:\n",
    "                    top_n_df = agg_customers.sort_values('simulated_total_sessions', ascending=False).head(top_n)\n",
    "                    rest_df = agg_customers.sort_values('simulated_total_sessions', ascending=False).iloc[top_n:]\n",
    "                    rest_row = {\n",
    "                        'customer': 'Rest',\n",
    "                        'simulated_num_x': rest_df['simulated_num_x'].sum(),\n",
    "                        'simulated_den_n': rest_df['simulated_den_n'].sum(),\n",
    "                        'simulated_total_sessions': rest_df['simulated_total_sessions'].sum()\n",
    "                    }\n",
    "                    rest_row['margin'] = calculate_margin_wilson(\n",
    "                        rest_row['simulated_num_x'],\n",
    "                        rest_row['simulated_den_n'],\n",
    "                        z_score\n",
    "                    )['margin']\n",
    "                    agg_customers_plot = pd.concat([top_n_df, pd.DataFrame([rest_row])], ignore_index=True)\n",
    "                else:\n",
    "                    agg_customers_plot = agg_customers\n",
    "\n",
    "                if not agg_customers_plot.empty:\n",
    "                    create_bar_chart(\n",
    "                        df=agg_customers_plot, x_col_or_index_values='customer', y_cols=['margin'],\n",
    "                        title=f'Error Margins for Top {top_n} Customers (Rest Aggregated) in Optimal Simulation ({optimal_strategy_name})',\n",
    "                        y_axis_title='Error Margin', x_axis_title='Customer',\n",
    "                        line_data={\n",
    "                            'y_value': ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "                            'name': f'Target {ABSOLUTE_ERROR_MARGIN_VALUE:.2f}',\n",
    "                            'color': \"Red\", 'width': 2, 'dash': \"dash\"\n",
    "                        },\n",
    "                        filename_suffix=\"optimal_sim_customers_margin\",\n",
    "                        save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"No customer data available for plotting in optimal simulation.\")\n",
    "            else:\n",
    "                warnings.warn(\"Customer dimension not found in optimal simulation DataFrame for plotting.\")\n",
    "\n",
    "            if 'primary_product' in optimal_sim_df_for_plots.columns:\n",
    "                agg_products = (\n",
    "                    optimal_sim_df_for_plots\n",
    "                    .groupby('primary_product', as_index=False)\n",
    "                    .agg({\n",
    "                        'simulated_num_x': 'sum',\n",
    "                        'simulated_den_n': 'sum',\n",
    "                        'simulated_total_sessions': 'sum'\n",
    "                    })\n",
    "                )\n",
    "                margins = calculate_margin_wilson(\n",
    "                    agg_products['simulated_num_x'],\n",
    "                    agg_products['simulated_den_n'],\n",
    "                    z_score\n",
    "                )\n",
    "                agg_products['margin'] = margins['margin']\n",
    "\n",
    "                if len(agg_products) > top_n:\n",
    "                    top_n_df = agg_products.sort_values('simulated_total_sessions', ascending=False).head(top_n)\n",
    "                    rest_df = agg_products.sort_values('simulated_total_sessions', ascending=False).iloc[top_n:]\n",
    "                    rest_row = {\n",
    "                        'primary_product': 'Rest',\n",
    "                        'simulated_num_x': rest_df['simulated_num_x'].sum(),\n",
    "                        'simulated_den_n': rest_df['simulated_den_n'].sum(),\n",
    "                        'simulated_total_sessions': rest_df['simulated_total_sessions'].sum()\n",
    "                    }\n",
    "                    rest_row['margin'] = calculate_margin_wilson(\n",
    "                        rest_row['simulated_num_x'],\n",
    "                        rest_row['simulated_den_n'],\n",
    "                        z_score\n",
    "                    )['margin']\n",
    "                    agg_products_plot = pd.concat([top_n_df, pd.DataFrame([rest_row])], ignore_index=True)\n",
    "                else:\n",
    "                    agg_products_plot = agg_products\n",
    "\n",
    "                if not agg_products_plot.empty:\n",
    "                    create_bar_chart(\n",
    "                        df=agg_products_plot, x_col_or_index_values='primary_product', y_cols=['margin'],\n",
    "                        title=f'Error Margins for Top {top_n} Primary Products (Rest Aggregated) in Optimal Simulation ({optimal_strategy_name})',\n",
    "                        y_axis_title='Error Margin', x_axis_title='Primary Product',\n",
    "                        line_data={\n",
    "                            'y_value': ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "                            'name': f'Target {ABSOLUTE_ERROR_MARGIN_VALUE:.2f}',\n",
    "                            'color': \"Red\", 'width': 2, 'dash': \"dash\"\n",
    "                        },\n",
    "                        filename_suffix=\"optimal_sim_products_margin\",\n",
    "                        save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"No primary product data available for plotting in optimal simulation.\")\n",
    "            else:\n",
    "                warnings.warn(\"Primary Product dimension not found in optimal simulation DataFrame for plotting.\")\n",
    "\n",
    "            if 'document_country' in optimal_sim_df_for_plots.columns:\n",
    "                agg_products = (\n",
    "                    optimal_sim_df_for_plots\n",
    "                    .groupby('document_country', as_index=False)\n",
    "                    .agg({\n",
    "                        'simulated_num_x': 'sum',\n",
    "                        'simulated_den_n': 'sum',\n",
    "                        'simulated_total_sessions': 'sum'\n",
    "                    })\n",
    "                )\n",
    "                margins = calculate_margin_wilson(\n",
    "                    agg_products['simulated_num_x'],\n",
    "                    agg_products['simulated_den_n'],\n",
    "                    z_score\n",
    "                )\n",
    "                agg_products['margin'] = margins['margin']\n",
    "\n",
    "                if len(agg_products) > top_n:\n",
    "                    top_n_df = agg_products.sort_values('simulated_total_sessions', ascending=False).head(top_n)\n",
    "                    rest_df = agg_products.sort_values('simulated_total_sessions', ascending=False).iloc[top_n:]\n",
    "                    rest_row = {\n",
    "                        'document_country': 'Rest',\n",
    "                        'simulated_num_x': rest_df['simulated_num_x'].sum(),\n",
    "                        'simulated_den_n': rest_df['simulated_den_n'].sum(),\n",
    "                        'simulated_total_sessions': rest_df['simulated_total_sessions'].sum()\n",
    "                    }\n",
    "                    rest_row['margin'] = calculate_margin_wilson(\n",
    "                        rest_row['simulated_num_x'],\n",
    "                        rest_row['simulated_den_n'],\n",
    "                        z_score\n",
    "                    )['margin']\n",
    "                    agg_products_plot = pd.concat([top_n_df, pd.DataFrame([rest_row])], ignore_index=True)\n",
    "                else:\n",
    "                    agg_products_plot = agg_products\n",
    "\n",
    "                if not agg_products_plot.empty:\n",
    "                    create_bar_chart(\n",
    "                        df=agg_products_plot, x_col_or_index_values='document_country', y_cols=['margin'],\n",
    "                        title=f'Error Margins for Top {top_n} Document Countries (Rest Aggregated) in Optimal Simulation ({optimal_strategy_name})',\n",
    "                        y_axis_title='Error Margin', x_axis_title='Document Country',\n",
    "                        line_data={\n",
    "                            'y_value': ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "                            'name': f'Target {ABSOLUTE_ERROR_MARGIN_VALUE:.2f}',\n",
    "                            'color': \"Red\", 'width': 2, 'dash': \"dash\"\n",
    "                        },\n",
    "                        filename_suffix=\"optimal_sim_products_margin\",\n",
    "                        save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"No document country data available for plotting in optimal simulation.\")\n",
    "            else:\n",
    "                warnings.warn(\" Document Country dimension not found in optimal simulation DataFrame for plotting.\")\n",
    "\n",
    "            if 'industry' in optimal_sim_df_for_plots.columns:\n",
    "                agg_products = (\n",
    "                    optimal_sim_df_for_plots\n",
    "                    .groupby('industry', as_index=False)\n",
    "                    .agg({\n",
    "                        'simulated_num_x': 'sum',\n",
    "                        'simulated_den_n': 'sum',\n",
    "                        'simulated_total_sessions': 'sum'\n",
    "                    })\n",
    "                )\n",
    "                margins = calculate_margin_wilson(\n",
    "                    agg_products['simulated_num_x'],\n",
    "                    agg_products['simulated_den_n'],\n",
    "                    z_score\n",
    "                )\n",
    "                agg_products['margin'] = margins['margin']\n",
    "\n",
    "                if len(agg_products) > top_n:\n",
    "                    top_n_df = agg_products.sort_values('simulated_total_sessions', ascending=False).head(top_n)\n",
    "                    rest_df = agg_products.sort_values('simulated_total_sessions', ascending=False).iloc[top_n:]\n",
    "                    rest_row = {\n",
    "                        'industry': 'Rest',\n",
    "                        'simulated_num_x': rest_df['simulated_num_x'].sum(),\n",
    "                        'simulated_den_n': rest_df['simulated_den_n'].sum(),\n",
    "                        'simulated_total_sessions': rest_df['simulated_total_sessions'].sum()\n",
    "                    }\n",
    "                    rest_row['margin'] = calculate_margin_wilson(\n",
    "                        rest_row['simulated_num_x'],\n",
    "                        rest_row['simulated_den_n'],\n",
    "                        z_score\n",
    "                    )['margin']\n",
    "                    agg_products_plot = pd.concat([top_n_df, pd.DataFrame([rest_row])], ignore_index=True)\n",
    "                else:\n",
    "                    agg_products_plot = agg_products\n",
    "\n",
    "                if not agg_products_plot.empty:\n",
    "                    create_bar_chart(\n",
    "                        df=agg_products_plot, x_col_or_index_values='industry', y_cols=['margin'],\n",
    "                        title=f'Error Margins for Top {top_n} Industries (Rest Aggregated) in Optimal Simulation ({optimal_strategy_name})',\n",
    "                        y_axis_title='Error Margin', x_axis_title='Industry',\n",
    "                        line_data={\n",
    "                            'y_value': ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "                            'name': f'Target {ABSOLUTE_ERROR_MARGIN_VALUE:.2f}',\n",
    "                            'color': \"Red\", 'width': 2, 'dash': \"dash\"\n",
    "                        },\n",
    "                        filename_suffix=\"optimal_sim_products_margin\",\n",
    "                        save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"No industry data available for plotting in optimal simulation.\")\n",
    "\n",
    "            else:\n",
    "                warnings.warn(\"Industry dimension not found in optimal simulation DataFrame for plotting.\")\n",
    "\n",
    "            if 'region' in optimal_sim_df_for_plots.columns:\n",
    "                agg_products = (\n",
    "                    optimal_sim_df_for_plots\n",
    "                    .groupby('region', as_index=False)\n",
    "                    .agg({\n",
    "                        'simulated_num_x': 'sum',\n",
    "                        'simulated_den_n': 'sum',\n",
    "                        'simulated_total_sessions': 'sum'\n",
    "                    })\n",
    "                )\n",
    "                margins = calculate_margin_wilson(\n",
    "                    agg_products['simulated_num_x'],\n",
    "                    agg_products['simulated_den_n'],\n",
    "                    z_score\n",
    "                )\n",
    "                agg_products['margin'] = margins['margin']\n",
    "\n",
    "                if len(agg_products) > top_n:\n",
    "                    top_n_df = agg_products.sort_values('simulated_total_sessions', ascending=False).head(top_n)\n",
    "                    rest_df = agg_products.sort_values('simulated_total_sessions', ascending=False).iloc[top_n:]\n",
    "                    rest_row = {\n",
    "                        'region': 'Rest',\n",
    "                        'simulated_num_x': rest_df['simulated_num_x'].sum(),\n",
    "                        'simulated_den_n': rest_df['simulated_den_n'].sum(),\n",
    "                        'simulated_total_sessions': rest_df['simulated_total_sessions'].sum()\n",
    "                    }\n",
    "                    rest_row['margin'] = calculate_margin_wilson(\n",
    "                        rest_row['simulated_num_x'],\n",
    "                        rest_row['simulated_den_n'],\n",
    "                        z_score\n",
    "                    )['margin']\n",
    "                    agg_products_plot = pd.concat([top_n_df, pd.DataFrame([rest_row])], ignore_index=True)\n",
    "                else:\n",
    "                    agg_products_plot = agg_products\n",
    "\n",
    "                if not agg_products_plot.empty:\n",
    "                    create_bar_chart(\n",
    "                        df=agg_products_plot, x_col_or_index_values='region', y_cols=['margin'],\n",
    "                        title=f'Error Margins for Top {top_n} Regions (Rest Aggregated) in Optimal Simulation ({optimal_strategy_name})',\n",
    "                        y_axis_title='Error Margin', x_axis_title='Region',\n",
    "                        line_data={\n",
    "                            'y_value': ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "                            'name': f'Target {ABSOLUTE_ERROR_MARGIN_VALUE:.2f}',\n",
    "                            'color': \"Red\", 'width': 2, 'dash': \"dash\"\n",
    "                        },\n",
    "                        filename_suffix=\"optimal_sim_products_margin\",\n",
    "                        save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"No region data available for plotting in optimal simulation.\")\n",
    "\n",
    "            else:\n",
    "                warnings.warn(\"Industry dimension not found in optimal simulation DataFrame for plotting.\")\n",
    "\n",
    "    return simulation_results\n",
    "\n",
    "\n",
    "# def display_simulation_summary(simulation_results: Dict[str, Any]) -> None:\n",
    "#     \"\"\"\n",
    "#     Displays a combined summary table of all simulation results, including additional metrics.\n",
    "#     \"\"\"\n",
    "#     summary_df = pd.DataFrame.from_dict(simulation_results, orient='index')\n",
    "\n",
    "#     if summary_df.empty:\n",
    "#         print(\"\\nNo simulation results to summarize.\")\n",
    "#         return\n",
    "\n",
    "#     # Calculate improvement if possible\n",
    "#     summary_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#     if 'overall_margin' in summary_df.columns:\n",
    "#         summary_df['overall_margin_perc'] = summary_df['overall_margin'] * 100\n",
    "\n",
    "#     if 'overall_margin_perc' in summary_df.columns and not summary_df.empty:\n",
    "#         base_margin = summary_df.loc['Current Random 1.0x', 'overall_margin_perc'] if 'Current Random 1.0x' in summary_df.index else np.nan\n",
    "#         if pd.isna(base_margin) or base_margin == 0:\n",
    "#             valid_bases = summary_df['overall_margin_perc'].dropna()\n",
    "#             base_margin = valid_bases.iloc[0] if not valid_bases.empty else 0\n",
    "#         if base_margin > 0:\n",
    "#             summary_df['Improvement (%)'] = (base_margin - summary_df['overall_margin_perc']) / base_margin * 100\n",
    "#         else:\n",
    "#             summary_df['Improvement (%)'] = 0\n",
    "#     else:\n",
    "#         summary_df['Improvement (%)'] = np.nan\n",
    "\n",
    "#     # Rename additional metrics columns for clarity\n",
    "#     summary_df.rename(columns={\n",
    "#         'total_sampled': 'Total QA Sampled',\n",
    "#         'cost_total': 'Total Cost (EUR)',\n",
    "#         'overall_margin_perc': 'Overall Margin (%)',\n",
    "#         'dimension_coverage': 'Selected dimension covered below threshold',\n",
    "#         'customer_coverage': 'Customers below threshold',\n",
    "#         'product_coverage': 'Products below threshold',\n",
    "#         'Improvement (%)': 'Improvement in Error Margin (%)',\n",
    "#         'share_top_3_customers': 'Share of Top 3 Customers in Sample (%)',\n",
    "#         'share_strategic_clients_met_sla': 'Share of Platinum Clients Met SLA (%)',\n",
    "#         'cost_over_coverage': 'Cost per Actionable Unit (EUR)'\n",
    "#     }, inplace=True)\n",
    "\n",
    "#     # Select columns to display (add/remove as needed)\n",
    "#     display_cols = [\n",
    "#         'Total QA Sampled',\n",
    "#         'Total Cost (EUR)',\n",
    "#         'Overall Margin (%)',\n",
    "#         'Selected dimension covered below threshold',\n",
    "#         'Customers below threshold',\n",
    "#         'Products below threshold',\n",
    "#         'Improvement in Error Margin (%)',\n",
    "#         'Share of Top 3 Customers in Sample (%)',\n",
    "#         'Share of Platinum Clients Met SLA (%)',\n",
    "#         'Cost per Actionable Unit (EUR)'\n",
    "#     ]\n",
    "#     existing_display_cols = [col for col in display_cols if col in summary_df.columns]\n",
    "\n",
    "#     print(\"\\n## Simulation Summary Table (Including Additional Metrics)\")\n",
    "#     display(summary_df[existing_display_cols].round(2))\n",
    "\n",
    "def display_simulation_summary(simulation_results: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Displays a combined summary table of all simulation results, including additional metrics.\n",
    "    - Colors improvement in error margin green if positive, red if negative.\n",
    "    - Bolds Overall Margin (%) if below threshold.\n",
    "    - Orders by sum of dimension, customer, product below threshold (descending).\n",
    "    - Removes 'Selected dimension covered below threshold' if reporting dimension is only customer or product.\n",
    "    - Rounds all columns to 0.00.\n",
    "    \"\"\"\n",
    "    summary_df = pd.DataFrame.from_dict(simulation_results, orient='index')\n",
    "\n",
    "    if summary_df.empty:\n",
    "        print(\"\\nNo simulation results to summarize.\")\n",
    "        return\n",
    "\n",
    "    # Calculate improvement if possible\n",
    "    summary_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    if 'overall_margin' in summary_df.columns:\n",
    "        summary_df['overall_margin_perc'] = summary_df['overall_margin'] * 100\n",
    "\n",
    "    if 'overall_margin_perc' in summary_df.columns and not summary_df.empty:\n",
    "        base_margin = summary_df.loc['Current Random 1.0x', 'overall_margin_perc'] if 'Current Random 1.0x' in summary_df.index else np.nan\n",
    "        if pd.isna(base_margin) or base_margin == 0:\n",
    "            valid_bases = summary_df['overall_margin_perc'].dropna()\n",
    "            base_margin = valid_bases.iloc[0] if not valid_bases.empty else 0\n",
    "        if base_margin > 0:\n",
    "            summary_df['Improvement (%)'] = (base_margin - summary_df['overall_margin_perc']) / base_margin * 100\n",
    "        else:\n",
    "            summary_df['Improvement (%)'] = 0\n",
    "    else:\n",
    "        summary_df['Improvement (%)'] = np.nan\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    summary_df.rename(columns={\n",
    "        'total_sampled': 'Total QA Sampled',\n",
    "        'cost_total': 'Total Cost (EUR)',\n",
    "        'overall_margin_perc': 'Overall Margin (%)',\n",
    "        'dimension_coverage': 'Selected dimension covered below threshold',\n",
    "        'customer_coverage': 'Customers below threshold',\n",
    "        'product_coverage': 'Products below threshold',\n",
    "        'industry_coverage': 'Industries below threshold',\n",
    "        'document_country_coverage': 'Document Countries below threshold',\n",
    "        'region_coverage': 'Regions below threshold',\n",
    "        'Improvement (%)': 'Improvement in Error Margin (%)',\n",
    "        'share_top_3_customers': 'Share of Top 3 Customers in Sample (%)',\n",
    "        'share_strategic_clients_met_sla': 'Share of Platinum Clients Met SLA (%)',\n",
    "        'cost_over_coverage': 'Cost per Actionable Unit (EUR)',\n",
    "        'uber_main_account_simulated_sessions': 'Uber Main Account Simulated Sessions',\n",
    "        'uber_main_account_left_to_cover': 'Uber Main Account Left to Cover',\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Remove 'Selected dimension covered below threshold' if reporting dimension is only customer or product\n",
    "    reporting_dims = globals().get('REPORTING_DIMENSIONS', ['customer'])\n",
    "    if reporting_dims == ['customer'] or reporting_dims == ['product']:\n",
    "        display_cols = [\n",
    "            'Total QA Sampled',\n",
    "            'Total Cost (EUR)',\n",
    "            'Overall Margin (%)',\n",
    "            'Customers below threshold',\n",
    "            'Products below threshold',\n",
    "            'Industries below threshold',\n",
    "            'Document Countries below threshold',\n",
    "            'Regions below threshold',\n",
    "            'Improvement in Error Margin (%)',\n",
    "            'Share of Top 3 Customers in Sample (%)',\n",
    "            'Share of Platinum Clients Met SLA (%)',\n",
    "            'Cost per Actionable Unit (EUR)',\n",
    "            'Uber Main Account Simulated Sessions',\n",
    "            'Uber Main Account Left to Cover'\n",
    "        ]\n",
    "    else:\n",
    "        display_cols = [\n",
    "            'Total QA Sampled',\n",
    "            'Total Cost (EUR)',\n",
    "            'Overall Margin (%)',\n",
    "            'Selected dimension covered below threshold',\n",
    "            'Customers below threshold',\n",
    "            'Products below threshold',\n",
    "            'Industries below threshold',\n",
    "            'Document Countries below threshold',\n",
    "            'Regions below threshold',\n",
    "            'Improvement in Error Margin (%)',\n",
    "            'Share of Top 3 Customers in Sample (%)',\n",
    "            'Share of Platinum Clients Met SLA (%)',\n",
    "            'Cost per Actionable Unit (EUR)',\n",
    "            'Uber Main Account Simulated Sessions',\n",
    "            'Uber Main Account Left to Cover'\n",
    "        ]\n",
    "\n",
    "    coverage_cols = []\n",
    "    if reporting_dims == ['customer']:\n",
    "        coverage_cols = ['Customers below threshold', 'Products below threshold', 'Industries below threshold', 'Document Countries below threshold', 'Regions below threshold']\n",
    "    elif reporting_dims == ['product']:\n",
    "        coverage_cols = ['Products below threshold', 'Customers below threshold', 'Industries below threshold', 'Document Countries below threshold', 'Regions below threshold']\n",
    "    else:\n",
    "        coverage_cols = [\n",
    "            'Selected dimension covered below threshold',\n",
    "            'Customers below threshold',\n",
    "            'Products below threshold'\n",
    "            'Industries below threshold',\n",
    "            'Document Countries below threshold',\n",
    "            'Regions below threshold'\n",
    "        ]\n",
    "    for col in coverage_cols:\n",
    "        if col not in summary_df.columns:\n",
    "            summary_df[col] = 0\n",
    "    # Only sum each group once\n",
    "    summary_df['total_coverage'] = summary_df[coverage_cols].sum(axis=1)\n",
    "\n",
    "    # Sort by total_coverage descending, then by Improvement in Error Margin ascending\n",
    "    summary_df = summary_df.sort_values(['total_coverage', 'Improvement in Error Margin (%)'], ascending=[False, False])\n",
    "\n",
    "    existing_display_cols = [col for col in display_cols if col in summary_df.columns]\n",
    "\n",
    "    # Threshold for bolding overall margin\n",
    "    threshold = globals().get('TARGET_OVERALL_ERROR_PERCENT', 2.0)\n",
    "\n",
    "    def highlight_improvement(val):\n",
    "        if pd.isna(val):\n",
    "            return ''\n",
    "        color = 'green' if val > 0 else 'red' if val < 0 else ''\n",
    "        return f'color: {color}'\n",
    "\n",
    "    def bold_margin(val):\n",
    "        try:\n",
    "            if float(val) < threshold:\n",
    "                return 'font-weight: bold'\n",
    "        except Exception:\n",
    "            pass\n",
    "        return ''\n",
    "\n",
    "    # Round all columns to 2 decimals\n",
    "    rounded_df = summary_df[existing_display_cols].copy().applymap(lambda x: round(x, 2) if pd.notnull(x) else x)\n",
    "\n",
    "    print(\"\\n## Simulation Summary Table (Including Additional Metrics)\")\n",
    "    styled = (\n",
    "        rounded_df\n",
    "        .style\n",
    "        .format(\"{:.2f}\")\n",
    "        .applymap(highlight_improvement, subset=['Improvement in Error Margin (%)'] if 'Improvement in Error Margin (%)' in rounded_df.columns else [])\n",
    "        .applymap(bold_margin, subset=['Overall Margin (%)'] if 'Overall Margin (%)' in rounded_df.columns else [])\n",
    "    )\n",
    "    display(styled)\n",
    "\n",
    "def plot_customer_coverage_vs_exponent(\n",
    "    population_df: pd.DataFrame,\n",
    "    target_sessions: int,\n",
    "    selected_metric: str,\n",
    "    global_avg_metrics: dict,\n",
    "    overall_avg_frequencies_in_qa: dict,\n",
    "    reporting_dimensions: list,\n",
    "    error_margin_logic: str,\n",
    "    absolute_error_margin_value: float,\n",
    "    relative_error_margin_value: float,\n",
    "    z_score: float,\n",
    "    population_df_original_for_metrics: pd.DataFrame,\n",
    "    save_plots_to_html: bool = False,\n",
    "    scales: list = None,\n",
    "    min_absolute_error_margin: float = 0.02\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs biased simulations for exponents from 0.1 to 1.5 (step 0.1) and multiple sample size scales,\n",
    "    collects number of customers under threshold, and plots the result with max points annotated.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    if scales is None:\n",
    "        scales = [0.5, 1, 2, 3]  # Default scales\n",
    "\n",
    "    exponents = np.arange(0.1, 1.51, 0.1)\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for scale in scales:\n",
    "        scaled_sessions = int(target_sessions * scale)\n",
    "        customer_coverage = []\n",
    "        for exp in exponents:\n",
    "            scenario_results = _run_single_simulation_scenario(\n",
    "                population_df=population_df,\n",
    "                target_sessions=scaled_sessions,\n",
    "                sampling_type='biased',\n",
    "                exponent=exp,\n",
    "                volume_col_for_sampling='volume',\n",
    "                selected_metric=selected_metric,\n",
    "                global_avg_metrics=global_avg_metrics,\n",
    "                overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa,\n",
    "                reporting_dimensions=['customer'],\n",
    "                error_margin_logic=error_margin_logic,\n",
    "                absolute_error_margin_value=absolute_error_margin_value,\n",
    "                relative_error_margin_value=relative_error_margin_value,\n",
    "                min_absolute_error_margin=min_absolute_error_margin,  # No minimum margin for this plot\n",
    "                z_score=z_score,\n",
    "                population_df_original_for_metrics=population_df_original_for_metrics\n",
    "            )\n",
    "            customer_coverage.append(scenario_results.get('customer_coverage', 0))\n",
    "\n",
    "        # Plot the curve for this scale\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=exponents, y=customer_coverage, mode='lines+markers',\n",
    "            name=f'Scale {scale}x ({scaled_sessions} sessions)'\n",
    "        ))\n",
    "\n",
    "        # Annotate the maximum point with a star and text label\n",
    "        max_idx = int(np.argmax(customer_coverage))\n",
    "        max_exp = exponents[max_idx]\n",
    "        max_cov = customer_coverage[max_idx]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[max_exp], y=[max_cov],\n",
    "            mode='markers',\n",
    "            marker=dict(color='red', size=14, symbol='star'),\n",
    "            showlegend=False\n",
    "        ))\n",
    "        fig.add_annotation(\n",
    "            x=max_exp, y=max_cov,\n",
    "            text=f\"★ Bias_exponent: {max_exp:.2f}<br>Max: {max_cov}<br>Scale: {scale}x\",\n",
    "            showarrow=False,\n",
    "            font=dict(color='red', size=12),\n",
    "            xanchor='left',\n",
    "            yanchor='bottom',\n",
    "            xshift=10,\n",
    "            yshift=10\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Number of Customers Under Target Threshold vs Bias Exponent (Multiple Sample Sizes)',\n",
    "        xaxis_title='Bias Exponent',\n",
    "        yaxis_title='Number of Customers Under Threshold',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    if save_plots_to_html:\n",
    "        fig.write_html(\"customer_coverage_vs_exponent.html\")\n",
    "        print(\"Plot saved to customer_coverage_vs_exponent.html\")\n",
    "    fig.show()\n",
    "\n",
    "def plot_customer_volume_distribution_binned(\n",
    "    df: pd.DataFrame,\n",
    "    quantile_threshold: float = 0.2,\n",
    "    bins: int = 20,\n",
    "    save_plots_to_html: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the distribution of customers by total volume, binned into buckets, and highlights quantile thresholds.\n",
    "    Shows selected quantile, 15%, 50%, 75%, and 90%.\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    # Aggregate total volume per customer\n",
    "    customer_volumes = df.groupby('customer')['volume'].sum()\n",
    "\n",
    "    # Ensure bins start at zero (or min value)\n",
    "    min_vol = customer_volumes.min()\n",
    "    max_vol = customer_volumes.max()\n",
    "    bin_start = 0 if min_vol >= 0 else min_vol\n",
    "    bins_arr = np.linspace(bin_start, max_vol, bins + 1)\n",
    "\n",
    "    # Bin the customer volumes\n",
    "    volume_bins = pd.cut(customer_volumes, bins=bins_arr, include_lowest=True)\n",
    "    binned_counts = volume_bins.value_counts().sort_index()\n",
    "    binned_counts = binned_counts[binned_counts > 0]\n",
    "\n",
    "    bin_labels = [str(interval) for interval in binned_counts.index]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=bin_labels,\n",
    "        y=binned_counts.values,\n",
    "        name='Customer Count per Volume Bin',\n",
    "        marker_color='blue',\n",
    "        opacity=0.7\n",
    "    ))\n",
    "\n",
    "    # Quantiles to plot\n",
    "    quantiles = [\n",
    "        quantile_threshold,\n",
    "        0.15,\n",
    "        0.5,\n",
    "        0.75,\n",
    "        0.9\n",
    "    ]\n",
    "    quantile_colors = ['red', 'orange', 'green', 'purple', 'black']\n",
    "    quantile_names = [\n",
    "        f\"Selected ({int(quantile_threshold*100)}%)\",\n",
    "        \"15%\",\n",
    "        \"50%\",\n",
    "        \"75%\",\n",
    "        \"90%\"\n",
    "    ]\n",
    "\n",
    "    for q, color, name in zip(quantiles, quantile_colors, quantile_names):\n",
    "        q_value = customer_volumes.quantile(q)\n",
    "        q_bin = None\n",
    "        for interval in binned_counts.index:\n",
    "            if interval.left <= q_value <= interval.right:\n",
    "                q_bin = interval\n",
    "                break\n",
    "        if q_bin is not None:\n",
    "            bin_index = bin_labels.index(str(q_bin))\n",
    "            fig.add_vline(\n",
    "                x=bin_index,\n",
    "                line=dict(color=color, width=2, dash='dash'),\n",
    "                annotation_text=f\"{name}: {q_value:.0f}\",\n",
    "                annotation_position=\"top right\"\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Distribution of Customers by Total Volume (Binned)',\n",
    "        xaxis_title='Volume Bin',\n",
    "        yaxis_title='Number of Customers',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    if save_plots_to_html:\n",
    "        fig.write_html(\"customer_volume_distribution_binned.html\")\n",
    "        print(\"Plot saved to customer_volume_distribution_binned.html\")\n",
    "    fig.show()\n",
    "\n",
    "def plot_customer_volume_boxplot(\n",
    "    df: pd.DataFrame,\n",
    "    quantile_threshold: float = 0.2,\n",
    "    save_plots_to_html: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a boxplot of customer volumes, overlays customer points, and marks selected and key quantiles.\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    # Aggregate total volume per customer\n",
    "    customer_volumes = df.groupby('customer')['volume'].sum()\n",
    "    customers = customer_volumes.index.tolist()\n",
    "    volumes = customer_volumes.values\n",
    "\n",
    "    # Quantiles to plot\n",
    "    quantiles = [\n",
    "        quantile_threshold,\n",
    "        0.15,\n",
    "        0.5,\n",
    "        0.75,\n",
    "        0.9\n",
    "    ]\n",
    "    quantile_colors = ['red', 'orange', 'green', 'purple', 'black']\n",
    "    quantile_names = [\n",
    "        f\"Selected ({int(quantile_threshold*100)}%)\",\n",
    "        \"15%\",\n",
    "        \"50%\",\n",
    "        \"75%\",\n",
    "        \"90%\"\n",
    "    ]\n",
    "    quantile_values = [customer_volumes.quantile(q) for q in quantiles]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Boxplot\n",
    "    fig.add_trace(go.Box(\n",
    "        y=volumes,\n",
    "        boxpoints='outliers',\n",
    "        name='Customer Volume',\n",
    "        marker_color='lightblue',\n",
    "        boxmean=True,\n",
    "        orientation='v'\n",
    "    ))\n",
    "\n",
    "    # Overlay customer points\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=volumes,\n",
    "        x=['']*len(volumes),  # All points on same x for vertical boxplot\n",
    "        mode='markers',\n",
    "        marker=dict(color='blue', size=6, opacity=0.6),\n",
    "        text=customers,\n",
    "        name='Customers'\n",
    "    ))\n",
    "\n",
    "    # Add quantile lines\n",
    "    for q_val, color, name in zip(quantile_values, quantile_colors, quantile_names):\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=-0.4, x1=0.4,\n",
    "            y0=q_val, y1=q_val,\n",
    "            line=dict(color=color, width=2, dash='dash'),\n",
    "        )\n",
    "        fig.add_annotation(\n",
    "            x=0.45, y=q_val,\n",
    "            text=f\"{name}: {q_val:.0f}\",\n",
    "            showarrow=False,\n",
    "            font=dict(color=color, size=10),\n",
    "            yshift=0\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Customer Volume Distribution (Boxplot with Quantiles)',\n",
    "        yaxis_title='Total Volume per Customer',\n",
    "        xaxis=dict(showticklabels=False),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "\n",
    "    if save_plots_to_html:\n",
    "        fig.write_html(\"customer_volume_boxplot.html\")\n",
    "        print(\"Plot saved to customer_volume_boxplot.html\")\n",
    "    fig.show()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_proportion_volume_vs_qa(population_df: pd.DataFrame, \n",
    "                                reporting_dimensions: list, \n",
    "                                top_n: int = 10, \n",
    "                                save_plots_to_html: bool = False):\n",
    "    \"\"\"\n",
    "    Plots the proportion of sessions in volume and in qa_reviewed per dimension.\n",
    "    \"\"\"\n",
    "    analysis_dimensions = list(set(reporting_dimensions + ['primary_product', 'customer']))\n",
    "\n",
    "    for dim in analysis_dimensions:\n",
    "        if dim not in population_df.columns:\n",
    "            print(f\"Dimension '{dim}' not found in data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Aggregate sums\n",
    "        agg = population_df.groupby(dim).agg(\n",
    "            volume_sum=('volume', 'sum'),\n",
    "            qa_reviewed_sum=('qa_reviewed', 'sum')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Calculate proportions\n",
    "        total_volume = agg['volume_sum'].sum()\n",
    "        total_qa = agg['qa_reviewed_sum'].sum()\n",
    "        agg['proportion_of_total_volume'] = agg['volume_sum'] / total_volume\n",
    "        agg['proportion_of_qa_sample'] = agg['qa_reviewed_sum'] / total_qa\n",
    "\n",
    "        # Top N + Rest\n",
    "        if len(agg) > top_n:\n",
    "            top_n_df = agg.sort_values('volume_sum', ascending=False).head(top_n)\n",
    "            rest_df = agg.sort_values('volume_sum', ascending=False).iloc[top_n:]\n",
    "            rest_row = {\n",
    "                dim: 'Rest',\n",
    "                'volume_sum': rest_df['volume_sum'].sum(),\n",
    "                'qa_reviewed_sum': rest_df['qa_reviewed_sum'].sum(),\n",
    "            }\n",
    "            rest_row['proportion_of_total_volume'] = rest_row['volume_sum'] / total_volume\n",
    "            rest_row['proportion_of_qa_sample'] = rest_row['qa_reviewed_sum'] / total_qa\n",
    "            plot_df = pd.concat([top_n_df, pd.DataFrame([rest_row])], ignore_index=True)\n",
    "        else:\n",
    "            plot_df = agg.copy()\n",
    "\n",
    "        # Prepare text labels rounded to 0.00\n",
    "        text_labels = {\n",
    "            'proportion_of_total_volume': plot_df['proportion_of_total_volume'].apply(lambda x: f\"{x:.2f}\"),\n",
    "            'proportion_of_qa_sample': plot_df['proportion_of_qa_sample'].apply(lambda x: f\"{x:.2f}\")\n",
    "        }\n",
    "\n",
    "        create_bar_chart(\n",
    "            df=plot_df,\n",
    "            x_col_or_index_values=dim,\n",
    "            y_cols=['proportion_of_total_volume', 'proportion_of_qa_sample'],\n",
    "            title=f'Proportion of Volume vs QA Reviewed by {dim}',\n",
    "            y_axis_title='Proportion',\n",
    "            x_axis_title=dim,\n",
    "            barmode='group',\n",
    "            filename_suffix=f'proportion_volume_vs_qa_{dim}',\n",
    "            save_plots_to_html=save_plots_to_html,\n",
    "            text_labels=text_labels\n",
    "        )\n",
    "\n",
    "def generate_report(df_data: pd.DataFrame, params: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Main function to generate the comprehensive simulation report.\n",
    "    Accepts parameters as a dictionary from the UI.\n",
    "    \"\"\"\n",
    "    # Update global parameters based on UI input\n",
    "    global SELECTED_METRIC, TARGET_OVERALL_ERROR_PERCENT, TARGET_DIMENSION_ERROR_PERCENT, \\\n",
    "           CONFIDENCE_LEVEL, REVIEW_COST_PER_SESSION, PREDEFINED_TOTAL_COST_BUDGET, \\\n",
    "           WEIGHT_FACTOR_SMALL_CUSTOMERS, SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD, \\\n",
    "           ERROR_MARGIN_LOGIC, ABSOLUTE_ERROR_MARGIN_VALUE, RELATIVE_ERROR_MARGIN_VALUE, \\\n",
    "           MIN_ABSOLUTE_ERROR_MARGIN, REPORTING_DIMENSIONS, STRATIFY_BY_DIMENSION, TOP_N, Z_SCORE, \\\n",
    "           SAVE_PLOTS_TO_HTML, MIN_PER_STRATUM, TARGET_QA_PROPORTIONS\n",
    "    \n",
    "    if 'qa_proportions' in params:\n",
    "        TARGET_QA_PROPORTIONS = params['qa_proportions']\n",
    "\n",
    "    MIN_PER_STRATUM = params['min_per_stratum']\n",
    "    SELECTED_METRIC = params['selected_metric']\n",
    "    TARGET_OVERALL_ERROR_PERCENT = params['target_overall_error_percent']\n",
    "    TARGET_DIMENSION_ERROR_PERCENT = params['target_dimension_error_percent']\n",
    "    CONFIDENCE_LEVEL = params['confidence_level']\n",
    "    REVIEW_COST_PER_SESSION = params['review_cost_per_session']\n",
    "    PREDEFINED_TOTAL_COST_BUDGET = params['predefined_total_cost_budget']\n",
    "    WEIGHT_FACTOR_SMALL_CUSTOMERS = params['weight_factor_small_customers']\n",
    "    SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD = params['small_customer_volume_quantile_threshold']\n",
    "    ERROR_MARGIN_LOGIC = params['error_margin_logic']\n",
    "    ABSOLUTE_ERROR_MARGIN_VALUE = params['absolute_error_margin_value']\n",
    "    RELATIVE_ERROR_MARGIN_VALUE = params['relative_error_margin_value']\n",
    "    MIN_ABSOLUTE_ERROR_MARGIN = params['min_absolute_error_margin']\n",
    "    REPORTING_DIMENSIONS = params['reporting_dimensions']\n",
    "    STRATIFY_BY_DIMENSION = params['stratify_by_dimension']\n",
    "    TOP_N = params['top_n']\n",
    "    SAVE_PLOTS_TO_HTML = params['save_plots_to_html']\n",
    "    \n",
    "    # Recalculate Z_SCORE based on potentially new CONFIDENCE_LEVEL\n",
    "    Z_SCORE = stats.norm.ppf(1 - (1 - CONFIDENCE_LEVEL) / 2)\n",
    "\n",
    "    # --- Print Selected Parameters Table ---\n",
    "    param_table = pd.DataFrame([params])\n",
    "    print(\"## Selected Parameters for Report\")\n",
    "    display(param_table.T)\n",
    "\n",
    "    print(\"Starting QA Sampling Strategy Report Generation...\")\n",
    "\n",
    "    print(\"NOTE: Simulation scenarios are for strategy comparison only and do not produce unbiased population estimates unless weights are applied.\")\n",
    "    \n",
    "    # Calculate total QA reviewed sessions\n",
    "    # If daily_reviews and days are provided, calculate total_qa_reviewed\n",
    "    # Otherwise, fallback to sum of 'qa_reviewed' column in df_data\n",
    "\n",
    "    daily_reviews = params.get('daily_reviews', None)\n",
    "    days = params.get('days', None)\n",
    "\n",
    "    qa_total_source = params.get('qa_total_source', 'Use Current QA Reviewed')\n",
    "\n",
    "    # --- SCALE DATA IF USER SELECTS FEWER DAYS THAN DATA COVERS ---\n",
    "    # Assume your data covers 30 days by default (adjust if needed)\n",
    "\n",
    "    # Get unique days_count from the DataFrame\n",
    "    if 'days_count' in df_data.columns:\n",
    "        default_days = int(df_data['days_count'].iloc[0])\n",
    "    else:\n",
    "        default_days = 30  # fallback if not present\n",
    "\n",
    "    data_days = default_days\n",
    "\n",
    "    if days is not None and days < data_days:\n",
    "        scale_factor = days / data_days\n",
    "        cols_to_scale = [\n",
    "            'volume', 'qa_reviewed', 'qa_reviewed_for_reporting',\n",
    "            'decision_error_sessions', 'extraction_error_sessions',\n",
    "            'false_approves', 'false_declines', 'missed_fraud_sessions',\n",
    "            'declinable_sessions', 'approvable_sessions', 'fraud_sessions',\n",
    "            'full_auto_sessions', 'declined_sessions', 'approved_sessions',\n",
    "            'declined_due_to_fraud_sessions', 'approved_due_to_fraud_sessions'\n",
    "        ]\n",
    "        for col in cols_to_scale:\n",
    "            if col in df_data.columns:\n",
    "               df_data[col] = (df_data[col] * scale_factor).round().fillna(0).astype(int)\n",
    "        print(f\"[INFO] Scaled all relevant columns by {scale_factor:.2f} to match selected {days} days.\")\n",
    "\n",
    "    if qa_total_source == 'Input Daily Reviews * Days' and daily_reviews is not None and days is not None:\n",
    "        total_qa_reviewed = daily_reviews * days\n",
    "        print(f\"\\n[INFO] Using QA total from widget input: daily_reviews ({daily_reviews}) * days ({days}) = {total_qa_reviewed}\")\n",
    "    else:\n",
    "        total_qa_reviewed = df_data['qa_reviewed'].sum()\n",
    "        print(f\"\\n[INFO] Using QA total from current data: sum(df_data['qa_reviewed']) = {total_qa_reviewed}\")\n",
    "\n",
    "\n",
    "    # Data Preparation\n",
    "    population_df, global_rates, global_freq = prepare_population_data(df_data, SELECTED_METRIC, METRIC_PROPERTIES, Z_SCORE)\n",
    "    \n",
    "    total_qa = total_qa_reviewed #population_df['qa_reviewed'].sum()\n",
    "    population_df = assign_true_random_sample(population_df, int(total_qa))\n",
    "\n",
    "    # If widget is set to use Input Daily Reviews * Days, use true_random_qa_reviewed for reporting\n",
    "    if qa_total_source == 'Input Daily Reviews * Days' and 'true_random_qa_reviewed' in population_df.columns:\n",
    "        # For reporting, overwrite 'qa_reviewed' with 'true_random_qa_reviewed'\n",
    "        population_df['qa_reviewed_for_reporting'] = population_df['true_random_qa_reviewed']\n",
    "    else:\n",
    "        population_df['qa_reviewed_for_reporting'] = population_df['qa_reviewed']\n",
    "    \n",
    "    # Display Current State Analysis\n",
    "    display_current_state_analysis(population_df, SELECTED_METRIC, METRIC_PROPERTIES,\n",
    "                                   REVIEW_COST_PER_SESSION, TARGET_OVERALL_ERROR_PERCENT, Z_SCORE,\n",
    "                                   SAVE_PLOTS_TO_HTML)\n",
    "    \n",
    "    display_metric_with_error_bars(\n",
    "        population_df, SELECTED_METRIC, METRIC_PROPERTIES,\n",
    "        REPORTING_DIMENSIONS, TOP_N, global_rates, Z_SCORE, SAVE_PLOTS_TO_HTML\n",
    "    )\n",
    "\n",
    "    plot_proportion_volume_vs_qa(\n",
    "        population_df,\n",
    "        REPORTING_DIMENSIONS,\n",
    "        top_n=10,\n",
    "        save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "    )\n",
    "    \n",
    "    # # Display Current Dimension Metrics and Margins\n",
    "    # display_current_dimension_metrics_and_margins(population_df, SELECTED_METRIC, METRIC_PROPERTIES,\n",
    "    #                                               REPORTING_DIMENSIONS, TOP_N, TARGET_OVERALL_ERROR_PERCENT,\n",
    "    #                                               global_rates, Z_SCORE, SAVE_PLOTS_TO_HTML)\n",
    "\n",
    "    # Display Sample Size Calculations\n",
    "    display_sample_size_calculations(population_df, SELECTED_METRIC, METRIC_PROPERTIES,\n",
    "                                     REPORTING_DIMENSIONS, ERROR_MARGIN_LOGIC, ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "                                     RELATIVE_ERROR_MARGIN_VALUE, MIN_ABSOLUTE_ERROR_MARGIN,\n",
    "                                     TOP_N,\n",
    "                                     global_rates, global_freq,\n",
    "                                     CONFIDENCE_LEVEL, Z_SCORE, SAVE_PLOTS_TO_HTML)\n",
    "\n",
    "    # Run Simulations\n",
    "    simulation_results = run_simulations(\n",
    "        population_df = population_df, \n",
    "        global_avg_metrics = global_rates, \n",
    "        overall_avg_frequencies_in_qa = global_freq, \n",
    "        selected_metric = SELECTED_METRIC, \n",
    "        reporting_dimensions = REPORTING_DIMENSIONS,\n",
    "        error_margin_logic = ERROR_MARGIN_LOGIC, \n",
    "        absolute_error_margin_value = ABSOLUTE_ERROR_MARGIN_VALUE, \n",
    "        relative_error_margin_value = RELATIVE_ERROR_MARGIN_VALUE, \n",
    "        min_absolute_error_margin = MIN_ABSOLUTE_ERROR_MARGIN,\n",
    "        target_overall_error_percent = TARGET_OVERALL_ERROR_PERCENT, \n",
    "        target_dimension_error_percent = TARGET_DIMENSION_ERROR_PERCENT,\n",
    "        weight_factor_small_customers = WEIGHT_FACTOR_SMALL_CUSTOMERS,\n",
    "        small_customer_volume_quantile_threshold = SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD, \n",
    "        stratify_by_dimension = STRATIFY_BY_DIMENSION,\n",
    "        sampling_types_to_run = params['sampling_types_to_run'], \n",
    "        top_n = TOP_N, \n",
    "        z_score = Z_SCORE,\n",
    "        min_per_stratum=MIN_PER_STRATUM\n",
    "    )\n",
    "    \n",
    "    # Display Simulation Summary\n",
    "    display_simulation_summary(simulation_results)\n",
    "\n",
    "    # Display Additional Metrics Summary\n",
    "    #display_additional_metrics_summary(simulation_results)\n",
    "\n",
    "    # Example usage (add to your report function where appropriate):\n",
    "    plot_customer_coverage_vs_exponent(\n",
    "        population_df=population_df,\n",
    "        target_sessions=int(population_df['qa_reviewed'].sum()),\n",
    "        selected_metric=SELECTED_METRIC,\n",
    "        global_avg_metrics=global_rates,\n",
    "        overall_avg_frequencies_in_qa=global_freq,\n",
    "        reporting_dimensions=REPORTING_DIMENSIONS,\n",
    "        error_margin_logic=ERROR_MARGIN_LOGIC,\n",
    "        absolute_error_margin_value=ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "        relative_error_margin_value=RELATIVE_ERROR_MARGIN_VALUE,\n",
    "        z_score=Z_SCORE,\n",
    "        population_df_original_for_metrics=population_df,\n",
    "        save_plots_to_html=SAVE_PLOTS_TO_HTML,\n",
    "        min_absolute_error_margin=MIN_ABSOLUTE_ERROR_MARGIN\n",
    "    )\n",
    "\n",
    "    # --- Save simulation summary to CSV ---\n",
    "    summary_df = pd.DataFrame.from_dict(simulation_results, orient='index')\n",
    "    summary_df.to_csv('simulation_summary.csv')\n",
    "    print(\"Simulation summary saved to simulation_summary.csv\")\n",
    "    # # Example usage (add to your report function where appropriate):\n",
    "    # plot_customer_volume_distribution_binned(df, quantile_threshold=SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD, bins=10, save_plots_to_html=SAVE_PLOTS_TO_HTML)\n",
    "\n",
    "    # # Example usage (add to your report function where appropriate):\n",
    "    # plot_customer_volume_boxplot(df, quantile_threshold=SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD, save_plots_to_html=SAVE_PLOTS_TO_HTML)\n",
    "\n",
    "    # print_simulation_summaries(\n",
    "    # simulation_results,\n",
    "    # ERROR_MARGIN_LOGIC,\n",
    "    # ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "    # RELATIVE_ERROR_MARGIN_VALUE\n",
    "    # )\n",
    "    # print(\"\\nQA Sampling Strategy Report Generation Completed.\")\n",
    "\n",
    "\n",
    "# def find_optimal_relative_error_margin(\n",
    "#     df_data,\n",
    "#     params,\n",
    "#     margin_range=(0.2, 0.5),\n",
    "#     step=0.1,\n",
    "#     verbose=True\n",
    "# ):\n",
    "#     import pandas as pd\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import sys\n",
    "#     import contextlib\n",
    "#     import io\n",
    "\n",
    "#     results = []\n",
    "#     for rel_margin in np.arange(margin_range[0], margin_range[1] + step, step):\n",
    "#         test_params = params.copy()\n",
    "#         test_params['error_margin_logic'] = 'Relative Error Margin'\n",
    "#         test_params['relative_error_margin_value'] = rel_margin\n",
    "#         test_params['absolute_error_margin_value'] = 0.001\n",
    "\n",
    "#         filtered_df = df_data\n",
    "#         if 'primary_product' in test_params:\n",
    "#             filtered_df = filtered_df[filtered_df['primary_product'].isin(test_params['primary_product'])]\n",
    "\n",
    "#         population_df, global_rates, global_freq = prepare_population_data(\n",
    "#             filtered_df, test_params['selected_metric'], METRIC_PROPERTIES, Z_SCORE\n",
    "#         )\n",
    "#         total_qa = population_df['qa_reviewed'].sum()\n",
    "#         population_df = assign_true_random_sample(population_df, int(total_qa))\n",
    "\n",
    "#         if test_params.get('qa_total_source', 'Use Current QA Reviewed') == 'Input Daily Reviews * Days' and 'true_random_qa_reviewed' in population_df.columns:\n",
    "#             population_df['qa_reviewed_for_reporting'] = population_df['true_random_qa_reviewed']\n",
    "#         else:\n",
    "#             population_df['qa_reviewed_for_reporting'] = population_df['qa_reviewed']\n",
    "\n",
    "#         with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
    "#             simulation_results = run_simulations(\n",
    "#                 population_df=population_df,\n",
    "#                 global_avg_metrics=global_rates,\n",
    "#                 overall_avg_frequencies_in_qa=global_freq,\n",
    "#                 selected_metric=test_params['selected_metric'],\n",
    "#                 reporting_dimensions=test_params['reporting_dimensions'],\n",
    "#                 error_margin_logic=test_params['error_margin_logic'],\n",
    "#                 absolute_error_margin_value=test_params['absolute_error_margin_value'],\n",
    "#                 relative_error_margin_value=test_params['relative_error_margin_value'],\n",
    "#                 min_absolute_error_margin=test_params['min_absolute_error_margin'],\n",
    "#                 target_overall_error_percent=test_params['target_overall_error_percent'],\n",
    "#                 target_dimension_error_percent=test_params['target_dimension_error_percent'],\n",
    "#                 weight_factor_small_customers=test_params['weight_factor_small_customers'],\n",
    "#                 small_customer_volume_quantile_threshold=test_params['small_customer_volume_quantile_threshold'],\n",
    "#                 stratify_by_dimension=test_params['stratify_by_dimension'],\n",
    "#                 sampling_types_to_run=test_params['sampling_types_to_run'],\n",
    "#                 top_n=test_params['top_n'],\n",
    "#                 z_score=Z_SCORE\n",
    "#             )\n",
    "#         summary_df = pd.DataFrame.from_dict(simulation_results, orient='index')\n",
    "\n",
    "#         # --- Patch: Avoid NaN for min cost per unit ---\n",
    "#         if 'Cost per Actionable Unit (EUR)' in summary_df.columns:\n",
    "#             min_cost = summary_df['Cost per Actionable Unit (EUR)'].min()\n",
    "#             if pd.isna(min_cost) or min_cost == 0:\n",
    "#                 min_cost = summary_df['Total Cost (EUR)'].iloc[0] if 'Total Cost (EUR)' in summary_df.columns else np.nan\n",
    "#         else:\n",
    "#             min_cost = summary_df['Total Cost (EUR)'].iloc[0] if 'Total Cost (EUR)' in summary_df.columns else np.nan\n",
    "#         # ------------------------------------------------\n",
    "\n",
    "#         main_row = summary_df.iloc[0] if not summary_df.empty else pd.Series()\n",
    "#         # Try to find a scenario with coverage > 0\n",
    "#         best_row = None\n",
    "#         for idx, row in summary_df.iterrows():\n",
    "#             cust_cov = row.get('customer_coverage', 0)\n",
    "#             prod_cov = row.get('product_coverage', 0)\n",
    "#             if (pd.notnull(cust_cov) and cust_cov > 0) or (pd.notnull(prod_cov) and prod_cov > 0):\n",
    "#                 best_row = row\n",
    "#                 break\n",
    "#         if best_row is not None:\n",
    "#             main_row = best_row\n",
    "\n",
    "#         results.append({\n",
    "#             'Scenario': main_row.name if not summary_df.empty else '',\n",
    "#             'Relative Error Margin': rel_margin,\n",
    "#             'Min Cost per Unit': min_cost,\n",
    "#             'Total QA Sampled': main_row.get('total_sampled', np.nan),\n",
    "#             'Overall Margin (%)': main_row.get('overall_margin', np.nan) * 100 if not pd.isna(main_row.get('overall_margin', np.nan)) else np.nan,\n",
    "#             'Customers below threshold': main_row.get('customer_coverage', np.nan),\n",
    "#             'Products below threshold': main_row.get('product_coverage', np.nan),\n",
    "#         })\n",
    "#         if verbose:\n",
    "#             print(f\"Tested margin {rel_margin:.3f}: min cost per unit = {min_cost:.2f}\")\n",
    "\n",
    "#     # Prefer results with coverage > 0\n",
    "#     results_with_coverage = [r for r in results if (pd.notnull(r['Customers below threshold']) and r['Customers below threshold'] > 0) or (pd.notnull(r['Products below threshold']) and r['Products below threshold'] > 0)]\n",
    "#     if results_with_coverage:\n",
    "#         best = min(results_with_coverage, key=lambda x: x['Min Cost per Unit'] if pd.notnull(x['Min Cost per Unit']) else np.inf)\n",
    "#     else:\n",
    "#         best = min(results, key=lambda x: x['Min Cost per Unit'] if pd.notnull(x['Min Cost per Unit']) else np.inf)\n",
    "\n",
    "#     print(f\"\\nOptimal relative error margin: {best['Relative Error Margin']:.3f} (min cost per unit: {best['Min Cost per Unit']:.2f})\")\n",
    "\n",
    "#     summary_table = pd.DataFrame(results)\n",
    "#     def highlight_optimal(row):\n",
    "#         return ['background-color: lightgreen' if row['Relative Error Margin'] == best['Relative Error Margin'] else '' for _ in row]\n",
    "\n",
    "#     display(summary_table.style.apply(highlight_optimal, axis=1).format(precision=2))\n",
    "\n",
    "#     plt.plot(summary_table['Relative Error Margin'], summary_table['Min Cost per Unit'], marker='o')\n",
    "#     plt.xlabel('Relative Error Margin')\n",
    "#     plt.ylabel('Min Cost per Actionable Unit (EUR)')\n",
    "#     plt.title('Cost per Unit vs. Relative Error Margin')\n",
    "#     plt.show()\n",
    "\n",
    "#     return best\n",
    "\n",
    "# def print_simulation_summaries(simulation_results, error_margin_logic, abs_margin_value, rel_margin_value):\n",
    "#     for sim_name, res in simulation_results.items():\n",
    "#         print(f\"\\n## Simulation Summary: {sim_name}\")\n",
    "#         # Determine threshold logic and value\n",
    "#         if error_margin_logic == 'Absolute Error Margin':\n",
    "#             threshold_type = \"Absolute Error Margin\"\n",
    "#             threshold_value = abs_margin_value * 100\n",
    "#             threshold_unit = \"%\"\n",
    "#         else:\n",
    "#             threshold_type = \"Relative Error Margin\"\n",
    "#             threshold_value = rel_margin_value * 100\n",
    "#             threshold_unit = \"% of group rate\"\n",
    "#         print(f\"Threshold logic used: {threshold_type} (Threshold: {threshold_value:.2f}{threshold_unit})\")\n",
    "#         print(f\"Sample size: {res.get('total_sampled', 'N/A')} sessions (Strategy: {sim_name})\")\n",
    "#         print(f\"  - Overall error margin for this strategy: {res.get('overall_margin', float('nan'))*100:.2f}%\")\n",
    "#         print(f\"  - Covers {res.get('customer_coverage', 0)} customers below threshold.\")\n",
    "#         print(f\"  - Customers meeting target: {res.get('customer_list', '')}\")\n",
    "#         print(f\"  - Covers {res.get('product_coverage', 0)} products below threshold.\")\n",
    "#         print(f\"  - Products meeting target: {res.get('product_list', '')}\")\n",
    "#         print(f\"  - Covers {res.get('industry_coverage', 0)} industries below threshold.\")\n",
    "#         print(f\"  - Industries meeting target: {res.get('industry_list', '')}\")\n",
    "#         print(f\"  - Covers {res.get('document_country_coverage', 0)} document countries below threshold.\")\n",
    "#         print(f\"  - Document countries meeting target: {res.get('document_country_list', '')}\")\n",
    "#         print(f\"  - Covers {res.get('region_coverage', 0)} regions below threshold.\")\n",
    "#         print(f\"  - Regions meeting target: {res.get('region_list', '')}\")\n",
    "#         print(f\"  - Share of top 3 customers: {res.get('share_top_3_customers', 0):.2f}%\")\n",
    "#         print(f\"  - Share of strategic clients meeting SLA: {res.get('share_strategic_clients_met_sla', 0):.2f}%\")\n",
    "#         print(f\"  - Total Cost for this strategy: ${res.get('cost_total', 0):.2f} EUR\")\n",
    "\n",
    "def print_simulation_summaries(simulation_results, error_margin_logic, abs_margin_value, rel_margin_value):\n",
    "    \"\"\"\n",
    "    Display simulation summaries as a table instead of printing line by line.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "\n",
    "    rows = []\n",
    "    for sim_name, res in simulation_results.items():\n",
    "        if error_margin_logic == 'Absolute Error Margin':\n",
    "            threshold_type = \"Absolute Error Margin\"\n",
    "            threshold_value = abs_margin_value * 100\n",
    "            threshold_unit = \"%\"\n",
    "        else:\n",
    "            threshold_type = \"Relative Error Margin\"\n",
    "            threshold_value = rel_margin_value * 100\n",
    "            threshold_unit = \"% of group rate\"\n",
    "        rows.append({\n",
    "            \"Simulation\": sim_name,\n",
    "            \"Threshold Logic\": f\"{threshold_type} ({threshold_value:.2f}{threshold_unit})\",\n",
    "            \"Sample Size\": res.get('total_sampled', 'N/A'),\n",
    "            \"Overall Error Margin (%)\": f\"{res.get('overall_margin', float('nan'))*100:.2f}\",\n",
    "            \"Customers < Threshold\": res.get('customer_coverage', 0),\n",
    "            \"Customers Meeting Target\": res.get('customer_list', ''),\n",
    "            \"Products < Threshold\": res.get('product_coverage', 0),\n",
    "            \"Products Meeting Target\": res.get('product_list', ''),\n",
    "            \"Industries < Threshold\": res.get('industry_coverage', 0),\n",
    "            \"Industries Meeting Target\": res.get('industry_list', ''),\n",
    "            \"Doc Countries < Threshold\": res.get('document_country_coverage', 0),\n",
    "            \"Doc Countries Meeting Target\": res.get('document_country_list', ''),\n",
    "            \"Regions < Threshold\": res.get('region_coverage', 0),\n",
    "            \"Regions Meeting Target\": res.get('region_list', ''),\n",
    "            \"Share Top 3 Cust. (%)\": f\"{res.get('share_top_3_customers', 0):.2f}\",\n",
    "            \"Share Strat. Clients SLA (%)\": f\"{res.get('share_strategic_clients_met_sla', 0):.2f}\",\n",
    "            \"Total Cost (EUR)\": f\"{res.get('cost_total', 0):.2f}\",\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    display(df)\n",
    "\n",
    "\n",
    "def _run_single_hyperparam_sim(args):\n",
    "    import copy\n",
    "    import numpy as np\n",
    "\n",
    "    df_data, base_params, param_set = args\n",
    "    params = copy.deepcopy(base_params)\n",
    "    params.update(param_set)\n",
    "\n",
    "    filtered_df = df_data.copy()\n",
    "\n",
    "    # # 1. Filter by primary_product if needed\n",
    "    # filtered_df = df_data[\n",
    "    #     df_data['primary_product'].isin(params.get('primary_product', [])) if 'primary_product' in params else True]\n",
    "\n",
    "    # if filtered_df.empty:\n",
    "    # # Return a dummy result row with NaNs and zeros\n",
    "    #     return [{\n",
    "    #         **param_set,\n",
    "    #         'scenario': 'No Data',\n",
    "    #         'cost_total': np.nan,\n",
    "    #         'overall_margin': np.nan,\n",
    "    #         'share_top_3_customers': np.nan,\n",
    "    #         'customer_coverage': 0,\n",
    "    #         'product_coverage': 0,\n",
    "    #         'industry_coverage': 0,\n",
    "    #         'document_country_coverage': 0,\n",
    "    #         'region_coverage': 0\n",
    "    #     }]\n",
    "    \n",
    "    # 2. Scale data if days < data_days (as in generate_report)\n",
    "    if 'days_count' in filtered_df.columns:\n",
    "        default_days = int(filtered_df['days_count'].iloc[0])\n",
    "    else:\n",
    "        default_days = 30\n",
    "\n",
    "    data_days = default_days\n",
    "    days = params.get('days', data_days)\n",
    "    if days < data_days:\n",
    "        scale_factor = days / data_days\n",
    "        cols_to_scale = [\n",
    "            'volume', 'qa_reviewed', 'qa_reviewed_for_reporting',\n",
    "            'decision_error_sessions', 'extraction_error_sessions',\n",
    "            'false_approves', 'false_declines', 'missed_fraud_sessions',\n",
    "            'declinable_sessions', 'approvable_sessions', 'fraud_sessions',\n",
    "            'full_auto_sessions', 'declined_sessions', 'approved_sessions',\n",
    "            'declined_due_to_fraud_sessions', 'approved_due_to_fraud_sessions'\n",
    "        ]\n",
    "        for col in cols_to_scale:\n",
    "            if col in filtered_df.columns:\n",
    "                filtered_df[col] = (filtered_df[col] * scale_factor).round().fillna(0).astype(int)\n",
    "\n",
    "    # 3. Calculate total QA reviewed\n",
    "    qa_total_source = params.get('qa_total_source', 'Use Current QA Reviewed')\n",
    "    daily_reviews = params.get('daily_reviews', None)\n",
    "    days = params.get('days', data_days)\n",
    "    total_qa_reviewed = daily_reviews * days\n",
    "\n",
    "    # 4. Prepare population data (aggregation, rates, etc.)\n",
    "    prepare_population_data = globals().get('prepare_population_data')\n",
    "    selected_metric = params.get('selected_metric', 'Missed Fraud Rate')\n",
    "    metric_properties = globals().get('METRIC_PROPERTIES', {})\n",
    "    reporting_dimensions = params.get('reporting_dimensions', ['customer'])\n",
    "\n",
    "    # Only add 'customer_priority' if running Client-Tier Weighting scenario\n",
    "    sampling_types = params.get('sampling_types_to_run', ['Current Random'])\n",
    "    needs_priority = 'Client-Tier Weighting' in sampling_types\n",
    "    if needs_priority and 'customer_priority' not in reporting_dimensions and 'customer_priority' in filtered_df.columns:\n",
    "        reporting_dimensions = list(reporting_dimensions) + ['customer_priority']\n",
    "\n",
    "    z_score = params.get('z_score', 1.96)\n",
    "    population_df, global_avg_metrics, overall_avg_frequencies_in_qa = prepare_population_data(\n",
    "        filtered_df, selected_metric, metric_properties, z_score, reporting_dimensions\n",
    "    )\n",
    "\n",
    "    # Defensive: If population_df is empty, return dummy row\n",
    "    if population_df.empty or population_df['volume'].sum() == 0:\n",
    "        return [{\n",
    "            **param_set,\n",
    "            'scenario': 'No Data',\n",
    "            'cost_total': np.nan,\n",
    "            'overall_margin': np.nan,\n",
    "            'share_top_3_customers': np.nan,\n",
    "            'customer_coverage': 0,\n",
    "            'product_coverage': 0,\n",
    "            'industry_coverage': 0,\n",
    "            'document_country_coverage': 0,\n",
    "            'region_coverage': 0\n",
    "        }]\n",
    "\n",
    "    # 5. Assign true random sample if needed\n",
    "    assign_true_random_sample = globals().get('assign_true_random_sample')\n",
    "    if assign_true_random_sample is not None:\n",
    "        population_df = assign_true_random_sample(population_df, int(total_qa_reviewed))\n",
    "\n",
    "    # 6. Set 'qa_reviewed_for_reporting'\n",
    "    population_df['qa_reviewed_for_reporting'] = population_df['true_random_qa_reviewed']\n",
    "\n",
    "    # 7. Run simulations as usual, but force each scenario to run only ONCE (no scales)\n",
    "    # Patch: For each scenario, run with current total QA reviewed only\n",
    "    simulation_results = {}\n",
    "    for scenario in sampling_types:\n",
    "        scenario_results = run_simulations(\n",
    "            population_df=population_df,\n",
    "            global_avg_metrics=global_avg_metrics,\n",
    "            overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa,\n",
    "            selected_metric=selected_metric,\n",
    "            reporting_dimensions=reporting_dimensions,\n",
    "            error_margin_logic=params.get('error_margin_logic', 'Absolute Error Margin'),\n",
    "            absolute_error_margin_value=params.get('absolute_error_margin_value', 0.02),\n",
    "            relative_error_margin_value=params.get('relative_error_margin_value', 0.2),\n",
    "            min_absolute_error_margin=params.get('min_absolute_error_margin', 0.005),\n",
    "            target_overall_error_percent=params.get('target_overall_error_percent', 2.0),\n",
    "            target_dimension_error_percent=params.get('target_dimension_error_percent', 2.0),\n",
    "            weight_factor_small_customers=params.get('weight_factor_small_customers', 10),\n",
    "            small_customer_volume_quantile_threshold=params.get('small_customer_volume_quantile_threshold', 0.99),\n",
    "            stratify_by_dimension=params.get('stratify_by_dimension', ['customer_top20', 'primary_product', 'industry', 'region', 'document_country', 'platform']),\n",
    "            sampling_types_to_run=[scenario],  # Only run this scenario\n",
    "            top_n=params.get('top_n', 10),\n",
    "            z_score=z_score,\n",
    "            min_per_stratum=params.get('min_per_stratum', 500),\n",
    "            verbose=False\n",
    "        )\n",
    "        # Only keep the first result for this scenario\n",
    "        for scenario, res in scenario_results.items():\n",
    "            simulation_results[scenario] = res\n",
    "\n",
    "    rows = []\n",
    "    for scenario, res in simulation_results.items():\n",
    "        row = dict(param_set)\n",
    "        row['scenario'] = scenario\n",
    "        row.update(res)\n",
    "        for col in ['cost_total', 'overall_margin', 'share_top_3_customers']:\n",
    "            if col not in row:\n",
    "                row[col] = np.nan\n",
    "        rows.append(row)\n",
    "\n",
    "    # If all coverage columns are zero, print debug info\n",
    "    if all(row.get(cov, 0) == 0 for cov in ['customer_coverage', 'product_coverage', 'industry_coverage', 'document_country_coverage', 'region_coverage']):\n",
    "        print(\"[DEBUG] Scenario:\", row.get('scenario', 'Unknown'))\n",
    "        print(\"[DEBUG] No coverage found for params:\", param_set)\n",
    "        print(\"[DEBUG] reporting_dimensions:\", reporting_dimensions)\n",
    "        print(\"[DEBUG] population_df shape:\", population_df.shape)\n",
    "        print(\"[DEBUG] population_df columns:\", list(population_df.columns))\n",
    "        print(\"[DEBUG] population_df volume sum:\", population_df['volume'].sum())\n",
    "        print(\"[DEBUG] simulation_results:\", simulation_results)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# def _run_single_hyperparam_sim(args):\n",
    "#     import copy\n",
    "#     import numpy as np\n",
    "\n",
    "#     df_data, base_params, param_set = args\n",
    "#     params = copy.deepcopy(base_params)\n",
    "#     params.update(param_set)\n",
    "\n",
    "#     # 1. Filter by primary_product if needed\n",
    "#     filtered_df = df_data\n",
    "#     if 'primary_product' in params and isinstance(params['primary_product'], list):\n",
    "#         filtered_df = filtered_df[filtered_df['primary_product'].isin(params['primary_product'])]\n",
    "\n",
    "#     # 2. Scale data if days < data_days (as in generate_report)\n",
    "#     if 'days_count' in filtered_df.columns:\n",
    "#         default_days = int(filtered_df['days_count'].iloc[0])\n",
    "#     else:\n",
    "#         default_days = 30\n",
    "#     data_days = default_days\n",
    "#     days = params.get('days', data_days)\n",
    "#     if days < data_days:\n",
    "#         scale_factor = days / data_days\n",
    "#         cols_to_scale = [\n",
    "#             'volume', 'qa_reviewed', 'qa_reviewed_for_reporting',\n",
    "#             'decision_error_sessions', 'extraction_error_sessions',\n",
    "#             'false_approves', 'false_declines', 'missed_fraud_sessions',\n",
    "#             'declinable_sessions', 'approvable_sessions', 'fraud_sessions',\n",
    "#             'full_auto_sessions', 'declined_sessions', 'approved_sessions',\n",
    "#             'declined_due_to_fraud_sessions', 'approved_due_to_fraud_sessions'\n",
    "#         ]\n",
    "#         for col in cols_to_scale:\n",
    "#             if col in filtered_df.columns:\n",
    "#                 filtered_df[col] = (filtered_df[col] * scale_factor).round().fillna(0).astype(int)\n",
    "\n",
    "#     # 3. Calculate total QA reviewed\n",
    "#     qa_total_source = params.get('qa_total_source', 'Use Current QA Reviewed')\n",
    "#     daily_reviews = params.get('daily_reviews', None)\n",
    "#     days = params.get('days', data_days)\n",
    "#     # if qa_total_source == 'Input Daily Reviews * Days' and daily_reviews is not None and days is not None:\n",
    "#     #     total_qa_reviewed = daily_reviews * days\n",
    "#     # else:\n",
    "#     #     total_qa_reviewed = filtered_df['qa_reviewed'].sum()\n",
    "#     total_qa_reviewed = daily_reviews * days\n",
    "\n",
    "#     # 4. Prepare population data (aggregation, rates, etc.)\n",
    "#     prepare_population_data = globals().get('prepare_population_data')\n",
    "#     selected_metric = params.get('selected_metric', 'Missed Fraud Rate')\n",
    "#     metric_properties = globals().get('METRIC_PROPERTIES', {})\n",
    "#     reporting_dimensions = params.get('reporting_dimensions', ['customer'])\n",
    "\n",
    "#     # Only add 'customer_priority' if running Client-Tier Weighting scenario\n",
    "#     sampling_types = params.get('sampling_types_to_run', ['Current Random'])\n",
    "#     needs_priority = 'Client-Tier Weighting' in sampling_types\n",
    "#     if needs_priority and 'customer_priority' not in reporting_dimensions and 'customer_priority' in filtered_df.columns:\n",
    "#         reporting_dimensions = list(reporting_dimensions) + ['customer_priority']\n",
    "\n",
    "#     z_score = params.get('z_score', 1.96)\n",
    "#     population_df, global_avg_metrics, overall_avg_frequencies_in_qa = prepare_population_data(\n",
    "#         filtered_df, selected_metric, metric_properties, z_score, reporting_dimensions\n",
    "#     )\n",
    "\n",
    "#     # Defensive: If population_df is empty, return dummy row\n",
    "#     if population_df.empty or population_df['volume'].sum() == 0:\n",
    "#         return [{\n",
    "#             **param_set,\n",
    "#             'scenario': 'No Data',\n",
    "#             'cost_total': np.nan,\n",
    "#             'overall_margin': np.nan,\n",
    "#             'share_top_3_customers': np.nan,\n",
    "#             'customer_coverage': 0,\n",
    "#             'product_coverage': 0,\n",
    "#             'industry_coverage': 0,\n",
    "#             'document_country_coverage': 0,\n",
    "#             'region_coverage': 0\n",
    "#         }]\n",
    "\n",
    "#     # 5. Assign true random sample if needed\n",
    "#     assign_true_random_sample = globals().get('assign_true_random_sample')\n",
    "#     if assign_true_random_sample is not None:\n",
    "#         population_df = assign_true_random_sample(population_df, int(total_qa_reviewed))\n",
    "\n",
    "#     # 6. Set 'qa_reviewed_for_reporting'\n",
    "#     # if qa_total_source == 'Input Daily Reviews * Days' and 'true_random_qa_reviewed' in population_df.columns:\n",
    "#     #     population_df['qa_reviewed_for_reporting'] = population_df['true_random_qa_reviewed']\n",
    "#     # else:\n",
    "#     #     population_df['qa_reviewed_for_reporting'] = population_df['qa_reviewed']\n",
    "#     population_df['qa_reviewed_for_reporting'] = population_df['true_random_qa_reviewed']\n",
    "\n",
    "#     # 7. Run simulations as usual\n",
    "#     simulation_results = run_simulations(\n",
    "#         population_df=population_df,\n",
    "#         global_avg_metrics=global_avg_metrics,\n",
    "#         overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa,\n",
    "#         selected_metric=selected_metric,\n",
    "#         reporting_dimensions=reporting_dimensions,\n",
    "#         error_margin_logic=params.get('error_margin_logic', 'Absolute Error Margin'),\n",
    "#         absolute_error_margin_value=params.get('absolute_error_margin_value', 0.02),\n",
    "#         relative_error_margin_value=params.get('relative_error_margin_value', 0.2),\n",
    "#         min_absolute_error_margin=params.get('min_absolute_error_margin', 0.005),\n",
    "#         target_overall_error_percent=params.get('target_overall_error_percent', 2.0),\n",
    "#         target_dimension_error_percent=params.get('target_dimension_error_percent', 2.0),\n",
    "#         weight_factor_small_customers=params.get('weight_factor_small_customers', 10),\n",
    "#         small_customer_volume_quantile_threshold=params.get('small_customer_volume_quantile_threshold', 0.99),\n",
    "#         stratify_by_dimension=params.get('stratify_by_dimension', ['product_portfolio', 'industry', 'region']),\n",
    "#         sampling_types_to_run=sampling_types,\n",
    "#         top_n=params.get('top_n', 10),\n",
    "#         z_score=z_score,\n",
    "#         min_per_stratum=params.get('min_per_stratum', 500),\n",
    "#         verbose=False\n",
    "#     )\n",
    "#     rows = []\n",
    "#     for scenario, res in simulation_results.items():\n",
    "#         row = dict(param_set)\n",
    "#         row['scenario'] = scenario\n",
    "#         row.update(res)\n",
    "#         for col in ['cost_total', 'overall_margin', 'share_top_3_customers']:\n",
    "#             if col not in row:\n",
    "#                 row[col] = np.nan\n",
    "#         rows.append(row)\n",
    "\n",
    "#     # If all coverage columns are zero, print debug info\n",
    "#     if all(row.get(cov, 0) == 0 for cov in ['customer_coverage', 'product_coverage', 'industry_coverage', 'document_country_coverage', 'region_coverage']):\n",
    "#         print(\"[DEBUG] No coverage found for params:\", param_set)\n",
    "#         print(\"[DEBUG] reporting_dimensions:\", reporting_dimensions)\n",
    "#         print(\"[DEBUG] population_df shape:\", population_df.shape)\n",
    "#         print(\"[DEBUG] population_df columns:\", list(population_df.columns))\n",
    "#         print(\"[DEBUG] population_df volume sum:\", population_df['volume'].sum())\n",
    "#         print(\"[DEBUG] simulation_results:\", simulation_results)\n",
    "\n",
    "#     return rows\n",
    "        \n",
    "\n",
    "def hyperparameter_simulation_search(\n",
    "    df_data,\n",
    "    base_params,\n",
    "    param_grid,\n",
    "    constraints=None,\n",
    "    scoring_fn=None,\n",
    "    max_workers=8  # Adjust based on your CPU\n",
    "):\n",
    "    import concurrent.futures\n",
    "    import itertools\n",
    "    import pandas as pd\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    args_list = [(df_data, base_params, param_set) for param_set in param_combinations]\n",
    "\n",
    "    all_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(_run_single_hyperparam_sim, args) for args in args_list]\n",
    "        for i, future in enumerate(tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Hyperparameter Search\")):\n",
    "            rows = future.result()\n",
    "            all_results.extend(rows)\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    if constraints:\n",
    "        mask = pd.Series([True] * len(results_df))\n",
    "        for k, fn in constraints.items():\n",
    "            mask &= results_df[k].apply(fn)\n",
    "        filtered = results_df[mask]\n",
    "    else:\n",
    "        filtered = results_df\n",
    "\n",
    "    # Improved scoring: maximize coverage, then minimize cost\n",
    "    def default_scoring(row):\n",
    "        coverage = sum([\n",
    "            row.get('customer_coverage', 0),\n",
    "            row.get('product_coverage', 0),\n",
    "            row.get('industry_coverage', 0),\n",
    "            row.get('document_country_coverage', 0),\n",
    "            row.get('region_coverage', 0)\n",
    "        ])\n",
    "        # Use negative cost so that lower cost is better for tie-breaking\n",
    "        return (coverage, -row.get('cost_total', 1e9))\n",
    "\n",
    "    def weighted_scoring(row):\n",
    "        # Example weights (tune as needed)\n",
    "        w_coverage = 2\n",
    "        w_cost = -0.01\n",
    "        w_margin = -1\n",
    "        w_top3 = -0.5\n",
    "        # Calculate coverage\n",
    "        coverage = sum([\n",
    "            row.get('customer_coverage', 0),\n",
    "            row.get('product_coverage', 0),\n",
    "            row.get('industry_coverage', 0),\n",
    "            row.get('document_country_coverage', 0),\n",
    "            row.get('region_coverage', 0)\n",
    "        ])\n",
    "        cost = row.get('cost_total', 1e9)\n",
    "        margin = row.get('overall_margin', 1)\n",
    "        top3 = row.get('share_top_3_customers', 100)\n",
    "        # Combine with weights\n",
    "        return w_coverage * coverage + w_cost * cost + w_margin * margin + w_top3 * top3\n",
    "\n",
    "    if scoring_fn is None:\n",
    "        scoring_fn = default_scoring\n",
    "    if scoring_fn == 'weighted_scoring':\n",
    "        scoring_fn = weighted_scoring\n",
    "\n",
    "    # Find best row: maximize coverage, then minimize cost\n",
    "    if not filtered.empty:\n",
    "        best_idx = filtered.apply(scoring_fn, axis=1).idxmax()\n",
    "        best_row = filtered.loc[best_idx]\n",
    "        print(\"\\nBest configuration found (meets constraints):\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] No configuration met all constraints. Showing best available result.\")\n",
    "        best_idx = results_df.apply(scoring_fn, axis=1).idxmax()\n",
    "        best_row = results_df.loc[best_idx]\n",
    "\n",
    "    print(best_row)\n",
    "    return results_df, best_row\n",
    "\n",
    "def display_hyperparam_search_summary(results_df):\n",
    "    \"\"\"\n",
    "    Display a styled summary table for hyperparameter search results, similar to display_simulation_summary.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "\n",
    "    if results_df.empty:\n",
    "        print(\"\\nNo hyperparameter search results to summarize.\")\n",
    "        return\n",
    "\n",
    "    # Calculate improvement if possible\n",
    "    results_df.replace([np.inf, -np.inf], pd.NA, inplace=True)\n",
    "    if 'overall_margin' in results_df.columns:\n",
    "        results_df['overall_margin_perc'] = results_df['overall_margin'] * 100\n",
    "\n",
    "    if 'overall_margin_perc' in results_df.columns and not results_df.empty:\n",
    "        base_margin = results_df['overall_margin_perc'].iloc[0]\n",
    "        if pd.isna(base_margin) or base_margin == 0:\n",
    "            valid_bases = results_df['overall_margin_perc'].dropna()\n",
    "            base_margin = valid_bases.iloc[0] if not valid_bases.empty else 0\n",
    "        if base_margin > 0:\n",
    "            results_df['Improvement (%)'] = (base_margin - results_df['overall_margin_perc']) / base_margin * 100\n",
    "        else:\n",
    "            results_df['Improvement (%)'] = 0\n",
    "    else:\n",
    "        results_df['Improvement (%)'] = pd.NA\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    results_df.rename(columns={\n",
    "        'total_sampled': 'Total QA Sampled',\n",
    "        'cost_total': 'Total Cost (EUR)',\n",
    "        'overall_margin_perc': 'Overall Margin (%)',\n",
    "        'dimension_coverage': 'Selected dimension covered below threshold',\n",
    "        'customer_coverage': 'Customers below threshold',\n",
    "        'product_coverage': 'Products below threshold',\n",
    "        'industry_coverage': 'Industries below threshold',\n",
    "        'document_country_coverage': 'Document Countries below threshold',\n",
    "        'region_coverage': 'Regions below threshold',\n",
    "        'Improvement (%)': 'Improvement in Error Margin (%)',\n",
    "        'share_top_3_customers': 'Share of Top 3 Customers in Sample (%)',\n",
    "        'share_strategic_clients_met_sla': 'Share of Platinum Clients Met SLA (%)',\n",
    "        'cost_over_coverage': 'Cost per Actionable Unit (EUR)'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Choose columns to display (add/remove as needed)\n",
    "    display_cols = [\n",
    "        'scenario',\n",
    "        'min_per_stratum',\n",
    "        'absolute_error_margin_value',\n",
    "        'days',\n",
    "        'daily_reviews',\n",
    "        'Total QA Sampled',\n",
    "        'Total Cost (EUR)',\n",
    "        'Overall Margin (%)',\n",
    "        'Customers below threshold',\n",
    "        'Products below threshold',\n",
    "        'Industries below threshold',\n",
    "        'Document Countries below threshold',\n",
    "        'Regions below threshold',\n",
    "        'Improvement in Error Margin (%)',\n",
    "        'Share of Top 3 Customers in Sample (%)',\n",
    "        'Share of Platinum Clients Met SLA (%)',\n",
    "        'Cost per Actionable Unit (EUR)'\n",
    "    ]\n",
    "    existing_display_cols = [col for col in display_cols if col in results_df.columns]\n",
    "\n",
    "    # Round all columns to 2 decimals where appropriate\n",
    "    rounded_df = results_df[existing_display_cols].copy().applymap(\n",
    "        lambda x: round(x, 2) if isinstance(x, (float, int)) and pd.notnull(x) else x\n",
    "    )\n",
    "\n",
    "    print(\"\\n## Hyperparameter Search Summary Table\")\n",
    "    def highlight_improvement(val):\n",
    "        if pd.isna(val):\n",
    "            return ''\n",
    "        color = 'green' if val > 0 else 'red' if val < 0 else ''\n",
    "        return f'color: {color}'\n",
    "\n",
    "    def bold_margin(val):\n",
    "        try:\n",
    "            threshold = globals().get('TARGET_OVERALL_ERROR_PERCENT', 2.0)\n",
    "            if float(val) < threshold:\n",
    "                return 'font-weight: bold'\n",
    "        except Exception:\n",
    "            pass\n",
    "        return ''\n",
    "\n",
    "    styled = (\n",
    "        rounded_df\n",
    "        .style\n",
    "        .format(\"{:.2f}\")\n",
    "        .applymap(highlight_improvement, subset=['Improvement in Error Margin (%)'] if 'Improvement in Error Margin (%)' in rounded_df.columns else [])\n",
    "        .applymap(bold_margin, subset=['Overall Margin (%)'] if 'Overall Margin (%)' in rounded_df.columns else [])\n",
    "    )\n",
    "    display(styled)\n",
    "\n",
    "def print_hyper_search_summary(best_row, error_margin_logic, abs_margin_value, rel_margin_value, results_df=None):\n",
    "    \"\"\"\n",
    "    Print a summary for the best hyperparameter search result, or fallback to best available if none met constraints.\n",
    "    \"\"\"\n",
    "    print(\"\\n## Hyperparameter Search: Best Configuration Summary\")\n",
    "    if best_row is None or (hasattr(best_row, \"empty\") and best_row.empty):\n",
    "        print(\"No valid configuration met all constraints.\")\n",
    "        # Fallback: show best available by cost or coverage if results_df is provided\n",
    "        if results_df is not None and not results_df.empty:\n",
    "            print(\"\\nShowing best available configuration (did not meet all constraints):\")\n",
    "            # Use the same scoring as in hyperparameter_simulation_search\n",
    "            def scoring(row):\n",
    "                coverage = sum([\n",
    "                    row.get('customer_coverage', 0),\n",
    "                    row.get('product_coverage', 0),\n",
    "                    row.get('industry_coverage', 0),\n",
    "                    row.get('document_country_coverage', 0),\n",
    "                    row.get('region_coverage', 0)\n",
    "                ])\n",
    "                return (coverage, -row.get('cost_total', 1e9))\n",
    "            best_idx = results_df.apply(scoring, axis=1).idxmax()\n",
    "            best_row = results_df.loc[best_idx]\n",
    "        else:\n",
    "            print(\"No configurations available.\")\n",
    "            return\n",
    "\n",
    "    # Determine threshold logic and value\n",
    "    if error_margin_logic == 'Absolute Error Margin':\n",
    "        threshold_type = \"Absolute Error Margin\"\n",
    "        threshold_value = abs_margin_value * 100\n",
    "        threshold_unit = \"%\"\n",
    "    else:\n",
    "        threshold_type = \"Relative Error Margin\"\n",
    "        threshold_value = rel_margin_value * 100\n",
    "        threshold_unit = \"% of group rate\"\n",
    "\n",
    "    print(f\"Threshold logic used: {threshold_type} (Threshold: {threshold_value:.2f}{threshold_unit})\")\n",
    "    print(f\"Sample size: {best_row.get('total_sampled', 'N/A')} sessions (Scenario: {best_row.get('scenario', 'N/A')})\")\n",
    "    print(f\"  - Overall error margin: {best_row.get('overall_margin', float('nan'))*100:.2f}%\")\n",
    "    print(f\"  - Covers {best_row.get('customer_coverage', 0)} customers below threshold.\")\n",
    "    print(f\"  - Customers meeting target: {best_row.get('customer_list', '')}\")\n",
    "    print(f\"  - Covers {best_row.get('product_coverage', 0)} products below threshold.\")\n",
    "    print(f\"  - Products meeting target: {best_row.get('product_list', '')}\")\n",
    "    print(f\"  - Covers {best_row.get('industry_coverage', 0)} industries below threshold.\")\n",
    "    print(f\"  - Industries meeting target: {best_row.get('industry_list', '')}\")\n",
    "    print(f\"  - Covers {best_row.get('document_country_coverage', 0)} document countries below threshold.\")\n",
    "    print(f\"  - Document countries meeting target: {best_row.get('document_country_list', '')}\")\n",
    "    print(f\"  - Covers {best_row.get('region_coverage', 0)} regions below threshold.\")\n",
    "    print(f\"  - Regions meeting target: {best_row.get('region_list', '')}\")\n",
    "    print(f\"  - Share of top 3 customers: {best_row.get('share_top_3_customers', 0):.2f}%\")\n",
    "    print(f\"  - Share of strategic clients meeting SLA: {best_row.get('share_strategic_clients_met_sla', 0):.2f}%\")\n",
    "    print(f\"  - Total Cost for this configuration: ${best_row.get('cost_total', 0):.2f} EUR\")\n",
    "\n",
    "\n",
    "from skopt.space import Integer, Real\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def skopt_hyperparameter_search(\n",
    "    df_data,\n",
    "    base_params,\n",
    "    param_space,  # list of skopt.space objects\n",
    "    param_names,  # list of param names in order\n",
    "    n_calls=50,\n",
    "    constraints=None,  # dict of {col: lambda x: ...}\n",
    "    verbose=True,\n",
    "    scenario_names=None  # list of scenario names to run for each param set\n",
    "):\n",
    "    from tqdm.notebook import tqdm\n",
    "    import pandas as pd\n",
    "    from skopt import gp_minimize\n",
    "\n",
    "    tried_params = []\n",
    "    tried_results = []\n",
    "    tried_objectives = []\n",
    "    tried_scenarios = []\n",
    "\n",
    "    # Only run n_calls, not n_calls * len(scenario_names), since sample size is fixed by daily_reviews * days\n",
    "    pbar = tqdm(total=n_calls * (len(scenario_names) if scenario_names else 1), desc=\"skopt Optimization Progress\")\n",
    "\n",
    "    # Cache to avoid duplicate runs\n",
    "    cache = {}\n",
    "\n",
    "    def objective(params):\n",
    "        param_set = dict(zip(param_names, params))\n",
    "        # Always pass all scenario_names at once\n",
    "        if scenario_names:\n",
    "            param_set['sampling_types_to_run'] = scenario_names\n",
    "            cache_key = tuple(params) + tuple(scenario_names)\n",
    "        else:\n",
    "            cache_key = tuple(params)\n",
    "        if cache_key in cache:\n",
    "            rows = cache[cache_key]\n",
    "        else:\n",
    "            rows = _run_single_hyperparam_sim((df_data, base_params, param_set))\n",
    "            cache[cache_key] = rows\n",
    "        objs = []\n",
    "        for row in rows:\n",
    "            coverage = sum([\n",
    "                row.get('customer_coverage', 0),\n",
    "                row.get('product_coverage', 0),\n",
    "                row.get('industry_coverage', 0),\n",
    "                row.get('document_country_coverage', 0),\n",
    "                row.get('region_coverage', 0)\n",
    "            ])\n",
    "            share_sla = row.get('share_strategic_clients_met_sla', 0)\n",
    "            share_top_3_customers = row.get('share_top_3_customers', 0)\n",
    "            cost_per_unit = row.get('cost_over_coverage', 1e9)\n",
    "            uber_simulated_sessions = row.get('uber_main_account_simulated_sessions', 0)\n",
    "            uber_left_to_cover = row.get('uber_main_account_left_to_cover', 0)\n",
    "            penalty = 0\n",
    "            if constraints:\n",
    "                for k, fn in constraints.items():\n",
    "                    if not fn(row.get(k, None)):\n",
    "                        penalty += 10000\n",
    "            obj = -(2 * coverage + 3 * share_sla) + 0.1 * cost_per_unit + 0.1 * share_top_3_customers + penalty\n",
    "            tried_params.append(param_set.copy())\n",
    "            tried_results.append(row)\n",
    "            tried_objectives.append(obj)\n",
    "            tried_scenarios.append(row.get('scenario', None))\n",
    "            objs.append(obj)\n",
    "            pbar.update(1)\n",
    "        return min(objs)  # or mean(objs), depending on your goal\n",
    "\n",
    "    # Run the optimization\n",
    "    result = gp_minimize(\n",
    "        objective,\n",
    "        param_space,\n",
    "        n_calls=n_calls,\n",
    "        random_state=42\n",
    "    )\n",
    "    pbar.close()\n",
    "\n",
    "    # --- Build results DataFrame robustly ---\n",
    "    results_records = []\n",
    "    for param_set, row, obj, scenario in zip(tried_params, tried_results, tried_objectives, tried_scenarios):\n",
    "        record = dict(param_set)\n",
    "        record.update(row)\n",
    "        record['scenario'] = scenario\n",
    "        record['objective'] = obj\n",
    "        record['stratify_by_dimension'] = param_set.get('stratify_by_dimension', row.get('stratify_by_dimension', None))\n",
    "        results_records.append(record)\n",
    "    results_df = pd.DataFrame(results_records)\n",
    "\n",
    "    # Apply constraints to filter best\n",
    "    if constraints:\n",
    "        mask = pd.Series([True] * len(results_df))\n",
    "        for k, fn in constraints.items():\n",
    "            mask &= results_df[k].apply(fn)\n",
    "        filtered = results_df[mask]\n",
    "    else:\n",
    "        filtered = results_df\n",
    "\n",
    "    # Find best row: lowest objective value\n",
    "    if not filtered.empty:\n",
    "        best_idx = filtered['objective'].idxmin()\n",
    "        best_row = filtered.loc[best_idx]\n",
    "        print(\"\\nBest configuration found (meets constraints):\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] No configuration met all constraints. Showing best available result.\")\n",
    "        best_idx = results_df['objective'].idxmin()\n",
    "        best_row = results_df.loc[best_idx]\n",
    "\n",
    "    print(\"\\nBest parameters found by skopt:\")\n",
    "    best_params = dict(zip(param_names, result.x))\n",
    "    print(best_params)\n",
    "    print(\"Best objective value:\", result.fun)\n",
    "    print(best_row)\n",
    "\n",
    "    results_df['is_best_row'] = results_df.index == best_row.name\n",
    "    \n",
    "    cols = ['min_per_stratum', 'daily_reviews', 'scenario',\n",
    "       'overall_margin', 'total_sampled', 'cost_total', 'customer_coverage', 'customer_list',\n",
    "       'product_coverage', 'product_list', 'industry_coverage',\n",
    "       'industry_list', 'document_country_coverage', 'document_country_list',\n",
    "       'region_coverage', 'region_list', 'share_top_3_customers',\n",
    "       'share_strategic_clients_met_sla', 'cost_over_coverage', 'uber_main_account_simulated_sessions', 'uber_main_account_left_to_cover']\n",
    "\n",
    "    # Automatically save results_df to CSV\n",
    "    results_df = results_df.sort_values(\n",
    "                by=['is_best_row', 'customer_coverage', 'product_coverage', 'industry_coverage', 'document_country_coverage', 'region_coverage', 'cost_total'],\n",
    "                ascending=[False, False, False, False, False, False, True]\n",
    "    )\n",
    "\n",
    "    results_df = results_df[cols + ['is_best_row']].copy()\n",
    "    best_row = best_row[cols].copy()\n",
    "\n",
    "    results_df.to_csv('skopt_hyperparam_results.csv', index=False)\n",
    "    \n",
    "    print(\"skopt hyperparameter search results saved to skopt_hyperparam_results.csv\")\n",
    "\n",
    "    return results_df, best_row, results_df.columns\n",
    "\n",
    "#old\n",
    "# def skopt_hyperparameter_search(\n",
    "#     df_data,\n",
    "#     base_params,\n",
    "#     param_space,  # list of skopt.space objects\n",
    "#     param_names,  # list of param names in order\n",
    "#     n_calls=30,\n",
    "#     constraints=None,  # dict of {col: lambda x: ...}\n",
    "#     verbose=True,\n",
    "#     scenario_names=None  # list of scenario names to run for each param set\n",
    "# ):\n",
    "#     from tqdm.notebook import tqdm\n",
    "#     import pandas as pd\n",
    "#     from skopt import gp_minimize\n",
    "\n",
    "#     tried_params = []\n",
    "#     tried_results = []\n",
    "#     tried_objectives = []\n",
    "#     tried_scenarios = []\n",
    "\n",
    "#     #pbar = tqdm(desc=\"skopt Optimization Progress\")\n",
    "#     pbar = tqdm(total=n_calls * (len(scenario_names) if scenario_names else 1), desc=\"skopt Optimization Progress\")\n",
    "\n",
    "#     # Cache to avoid duplicate runs\n",
    "#     cache = {}\n",
    "\n",
    "#     def objective(params):\n",
    "#         param_set = dict(zip(param_names, params))\n",
    "#         # Always pass all scenario_names at once\n",
    "#         if scenario_names:\n",
    "#             param_set['sampling_types_to_run'] = scenario_names\n",
    "#             cache_key = tuple(params) + tuple(scenario_names)\n",
    "#         else:\n",
    "#             cache_key = tuple(params)\n",
    "#         if cache_key in cache:\n",
    "#             rows = cache[cache_key]\n",
    "#         else:\n",
    "#             rows = _run_single_hyperparam_sim((df_data, base_params, param_set))\n",
    "#             cache[cache_key] = rows\n",
    "#         objs = []\n",
    "#         for row in rows:\n",
    "#             coverage = sum([\n",
    "#                 row.get('customer_coverage', 0),\n",
    "#                 row.get('product_coverage', 0),\n",
    "#                 row.get('industry_coverage', 0),\n",
    "#                 row.get('document_country_coverage', 0),\n",
    "#                 row.get('region_coverage', 0)\n",
    "#             ])\n",
    "#             share_sla = row.get('share_strategic_clients_met_sla', 0)\n",
    "#             cost_per_unit = row.get('cost_over_coverage', 1e9)\n",
    "#             penalty = 0\n",
    "#             if constraints:\n",
    "#                 for k, fn in constraints.items():\n",
    "#                     if not fn(row.get(k, None)):\n",
    "#                         penalty += 1000\n",
    "#             obj = -(2 * coverage + 1 * share_sla) + 0.1 * cost_per_unit + penalty\n",
    "#             tried_params.append(param_set.copy())\n",
    "#             tried_results.append(row)\n",
    "#             tried_objectives.append(obj)\n",
    "#             tried_scenarios.append(row.get('scenario', None))\n",
    "#             objs.append(obj)\n",
    "#             pbar.update(1)\n",
    "#         return min(objs)  # or mean(objs), depending on your goal\n",
    "\n",
    "#     # Run the optimization\n",
    "#     result = gp_minimize(\n",
    "#         objective,\n",
    "#         param_space,\n",
    "#         n_calls=n_calls,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     pbar.close()\n",
    "\n",
    "#     # --- FIX: Build results DataFrame robustly ---\n",
    "#     results_records = []\n",
    "#     for param_set, row, obj, scenario in zip(tried_params, tried_results, tried_objectives, tried_scenarios):\n",
    "#         record = dict(param_set)\n",
    "#         record.update(row)\n",
    "#         record['scenario'] = scenario\n",
    "#         record['objective'] = obj\n",
    "#         results_records.append(record)\n",
    "#     results_df = pd.DataFrame(results_records)\n",
    "\n",
    "#     # Apply constraints to filter best\n",
    "#     if constraints:\n",
    "#         mask = pd.Series([True] * len(results_df))\n",
    "#         for k, fn in constraints.items():\n",
    "#             mask &= results_df[k].apply(fn)\n",
    "#         filtered = results_df[mask]\n",
    "#     else:\n",
    "#         filtered = results_df\n",
    "\n",
    "#     # Find best row: lowest objective value\n",
    "#     if not filtered.empty:\n",
    "#         best_idx = filtered['objective'].idxmin()\n",
    "#         best_row = filtered.loc[best_idx]\n",
    "#         print(\"\\nBest configuration found (meets constraints):\")\n",
    "#     else:\n",
    "#         print(\"\\n[WARNING] No configuration met all constraints. Showing best available result.\")\n",
    "#         best_idx = results_df['objective'].idxmin()\n",
    "#         best_row = results_df.loc[best_idx]\n",
    "\n",
    "#     print(\"\\nBest parameters found by skopt:\")\n",
    "#     best_params = dict(zip(param_names, result.x))\n",
    "#     print(best_params)\n",
    "#     print(\"Best objective value:\", result.fun)\n",
    "#     print(best_row)\n",
    "\n",
    "#     cols = ['min_per_stratum', 'daily_reviews', 'sampling_types_to_run', 'strategy_name','scenario',\n",
    "#        'overall_margin', 'total_sampled', 'cost_total', 'customer_coverage', 'customer_list',\n",
    "#        'product_coverage', 'product_list', 'industry_coverage',\n",
    "#        'industry_list', 'document_country_coverage', 'document_country_list',\n",
    "#        'region_coverage', 'region_list', 'share_top_3_customers',\n",
    "#        'share_strategic_clients_met_sla', 'cost_over_coverage']\n",
    "\n",
    "#     results_df = results_df[cols].copy()\n",
    "#     best_row = best_row[cols].copy()\n",
    "\n",
    "#     return results_df, best_row, results_df.columns\n",
    "\n",
    "# --- Interactive UI Setup ---\n",
    "def create_interactive_ui(df_data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Creates and displays an interactive UI using ipywidgets for configuring and running the report.\n",
    "    \"\"\"\n",
    "\n",
    "    run_button = widgets.Button(description=\"Create Report\", button_style='success', layout=widgets.Layout(width='auto'))\n",
    "    hyperparam_button = widgets.Button(description=\"Find Optimal Solution\", button_style='info', layout=widgets.Layout(width='auto'))\n",
    "    output_area = widgets.Output()\n",
    "    hyperparam_output_area = widgets.Output()\n",
    "\n",
    "    # Initial config to derive available dimensions and metrics (uses default global values)\n",
    "    metric_related_cols = set()\n",
    "    for props in METRIC_PROPERTIES.values():\n",
    "        metric_related_cols.add(props['numerator_col'])\n",
    "        metric_related_cols.add(props['denominator_col'])\n",
    "        metric_related_cols.add(props['freq_denominator_base_col'])\n",
    "\n",
    "    exclude_cols = metric_related_cols.union({\n",
    "        'volume', 'qa_reviewed', 'rand', 'bias_score', \n",
    "        'original_metric_rate', 'original_freq_den', \n",
    "        'simulated_total_sessions', 'simulated_num_x', 'simulated_den_n',\n",
    "        'margin', 'margin_rel', 'temp_sampling_volume_small', \n",
    "        'temp_sampling_volume_tier', 'temp_sampling_volume_stratified_proportional',\n",
    "        'temp_sampling_volume_stratified_equal',\n",
    "        'temp_sampling_volume_strat', 'strata', 'total_volume', 'total_qa_reviewed',\n",
    "        'proportion_of_total_volume', 'proportion_of_qa_sample', 'false_decline_rate',\n",
    "        'automation_rate', 'decline_rate_in_population', 'approve_rate_in_population', 'fraud_rate_in_population',\n",
    "        'decision_error_rate', 'extraction_error_rate', 'false_approve_rate',\n",
    "        'false_decline_rate', 'missed_fraud_rate', 'decline_rate_in_qa', 'approve_rate_in_qa', 'fraud_rate_in_qa'\n",
    "    })\n",
    "\n",
    "    available_products = sorted(df_data['primary_product'].dropna().unique())\n",
    "\n",
    "    available_dimensions = sorted(list(set(df_data.columns) - exclude_cols))\n",
    "    \n",
    "    initial_reporting_dims = [d for d in DEFAULT_REPORTING_DIMENSIONS if d in available_dimensions]\n",
    "    initial_stratify_dims = [d for d in DEFAULT_STRATIFY_BY_DIMENSION if d in available_dimensions]\n",
    "    initial_products = [p for p in DEFAULT_AVAILABLE_PRODUCTS if p in available_products]\n",
    "    \n",
    "    if not initial_reporting_dims and available_dimensions:\n",
    "        initial_reporting_dims = [available_dimensions[0]]\n",
    "    if not initial_stratify_dims and available_dimensions:\n",
    "        initial_stratify_dims = [available_dimensions[0]]\n",
    "    if not initial_products and available_products:\n",
    "        initial_products = [available_products[0]]\n",
    "\n",
    "    # available_qa_review_types = sorted(df_data['qa_review_type'].dropna().unique())\n",
    "    # initial_qa_review_types = available_qa_review_types[:1] if available_qa_review_types else []\n",
    "\n",
    "    # qa_review_type_widget = widgets.SelectMultiple(\n",
    "    #     options=available_qa_review_types,\n",
    "    #     value=initial_qa_review_types,\n",
    "    #     description='QA Review Type(s):',\n",
    "    #     disabled=False,\n",
    "    #     layout=widgets.Layout(width='90%', height='80px'),\n",
    "    #     style={'description_width': '200px'}\n",
    "    # )\n",
    "\n",
    "\n",
    "    all_metrics = sorted(list(METRIC_PROPERTIES.keys()))\n",
    "    all_sampling_types = [\n",
    "        'Current Random', 'True Random', 'Oversample Small Traditional', 'Client-Tier Weighting', \n",
    "        'Proportional Stratified Sampling',\n",
    "        'Equal Stratified Sampling',\n",
    "        'Biased with Exponents'\n",
    "        #,'Volume-Based Stratified Sampling'\n",
    "        #, 'Random'\n",
    "    ]\n",
    "\n",
    "    qa_tiers = ['Tier Platinum', 'Tier 1', 'Tier 2', 'Tier 3', 'Tier 4', 'Tier 5', 'UNKNOWN', '']\n",
    "    editable_tiers = ['Tier Platinum', 'Tier 1']\n",
    "    auto_tiers = [t for t in qa_tiers if t not in editable_tiers]\n",
    "\n",
    "    # Sliders for editable tiers\n",
    "    qa_proportion_widgets = {\n",
    "        tier: widgets.FloatSlider(\n",
    "            value=TARGET_QA_PROPORTIONS.get(tier, 0.1),\n",
    "            min=0.0, max=1.0, step=0.01,\n",
    "            description=f\"{tier}:\",\n",
    "            continuous_update=True,\n",
    "            readout_format='.2f',\n",
    "            layout=widgets.Layout(width='90%'),\n",
    "            style={'description_width': '120px'}\n",
    "        )\n",
    "        for tier in editable_tiers\n",
    "    }\n",
    "\n",
    "    # Read-only displays for auto tiers\n",
    "    qa_proportion_labels = {\n",
    "        tier: widgets.Label(\n",
    "            value=f\"{tier or 'Unknown'}: {TARGET_QA_PROPORTIONS.get(tier, 0.1):.2f}\"\n",
    "        )\n",
    "        for tier in auto_tiers\n",
    "    }\n",
    "\n",
    "    def update_auto_tiers(*args):\n",
    "        platinum = qa_proportion_widgets['Tier Platinum'].value\n",
    "        tier1 = qa_proportion_widgets['Tier 1'].value\n",
    "        remaining = max(0.0, 1.0 - platinum - tier1)\n",
    "        per_tier = remaining / len(auto_tiers) if auto_tiers else 0.0\n",
    "        for tier in auto_tiers:\n",
    "            qa_proportion_labels[tier].value = f\"{tier or 'Unknown'}: {per_tier:.2f}\"\n",
    "\n",
    "    for w in qa_proportion_widgets.values():\n",
    "        w.observe(update_auto_tiers, names='value')\n",
    "\n",
    "    update_auto_tiers()  # Initialize\n",
    "\n",
    "    qa_proportion_box = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>QA Proportions by Client Tier (For simulation) </h4>\")\n",
    "    ] + list(qa_proportion_widgets.values()) + list(qa_proportion_labels.values()))\n",
    "    \n",
    "    primary_product_widget = widgets.SelectMultiple(\n",
    "        options=available_products,\n",
    "        value=initial_products,\n",
    "        description='Primary Product(s):',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='90%', height='100px'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "\n",
    "    metric_widget = widgets.Dropdown(\n",
    "        options=all_metrics,\n",
    "        value=DEFAULT_SELECTED_METRIC,\n",
    "        description='Metric:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    overall_error_widget = widgets.FloatText(\n",
    "        value=DEFAULT_TARGET_OVERALL_ERROR_PERCENT, min=0.1, max=100.0, step=0.5,\n",
    "        description='Overall Error %:',\n",
    "        continuous_update=False, orientation='horizontal', readout=True, readout_format='.1f',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    dim_error_widget = widgets.FloatText(\n",
    "        value=DEFAULT_TARGET_DIMENSION_ERROR_PERCENT, min=0.1, max=100.0, step=0.5,\n",
    "        description='Dimension Error %:',\n",
    "        continuous_update=False, orientation='horizontal', readout=True, readout_format='.1f',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    confidence_widget = widgets.FloatText(\n",
    "        value=DEFAULT_CONFIDENCE_LEVEL, min=0.80, max=0.99, step=0.01,\n",
    "        description='Confidence Level:',\n",
    "        continuous_update=False, orientation='horizontal', readout=True, readout_format='.2f',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    cost_per_session_widget = widgets.FloatText(\n",
    "        value=DEFAULT_REVIEW_COST_PER_SESSION,\n",
    "        description='Cost/Session (EUR):',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    budget_widget = widgets.FloatText(\n",
    "        value=DEFAULT_PREDEFINED_TOTAL_COST_BUDGET,\n",
    "        description='Budget (EUR):',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    weight_small_customers_widget = widgets.FloatText(\n",
    "        value=DEFAULT_WEIGHT_FACTOR_SMALL_CUSTOMERS, min=1.0, max=50.0, step=0.1,\n",
    "        description='Weight for small customers:',\n",
    "        continuous_update=False, orientation='horizontal', readout=True, readout_format='.1f',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    small_customer_quantile_widget = widgets.FloatSlider(\n",
    "        value=DEFAULT_SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD, min=0.05, max=0.99, step=0.01,\n",
    "        description='Small Cust. Quantile:',\n",
    "        continuous_update=False, orientation='horizontal', readout=True, readout_format='.2f',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    error_margin_logic_widget = widgets.RadioButtons(\n",
    "        options=['Absolute Error Margin', 'Relative Error Margin'],\n",
    "        value=DEFAULT_ERROR_MARGIN_LOGIC,\n",
    "        description='Error Margin Logic:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    abs_error_value_widget = widgets.FloatText(\n",
    "        value=DEFAULT_ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "        description='Abs. Error Value:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    rel_error_value_widget = widgets.FloatText(\n",
    "        value=DEFAULT_RELATIVE_ERROR_MARGIN_VALUE,\n",
    "        description='Rel. Error Value:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    min_abs_error_widget = widgets.FloatText(\n",
    "        value=DEFAULT_MIN_ABSOLUTE_ERROR_MARGIN,\n",
    "        description='Min Abs. Error:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    reporting_dims_widget = widgets.SelectMultiple(\n",
    "        options=available_dimensions,\n",
    "        value=initial_reporting_dims,\n",
    "        description='Reporting Dimensions:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='90%', height='100px'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    stratify_dims_widget = widgets.SelectMultiple(\n",
    "        options=available_dimensions,\n",
    "        value=initial_stratify_dims,\n",
    "        description='Stratify Dimensions:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='90%', height='100px'),\n",
    "        style={'description_width': '200px'} \n",
    "    )\n",
    "    min_per_stratum_widget = widgets.IntText(\n",
    "    value=DEFAULT_MIN_PER_STRATUM,\n",
    "    description='Min per Stratum:',\n",
    "    layout=widgets.Layout(width='90%'),\n",
    "    style={'description_width': '200px'}\n",
    "    )\n",
    "    top_n_widget = widgets.IntSlider(\n",
    "        value=DEFAULT_TOP_N, min=1, max=20, step=1,\n",
    "        description='Top N for Plots:',\n",
    "        continuous_update=False, orientation='horizontal', readout=True,\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    \n",
    "    sampling_types_widget = widgets.SelectMultiple(\n",
    "        options=all_sampling_types,\n",
    "        value=all_sampling_types,\n",
    "        description='Run Scenarios:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='90%', height='150px'),\n",
    "        style={'description_width': '200px'} \n",
    "    )\n",
    "\n",
    "    save_plots_html_widget = widgets.Checkbox(\n",
    "        value=DEFAULT_SAVE_PLOTS_TO_HTML,\n",
    "        description='Save Plots to HTML Files',\n",
    "        disabled=False,\n",
    "        indent=False,\n",
    "        layout=widgets.Layout(width='auto'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "\n",
    "    daily_reviews_widget = widgets.IntText(\n",
    "    value=2000,\n",
    "    description='Daily Reviews:',\n",
    "    layout=widgets.Layout(width='90%'),\n",
    "    style={'description_width': '200px'}\n",
    "    )\n",
    "    days_widget = widgets.IntText(\n",
    "        value=30,\n",
    "        description='Days:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    qa_total_source_widget = widgets.RadioButtons(\n",
    "        options=['Use Current QA Reviewed', 'Input Daily Reviews * Days'],\n",
    "        value='Use Current QA Reviewed',\n",
    "        description='QA Total Source:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "\n",
    "    # --- Hyperparameter Search Section ---\n",
    "\n",
    "        # Widgets for param grid\n",
    "    hyper_min_per_stratum = widgets.SelectMultiple(\n",
    "        options=[100, 300, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000, 5000],\n",
    "        value=[800, 1000, 2000, 3000],\n",
    "        description='min_per_stratum:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "    hyper_abs_error = widgets.SelectMultiple(\n",
    "        options=[0.01, 0.02, 0.25, 0.03],\n",
    "        value=[0.02],\n",
    "        description='abs_error_margin:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "    hyper_days = widgets.IntText(\n",
    "        value=30,\n",
    "        description='days:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "    hyper_daily_reviews = widgets.SelectMultiple(\n",
    "        options=[1000, 1500, 2000, 2500, 3000, 3500, 4000, 5000, 6000],\n",
    "        value=[2000, 2500, 3000, 4000],\n",
    "        description='daily_reviews:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "\n",
    "    constraint_strategic_clients_sla = widgets.FloatText(\n",
    "    value=0.5,  # Example: require at least 50%\n",
    "    description='Min Strategic Clients SLA Share:',\n",
    "    layout=widgets.Layout(width='90%'),\n",
    "    style={'description_width': '150px'}\n",
    "    )\n",
    "\n",
    "    # Widgets for constraints\n",
    "    constraint_cost = widgets.IntText(\n",
    "        value=100000,\n",
    "        description='Max Cost:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "    constraint_margin = widgets.FloatText(\n",
    "        value=0.02,\n",
    "        description='Max Overall Margin:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "    constraint_share_top3 = widgets.FloatText(\n",
    "        value=0.5,\n",
    "        description='Max Top3 Share:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "\n",
    "    hyperparam_section = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Hyperparameter Search Settings</h4>\"),\n",
    "        widgets.HTML(\"<b>Parameter Grid:</b>\"),\n",
    "        hyper_min_per_stratum,\n",
    "        hyper_abs_error,\n",
    "        hyper_days,\n",
    "        hyper_daily_reviews,\n",
    "        widgets.HTML(\"<b>Constraints:</b>\"),\n",
    "        constraint_cost,\n",
    "        constraint_margin,\n",
    "        constraint_share_top3,\n",
    "        constraint_strategic_clients_sla\n",
    "    ])\n",
    "\n",
    "    # --- Group widgets into titled sections for better readability ---\n",
    "    section_metric = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Metric & Error Targets</h4>\"),\n",
    "        #qa_review_type_widget,\n",
    "        primary_product_widget,\n",
    "        metric_widget,\n",
    "        overall_error_widget,\n",
    "        qa_total_source_widget,\n",
    "        daily_reviews_widget,   \n",
    "        days_widget,           \n",
    "        # dim_error_widget,\n",
    "        top_n_widget\n",
    "    ])\n",
    "\n",
    "    section_price_per_session = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Price Per Reviewed Session</h4>\"),\n",
    "        cost_per_session_widget\n",
    "    ])\n",
    "\n",
    "    section_small_customer = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Small Customer Weighting (For Oversampling Simulation)</h4>\"),\n",
    "        weight_small_customers_widget,\n",
    "        small_customer_quantile_widget\n",
    "    ])\n",
    "\n",
    "    section_error_margin = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Error Margin Settings</h4>\"),\n",
    "        error_margin_logic_widget,\n",
    "        abs_error_value_widget,\n",
    "        rel_error_value_widget,\n",
    "        min_abs_error_widget\n",
    "    ])\n",
    "\n",
    "    section_dimensions = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Reporting & Stratification</h4>\"),\n",
    "        reporting_dims_widget,\n",
    "        stratify_dims_widget,\n",
    "        min_per_stratum_widget\n",
    "    ])\n",
    "\n",
    "    section_simulation = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Simulation Scenarios</h4>\"),\n",
    "        sampling_types_widget,\n",
    "        save_plots_html_widget\n",
    "    ])\n",
    "\n",
    "    main_layout = widgets.VBox([\n",
    "        section_metric,\n",
    "        section_price_per_session,\n",
    "        qa_proportion_box,\n",
    "        section_small_customer,\n",
    "        section_error_margin,\n",
    "        section_dimensions,\n",
    "        section_simulation,\n",
    "    ])\n",
    "\n",
    "    def on_run_report(b):\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            filtered_df = df_data[\n",
    "                df_data['primary_product'].isin(list(primary_product_widget.value)) \n",
    "                # &\n",
    "                # df_data['qa_review_type'].isin(list(qa_review_type_widget.value))\n",
    "            ]\n",
    "            # Collect QA proportions from widgets\n",
    "            platinum = qa_proportion_widgets['Tier Platinum'].value\n",
    "            tier1 = qa_proportion_widgets['Tier 1'].value\n",
    "            remaining = max(0.0, 1.0 - platinum - tier1)\n",
    "            per_tier = remaining / len(auto_tiers) if auto_tiers else 0.0\n",
    "            qa_proportions = {\n",
    "                'Tier Platinum': platinum,\n",
    "                'Tier 1': tier1,\n",
    "            }\n",
    "            for tier in auto_tiers:\n",
    "                qa_proportions[tier] = per_tier\n",
    "            current_params = {\n",
    "                #'qa_review_type': list(qa_review_type_widget.value),\n",
    "                'qa_total_source': qa_total_source_widget.value,\n",
    "                'daily_reviews': daily_reviews_widget.value,\n",
    "                'days': days_widget.value,\n",
    "                'qa_proportions': qa_proportions,\n",
    "                'selected_metric': metric_widget.value,\n",
    "                'target_overall_error_percent': overall_error_widget.value,\n",
    "                'target_dimension_error_percent': dim_error_widget.value,\n",
    "                'confidence_level': confidence_widget.value,\n",
    "                'review_cost_per_session': cost_per_session_widget.value,\n",
    "                'predefined_total_cost_budget': budget_widget.value,\n",
    "                'weight_factor_small_customers': weight_small_customers_widget.value,\n",
    "                'small_customer_volume_quantile_threshold': small_customer_quantile_widget.value,\n",
    "                'error_margin_logic': error_margin_logic_widget.value,\n",
    "                'absolute_error_margin_value': abs_error_value_widget.value,\n",
    "                'relative_error_margin_value': rel_error_value_widget.value,\n",
    "                'min_absolute_error_margin': min_abs_error_widget.value,\n",
    "                'reporting_dimensions': list(reporting_dims_widget.value),\n",
    "                'stratify_by_dimension': list(stratify_dims_widget.value),\n",
    "                'min_per_stratum': min_per_stratum_widget.value,\n",
    "                'top_n': top_n_widget.value,\n",
    "                'sampling_types_to_run': list(sampling_types_widget.value),\n",
    "                'save_plots_to_html': save_plots_html_widget.value\n",
    "            }\n",
    "            #current_params_global = current_params\n",
    "            generate_report(filtered_df, current_params)\n",
    "            #best_result = find_optimal_relative_error_margin(df, current_params_global, margin_range=(0.2, 0.5), step=0.1, verbose=False)\n",
    "            #print(f\"\\nBest Relative Error Margin found: {best_result['Relative Error Margin']:.2f} with overall margin: {best_result['Overall Margin (%)']:.2f}% and min cost per unit: {best_result['Min Cost per Unit']:.2f} EUR\")\n",
    "\n",
    "    def on_hyperparam_search(b):\n",
    "        with hyperparam_output_area:\n",
    "            clear_output(wait=True)\n",
    "            filtered_df = df_data[\n",
    "                df_data['primary_product'].isin(list(primary_product_widget.value)) \n",
    "                # &\n",
    "                # df_data['qa_review_type'].isin(list(qa_review_type_widget.value))\n",
    "            ]\n",
    "            platinum = qa_proportion_widgets['Tier Platinum'].value\n",
    "            tier1 = qa_proportion_widgets['Tier 1'].value\n",
    "            remaining = max(0.0, 1.0 - platinum - tier1)\n",
    "            per_tier = remaining / len(auto_tiers) if auto_tiers else 0.0\n",
    "            qa_proportions = {'Tier Platinum': platinum, 'Tier 1': tier1}\n",
    "            for tier in auto_tiers:\n",
    "                qa_proportions[tier] = per_tier\n",
    "            current_params = {\n",
    "                'qa_total_source': qa_total_source_widget.value,\n",
    "                'daily_reviews': daily_reviews_widget.value,\n",
    "                'days': days_widget.value,\n",
    "                'qa_proportions': qa_proportions,\n",
    "                'selected_metric': metric_widget.value,\n",
    "                'target_overall_error_percent': overall_error_widget.value,\n",
    "                'target_dimension_error_percent': dim_error_widget.value,\n",
    "                'confidence_level': confidence_widget.value,\n",
    "                'review_cost_per_session': cost_per_session_widget.value,\n",
    "                'predefined_total_cost_budget': budget_widget.value,\n",
    "                'weight_factor_small_customers': weight_small_customers_widget.value,\n",
    "                'small_customer_volume_quantile_threshold': small_customer_quantile_widget.value,\n",
    "                'error_margin_logic': error_margin_logic_widget.value,\n",
    "                'absolute_error_margin_value': abs_error_value_widget.value,\n",
    "                'relative_error_margin_value': rel_error_value_widget.value,\n",
    "                'min_absolute_error_margin': min_abs_error_widget.value,\n",
    "                'reporting_dimensions': list(reporting_dims_widget.value),\n",
    "                'stratify_by_dimension': list(stratify_dims_widget.value),\n",
    "                'min_per_stratum': min_per_stratum_widget.value,\n",
    "                'top_n': top_n_widget.value,\n",
    "                'sampling_types_to_run': list(sampling_types_widget.value),\n",
    "                'save_plots_to_html': save_plots_html_widget.value\n",
    "            }\n",
    "\n",
    "            # Build param_grid and constraints from widget values\n",
    "            param_grid = {\n",
    "                'min_per_stratum': list(hyper_min_per_stratum.value),\n",
    "                'absolute_error_margin_value': list(hyper_abs_error.value),\n",
    "                'days': [hyper_days.value],\n",
    "                'daily_reviews': list(hyper_daily_reviews.value),\n",
    "            }\n",
    "            constraints = {\n",
    "                'cost_total': lambda x: x < constraint_cost.value,\n",
    "                'overall_margin': lambda x: x < constraint_margin.value,\n",
    "                'share_top_3_customers': lambda x: x < constraint_share_top3.value,\n",
    "                'share_strategic_clients_met_sla': lambda x: x >= constraint_strategic_clients_sla.value\n",
    "            }\n",
    "\n",
    "            # results_df, best_row = hyperparameter_simulation_search(\n",
    "            #     df_data=filtered_df,\n",
    "            #     base_params=current_params,\n",
    "            #     param_grid=param_grid,\n",
    "            #     constraints=constraints,\n",
    "            #     scoring_fn=None,\n",
    "            #     max_workers=8\n",
    "            # )\n",
    "            from IPython.display import display, HTML\n",
    "            import io, sys\n",
    "            # old_stdout = sys.stdout\n",
    "            # captured_output = io.StringIO()\n",
    "            # sys.stdout = captured_output\n",
    "            # print(\"=== Best Row (Optimal Configuration) ===\")\n",
    "            # print_hyper_search_summary(\n",
    "            #     best_row,\n",
    "            #     error_margin_logic=current_params.get('error_margin_logic', 'Absolute Error Margin'),\n",
    "            #     abs_margin_value=current_params.get('absolute_error_margin_value', 0.02),\n",
    "            #     rel_margin_value=current_params.get('relative_error_margin_value', 0.2),\n",
    "            #     results_df=results_df\n",
    "            # )\n",
    "            # sys.stdout = old_stdout\n",
    "            # display(HTML(f\"<pre>{captured_output.getvalue()}</pre>\"))\n",
    "            # print(\"\\n=== Best Row Details ===\")\n",
    "            # if best_row is not None and not isinstance(best_row, pd.DataFrame):\n",
    "            #     display(pd.DataFrame([best_row]))\n",
    "            # else:\n",
    "            #     display(best_row)\n",
    "            # print(\"\\n=== All Results ===\")\n",
    "            # display(results_df)\n",
    "\n",
    "\n",
    "            from skopt.space import Integer, Real\n",
    "\n",
    "            # Example: dynamically build param_space based on widget values\n",
    "            param_space = []\n",
    "            param_names = []\n",
    "\n",
    "            if len(hyper_min_per_stratum.value) > 0:\n",
    "                min_val = min(hyper_min_per_stratum.value)\n",
    "                max_val = max(hyper_min_per_stratum.value)\n",
    "                param_space.append(Integer(min_val, max_val, name='min_per_stratum'))\n",
    "                param_names.append('min_per_stratum')\n",
    "\n",
    "            # if len(hyper_abs_error.value) > 0:\n",
    "            #     min_val = min(hyper_abs_error.value)\n",
    "            #     max_val = max(hyper_abs_error.value)\n",
    "            #     # Use Real if your error margin values are floats\n",
    "            #     param_space.append(Real(min_val, max_val, name='absolute_error_margin_value'))\n",
    "            #     param_names.append('absolute_error_margin_value')\n",
    "\n",
    "            if len(hyper_daily_reviews.value) > 0:\n",
    "                min_val = min(hyper_daily_reviews.value)\n",
    "                max_val = max(hyper_daily_reviews.value)\n",
    "                param_space.append(Integer(min_val, max_val, name='daily_reviews'))\n",
    "                param_names.append('daily_reviews')\n",
    "\n",
    "            # Now use param_space and param_names in skopt_hyperparameter_search\n",
    "            results_df, best_row, columns = skopt_hyperparameter_search(\n",
    "                filtered_df, current_params, param_space, param_names, n_calls=30, constraints=constraints, scenario_names=list(sampling_types_widget.value) if sampling_types_widget.value else None\n",
    "            )\n",
    "\n",
    "            from IPython.display import display\n",
    "            print(\"=== Best Row (Optimal Configuration) ===\")\n",
    "            display(pd.DataFrame(best_row))\n",
    "            print(\"=== Top 5 Results ===\")\n",
    "            display(results_df.sort_values(\n",
    "                by=['is_best_row','customer_coverage', 'product_coverage', 'industry_coverage', 'document_country_coverage', 'region_coverage', 'cost_total'],\n",
    "                ascending=[False, False, False, False, False, False, True]\n",
    "            ).head(15))\n",
    "            \n",
    "            display(columns)\n",
    "            \n",
    "    run_button.on_click(on_run_report)\n",
    "    hyperparam_button.on_click(on_hyperparam_search)\n",
    "\n",
    "    display(widgets.VBox([\n",
    "    main_layout,\n",
    "    widgets.HBox([run_button]),\n",
    "    hyperparam_section,\n",
    "    widgets.HBox([hyperparam_button]),\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.HTML(\"<h4>Output</h4>\"),\n",
    "    output_area,\n",
    "    hyperparam_output_area\n",
    "    ]))\n",
    "\n",
    "    #display(main_layout)\n",
    "\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Expand the display to fit the content\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4672b698",
   "metadata": {},
   "source": [
    "# REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56c97b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa51250637448d796ec347133f22e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(VBox(children=(HTML(value='<h4>Metric & Error Targets</h4>'), SelectMultiple(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_interactive_ui(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c21d59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c84d8216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "if 'days_count' in df.columns:\n",
    "    print(int(df['days_count'].iloc[0]))\n",
    "else:\n",
    "    print(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e87d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".coding-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
